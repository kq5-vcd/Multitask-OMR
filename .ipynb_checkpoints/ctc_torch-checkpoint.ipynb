{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#assert device == 'cuda:0' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return (255. - image)/255.\n",
    "\n",
    "\n",
    "def resize(image, height):\n",
    "    #new_width = old_width * (new_height/old_height)\n",
    "    width = int(float(height * image.shape[1]) / image.shape[0]) \n",
    "    sample_img = cv2.resize(image, (width, height))\n",
    "    \n",
    "    del width\n",
    "    \n",
    "    return sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(a,b):\n",
    "    \"Computes the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "\n",
    "    if n > m:\n",
    "        a,b = b,a\n",
    "        n,m = m,n\n",
    "\n",
    "    current = range(n+1)\n",
    "    for i in range(1,m+1):\n",
    "        previous, current = current, [i]+[0]*n\n",
    "        for j in range(1,n+1):\n",
    "            add, delete = previous[j]+1, current[j-1]+1\n",
    "            change = previous[j-1]\n",
    "            if a[j-1] != b[i-1]:\n",
    "                change = change + 1\n",
    "            current[j] = min(add, delete, change)\n",
    "\n",
    "    return current[n]/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_levenshtein(a,b):\n",
    "    n, m = len(a), len(b)\n",
    "    dist = 0\n",
    "\n",
    "    if n > m:\n",
    "        a,b = b,a\n",
    "        n,m = m,n\n",
    "        \n",
    "    for i in range(n):\n",
    "        word = a[i]\n",
    "        \n",
    "        if len(b) == 1:\n",
    "            dist += levenshtein(word, b[0])\n",
    "            break\n",
    "            \n",
    "        b_seq = b[:-n+i+1]\n",
    "        \n",
    "        seq_dist = [levenshtein(word, x) for x in b_seq]\n",
    "        best_val = min(seq_dist)\n",
    "        dest_idx = seq_dist.index(best_val)\n",
    "        \n",
    "        dist += dest_idx + best_val\n",
    "        \n",
    "        if i == n - 1:\n",
    "            dist += len(b) - dest_idx - 1\n",
    "            break\n",
    "            \n",
    "        b = b[dest_idx+1:]\n",
    "            \n",
    "\n",
    "    return dist/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_symbols(dict_path):\n",
    "    word2int = {} #map symbols to numeric values\n",
    "    int2word = {} #map numeric values to symbols\n",
    "\n",
    "    dict_file = open(dict_path,'r')\n",
    "    dict_list = dict_file.read().splitlines()\n",
    "    \n",
    "    for word in dict_list:\n",
    "        if not word in word2int:\n",
    "            word_idx = len(word2int)\n",
    "            word2int[word] = word_idx\n",
    "            int2word[word_idx] = word\n",
    "\n",
    "    dict_file.close()\n",
    "    \n",
    "    del dict_file\n",
    "    \n",
    "    return (word2int, int2word, len(int2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriMuS:\n",
    "    # Data preprocessor/loader for the model\n",
    "    gt_element_separator = '-'\n",
    "    PAD_COLUMN = 0\n",
    "\n",
    "\n",
    "    def __init__(self, corpus_dirpath, corpus_path, word2int, semantic, \n",
    "                 img_height, batch_size, img_channels, train_split=0.5, test_split=0.5):\n",
    "        corpus_file = open(corpus_path,'r')\n",
    "        corpus_list = corpus_file.read().splitlines()\n",
    "        corpus_file.close()\n",
    "        \n",
    "        del corpus_file\n",
    "\n",
    "        # Train and validation split\n",
    "        random.shuffle(corpus_list) \n",
    "        train_idx = int(len(corpus_list) * train_split) \n",
    "        test_idx = int(len(corpus_list) * test_split) \n",
    "\n",
    "        self.training_list = corpus_list[:train_idx]\n",
    "        self.validation_list = corpus_list[train_idx:-test_idx]\n",
    "        self.test_list = corpus_list[-test_idx:]\n",
    "        \n",
    "        del train_idx\n",
    "        del test_idx\n",
    "\n",
    "        print ('Training with ' + str(len(self.training_list)) + ' ,validating with ' \n",
    "               + str(len(self.validation_list)) + ' , and testing with ' + str(len(self.test_list)))\n",
    "        \n",
    "        self.semantic = semantic\n",
    "        self.corpus_dirpath = corpus_dirpath\n",
    "        \n",
    "        self.current_idx = 0 #identify current index in list of samples\n",
    "        self.current_eval_idx = 0\n",
    "        self.current_test_idx = 0\n",
    "\n",
    "        # Dictionary\n",
    "        self.word2int = word2int #map symbols to numeric values\n",
    "\n",
    "        self.vocabulary_size = len(self.word2int)\n",
    "        \n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "        self.img_channels = img_channels\n",
    "        \n",
    "        self.training_iterations = int(len(self.training_list)/self.batch_size) + 1\n",
    "        self.eval_iterations = int(len(self.validation_list)/self.batch_size) + 1\n",
    "        self.test_iterations = int(len(self.test_list)/self.batch_size) + 1\n",
    "        \n",
    "    \n",
    "    def load_data(self, filepath):\n",
    "        sample_fullpath = self.corpus_dirpath + '/' + filepath + '/' + filepath\n",
    "        #print(sample_filepath)\n",
    "\n",
    "        # IMAGE\n",
    "        image = cv2.imread(sample_fullpath + '.png', False)  # Grayscale is assumed!\n",
    "        \n",
    "        image = resize(image, self.img_height)\n",
    "        image = normalize(image)\n",
    "        \n",
    "\n",
    "        # GROUND TRUTH\n",
    "        if self.semantic:\n",
    "            sample_full_filepath = sample_fullpath + '.semantic'\n",
    "        else:\n",
    "            sample_full_filepath = sample_fullpath + '.agnostic'\n",
    "\n",
    "        sample_gt_file = open(sample_full_filepath, 'r')\n",
    "        sample_gt_plain = sample_gt_file.readline().rstrip().split('\\t')\n",
    "        sample_gt_file.close()\n",
    "        \n",
    "        del sample_fullpath\n",
    "\n",
    "        label = [self.word2int[lab] for lab in sample_gt_plain] #label: list of numeric values\n",
    "        \n",
    "        del sample_gt_plain\n",
    "        \n",
    "        return (image, label)\n",
    "    \n",
    "    \n",
    "    def transform_to_batch(self, images):\n",
    "        # Extend all images to match the longest in the batch\n",
    "        image_widths = [img.shape[1] for img in images]\n",
    "        max_image_width = max(image_widths)\n",
    "        \n",
    "        del image_widths\n",
    "\n",
    "        batch_images = np.ones(shape=[len(images),\n",
    "                                      self.img_channels,\n",
    "                                      self.img_height,\n",
    "                                      max_image_width], dtype=np.float32)*self.PAD_COLUMN\n",
    "        # batch shape: (b, c, h, w)\n",
    "        \n",
    "        del max_image_width\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            batch_images[i, 0, 0:img.shape[0], 0:img.shape[1]] = img\n",
    "        # shorter images will be padded\n",
    "\n",
    "        return batch_images\n",
    "\n",
    "        \n",
    "    def next_batch(self, phase=\"train\"):\n",
    "        # Create a batch\n",
    "        images = [] \n",
    "        labels = []\n",
    "\n",
    "        if phase == \"train\":\n",
    "            for _ in range(self.batch_size):\n",
    "                image, label = self.load_data(self.training_list[self.current_idx])\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "                self.current_idx = (self.current_idx + 1) % len(self.training_list) #increment index, turn back to beginning if overflow\n",
    "        elif phase == \"eval\":\n",
    "            for _ in range(self.batch_size):\n",
    "                image, label = self.load_data(self.validation_list[self.current_eval_idx])\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "                self.current_eval_idx = (self.current_eval_idx + 1) % len(self.validation_list)\n",
    "        elif phase == \"test\":\n",
    "            for _ in range(self.batch_size):\n",
    "                image, label = self.load_data(self.test_list[self.current_test_idx])\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "            \n",
    "                self.current_test_idx = (self.current_test_idx + 1) % len(self.test_list)\n",
    "\n",
    "        # Transform to batch\n",
    "        batch_images = self.transform_to_batch(images)\n",
    "        target_lengths = [len(x) for x in labels]\n",
    "        \n",
    "        flattened_labels = []\n",
    "        for label in labels:\n",
    "            flattened_labels += label\n",
    "            \n",
    "        del labels\n",
    "        \n",
    "        batch_images = torch.from_numpy(batch_images).to(device)\n",
    "        labels = torch.tensor(flattened_labels).to(device)\n",
    "        target_lengths = torch.tensor(target_lengths, dtype=torch.int32).to(device)\n",
    "\n",
    "        return (batch_images, labels, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut, dropout=0):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True, dropout=dropout)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        \n",
    "        del h\n",
    "        del recurrent\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        \n",
    "        del T\n",
    "        del b\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, img_height, nclass, conv_layers, rnn_hidden_states, dropout=0.5, kernel_size=3, pooling_size=2):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert img_height % 16 == 0, 'img_height has to be a multiple of 16'\n",
    "\n",
    "        self.cnn = nn.Sequential()\n",
    "        for i in range(len(conv_layers)):\n",
    "            input_channels = input_size if i == 0 else conv_layers[i-1]\n",
    "            output_channels = conv_layers[i]\n",
    "\n",
    "            self.cnn.add_module('conv{0}'.format(i),\n",
    "                                nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, padding = 1))\n",
    "            self.cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(output_channels))\n",
    "            self.cnn.add_module('relu{0}'.format(i), nn.LeakyReLU(0.2, inplace=True))\n",
    "            self.cnn.add_module('pooling{0}'.format(i), nn.MaxPool2d(pooling_size, 2))\n",
    "            \n",
    "            del input_channels\n",
    "            del output_channels\n",
    "            \n",
    "        self.rnn = nn.Sequential()\n",
    "        for i in range(len(rnn_hidden_states)):\n",
    "            inC = conv_layers[-1] if i == 0 else rnn_hidden_states[i-1]\n",
    "            \n",
    "            if i < len(rnn_hidden_states) - 1:\n",
    "                self.rnn.add_module('BiLSTM{0}'.format(i), \n",
    "                                    BidirectionalLSTM(inC, rnn_hidden_states[i], \n",
    "                                                      rnn_hidden_states[i], dropout=dropout))\n",
    "            else:\n",
    "                self.rnn.add_module('BiLSTM{0}'.format(i), \n",
    "                                    BidirectionalLSTM(inC, rnn_hidden_states[i], nclass))\n",
    "                \n",
    "            del inC\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        conv = conv.view(b, c, -1)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        #print(conv.size())\n",
    "        \n",
    "        del b\n",
    "        del c\n",
    "        del h\n",
    "        del w\n",
    "\n",
    "        # rnn features\n",
    "        output = self.rnn(conv)\n",
    "        #print(output.size())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "corpus = 'Data/Package' #Path to the corpus\n",
    "set_path = 'Data/package_list.txt' #Path to the set file\n",
    "save_model = 'Models/trained_semantic_model.pth' #Path to save the model\n",
    "vocabulary = 'Data/vocabulary_semantic.txt' #Path to the vocabulary file\n",
    "semantic = True\n",
    "\n",
    "train_split = 0.7\n",
    "test_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int, int2word, vocab_size = load_symbols(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 128\n",
    "img_width = None\n",
    "batch_size = 4\n",
    "img_channels = 1\n",
    "conv_channels = [32, 64, 128, 256]\n",
    "conv_filter_size = 3\n",
    "conv_pooling_size = 2\n",
    "rnn_hidden_states = [256,256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterization\n",
    "epochs = 5\n",
    "dropout = 0.5\n",
    "nclasses = vocab_size + 1\n",
    "input_size = 1\n",
    "\n",
    "prepared = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\envs\\bkai\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (cnn): Sequential(\n",
      "    (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (pooling2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (pooling3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rnn): Sequential(\n",
      "    (BiLSTM0): BidirectionalLSTM(\n",
      "      (rnn): LSTM(256, 256, dropout=0.5, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (BiLSTM1): BidirectionalLSTM(\n",
      "      (rnn): LSTM(256, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=1782, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CRNN(input_size, img_height, nclasses, \n",
    "             conv_channels, \n",
    "             rnn_hidden_states, \n",
    "             dropout=dropout)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "    #torch.distributed.init_process_group(\"mpi\")\n",
    "    #model = nn.parallel.DistributedDataParallel(model, device_ids=[0,1])\n",
    "    \n",
    "if prepared:\n",
    "    model.load_state_dict(torch.load('Models/trained_agnostic_model_adam.pth'))\n",
    "    \n",
    "loss_function = nn.CTCLoss(blank=vocab_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 61374 ,validating with 17537 , and testing with 8767\n"
     ]
    }
   ],
   "source": [
    "# Load primus\n",
    "primus = PriMuS(corpus, set_path, word2int, semantic, img_height, \n",
    "                batch_size, img_channels, train_split=train_split, test_split=test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, epochs, checkpoint):\n",
    "    ctc_train = []\n",
    "    ctc_eval = []\n",
    "    \n",
    "    best_val = 10000\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        print(\"Epoch:\", epoch+1)\n",
    "        \n",
    "        for i in range(dataloader.training_iterations):\n",
    "            batch, labels, label_lengths = dataloader.next_batch()\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(batch).log_softmax(2)\n",
    "            batch_lengths = Variable(torch.IntTensor([preds.size(0)] * preds.size(1))).to(device)\n",
    "\n",
    "            del batch\n",
    "\n",
    "            loss = loss_function(preds, labels.detach(), batch_lengths.detach(), label_lengths.detach())\n",
    "\n",
    "            del labels\n",
    "            del batch_lengths\n",
    "            del label_lengths\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            del preds\n",
    "            \n",
    "            if i % 100 == 99 or i == dataloader.training_iterations - 1:\n",
    "                ctc_train.append(loss.item()) \n",
    "                print(f'Iteration: {i+1:4}\\nLoss: {ctc_train[-1]:10.8f}')\n",
    "            del loss\n",
    "\n",
    "            if i % 100 == 99 or i == dataloader.training_iterations - 1:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    eval_data, eval_labels, eval_label_lengths = dataloader.next_batch(\"eval\")\n",
    "\n",
    "                    eval_preds = model(eval_data).log_softmax(2)\n",
    "\n",
    "                    del eval_data\n",
    "\n",
    "                    eval_lengths = Variable(torch.IntTensor([eval_preds.size(0)] * eval_preds.size(1))).to(device)\n",
    "                    error = loss_function(eval_preds, eval_labels.detach(), eval_lengths.detach(), eval_label_lengths.detach())\n",
    "\n",
    "                    del eval_labels\n",
    "                    del eval_label_lengths\n",
    "                    del eval_lengths\n",
    "                    del eval_preds\n",
    "\n",
    "                    err = error.item()\n",
    "                    del error\n",
    "                        \n",
    "                    ctc_eval.append(err)\n",
    "\n",
    "                    if err < best_val:\n",
    "                        best_val = err\n",
    "                        torch.save(model.state_dict(), checkpoint)\n",
    "\n",
    "                print(f'Validation loss: {err:10.8f}')\n",
    "                del err\n",
    "        \n",
    "    return (ctc_train, ctc_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Iteration:    1\n",
      "Loss: 212.05184937\n",
      "Validation loss: 292.40625000\n",
      "Iteration:  100\n",
      "Loss: 4.79466820\n",
      "Validation loss: 5.70991707\n",
      "Iteration:  200\n",
      "Loss: 5.00813055\n",
      "Validation loss: 5.41860771\n",
      "Iteration:  300\n",
      "Loss: 5.80828762\n",
      "Validation loss: 5.03245640\n",
      "Iteration:  400\n",
      "Loss: 5.84340048\n",
      "Validation loss: 4.39856434\n",
      "Iteration:  500\n",
      "Loss: 5.13826990\n",
      "Validation loss: 5.27742290\n",
      "Iteration:  600\n",
      "Loss: 4.63560009\n",
      "Validation loss: 4.77724075\n",
      "Iteration:  700\n",
      "Loss: 4.84565449\n",
      "Validation loss: 4.55789661\n",
      "Iteration:  800\n",
      "Loss: 5.03932762\n",
      "Validation loss: 5.13638353\n",
      "Iteration:  900\n",
      "Loss: 5.31932163\n",
      "Validation loss: 5.34674501\n",
      "Iteration: 1000\n",
      "Loss: 5.04793262\n",
      "Validation loss: 5.73913479\n",
      "Iteration: 1100\n",
      "Loss: 5.04252577\n",
      "Validation loss: 4.87506199\n",
      "Iteration: 1200\n",
      "Loss: 5.23706245\n",
      "Validation loss: 5.24279690\n",
      "Iteration: 1300\n",
      "Loss: 5.08675909\n",
      "Validation loss: 5.11178780\n",
      "Iteration: 1400\n",
      "Loss: 5.19071817\n",
      "Validation loss: 4.94801331\n",
      "Iteration: 1500\n",
      "Loss: 4.71531200\n",
      "Validation loss: 5.03131628\n",
      "Iteration: 1600\n",
      "Loss: 4.50171471\n",
      "Validation loss: 4.79599428\n",
      "Iteration: 1700\n",
      "Loss: 4.89006090\n",
      "Validation loss: 4.79471064\n",
      "Iteration: 1800\n",
      "Loss: 4.19512939\n",
      "Validation loss: 4.58207798\n",
      "Iteration: 1900\n",
      "Loss: 4.19780445\n",
      "Validation loss: 4.73796368\n",
      "Iteration: 2000\n",
      "Loss: 4.95351982\n",
      "Validation loss: 5.82398605\n",
      "Iteration: 2100\n",
      "Loss: 4.33236742\n",
      "Validation loss: 4.67496347\n",
      "Iteration: 2200\n",
      "Loss: 4.58822441\n",
      "Validation loss: 4.69011116\n",
      "Iteration: 2300\n",
      "Loss: 4.40652466\n",
      "Validation loss: 4.33996725\n",
      "Iteration: 2400\n",
      "Loss: 4.32081175\n",
      "Validation loss: 4.61894894\n",
      "Iteration: 2500\n",
      "Loss: 4.69216442\n",
      "Validation loss: 4.61296797\n",
      "Iteration: 2600\n",
      "Loss: 4.35689116\n",
      "Validation loss: 4.92131996\n",
      "Iteration: 2700\n",
      "Loss: 4.30371475\n",
      "Validation loss: 4.31412315\n",
      "Iteration: 2800\n",
      "Loss: 5.57762337\n",
      "Validation loss: 4.79447842\n",
      "Iteration: 2900\n",
      "Loss: 4.63994789\n",
      "Validation loss: 4.32604694\n",
      "Iteration: 3000\n",
      "Loss: 4.04725266\n",
      "Validation loss: 4.19692039\n",
      "Iteration: 3100\n",
      "Loss: 4.76873398\n",
      "Validation loss: 4.82172108\n",
      "Iteration: 3200\n",
      "Loss: 4.89210224\n",
      "Validation loss: 5.23893976\n",
      "Iteration: 3300\n",
      "Loss: 4.45898819\n",
      "Validation loss: 4.46196222\n",
      "Iteration: 3400\n",
      "Loss: 4.30495167\n",
      "Validation loss: 4.65800428\n",
      "Iteration: 3500\n",
      "Loss: 4.20039082\n",
      "Validation loss: 4.30416107\n",
      "Iteration: 3600\n",
      "Loss: 3.85057974\n",
      "Validation loss: 4.78954029\n",
      "Iteration: 3700\n",
      "Loss: 3.75306702\n",
      "Validation loss: 4.27439785\n",
      "Iteration: 3800\n",
      "Loss: 4.05260372\n",
      "Validation loss: 3.93319511\n",
      "Iteration: 3900\n",
      "Loss: 4.66528177\n",
      "Validation loss: 4.35584641\n",
      "Iteration: 4000\n",
      "Loss: 3.72256279\n",
      "Validation loss: 4.86792278\n",
      "Iteration: 4100\n",
      "Loss: 4.30725336\n",
      "Validation loss: 4.07820129\n",
      "Iteration: 4200\n",
      "Loss: 5.14338970\n",
      "Validation loss: 4.57930040\n",
      "Iteration: 4300\n",
      "Loss: 3.83282328\n",
      "Validation loss: 3.87365913\n",
      "Iteration: 4400\n",
      "Loss: 3.99852872\n",
      "Validation loss: 4.11070824\n",
      "Iteration: 4500\n",
      "Loss: 4.96438503\n",
      "Validation loss: 3.90635347\n",
      "Iteration: 4600\n",
      "Loss: 4.44001150\n",
      "Validation loss: 4.34198856\n",
      "Iteration: 4700\n",
      "Loss: 4.49856377\n",
      "Validation loss: 3.96083927\n",
      "Iteration: 4800\n",
      "Loss: 3.95898962\n",
      "Validation loss: 3.96438408\n",
      "Iteration: 4900\n",
      "Loss: 4.03040123\n",
      "Validation loss: 4.11821842\n",
      "Iteration: 5000\n",
      "Loss: 3.96288395\n",
      "Validation loss: 4.41383839\n",
      "Iteration: 5100\n",
      "Loss: 3.99433398\n",
      "Validation loss: 4.11090422\n",
      "Iteration: 5200\n",
      "Loss: 4.39606905\n",
      "Validation loss: 3.86017084\n",
      "Iteration: 5300\n",
      "Loss: 4.28406334\n",
      "Validation loss: 3.83496308\n",
      "Iteration: 5400\n",
      "Loss: 3.79445410\n",
      "Validation loss: 4.18136501\n",
      "Iteration: 5500\n",
      "Loss: 3.87783432\n",
      "Validation loss: 4.00780296\n",
      "Iteration: 5600\n",
      "Loss: 4.21500111\n",
      "Validation loss: 4.30333138\n",
      "Iteration: 5700\n",
      "Loss: 3.86873198\n",
      "Validation loss: 4.69447231\n",
      "Iteration: 5800\n",
      "Loss: 4.29000092\n",
      "Validation loss: 3.98506999\n",
      "Iteration: 5900\n",
      "Loss: 3.41040611\n",
      "Validation loss: 4.21413183\n",
      "Iteration: 6000\n",
      "Loss: 3.80721664\n",
      "Validation loss: 4.88531256\n",
      "Iteration: 6100\n",
      "Loss: 4.12490320\n",
      "Validation loss: 3.96923113\n",
      "Iteration: 6200\n",
      "Loss: 3.43027735\n",
      "Validation loss: 3.87645817\n",
      "Iteration: 6300\n",
      "Loss: 3.50750279\n",
      "Validation loss: 3.84433317\n",
      "Iteration: 6400\n",
      "Loss: 4.80678129\n",
      "Validation loss: 3.58290577\n",
      "Iteration: 6500\n",
      "Loss: 4.99589920\n",
      "Validation loss: 3.83664989\n",
      "Iteration: 6600\n",
      "Loss: 4.11958694\n",
      "Validation loss: 4.16589689\n",
      "Iteration: 6700\n",
      "Loss: 4.18866301\n",
      "Validation loss: 3.74449205\n",
      "Iteration: 6800\n",
      "Loss: 4.21914196\n",
      "Validation loss: 3.40128469\n",
      "Iteration: 6900\n",
      "Loss: 4.26908445\n",
      "Validation loss: 4.16087055\n",
      "Iteration: 7000\n",
      "Loss: 4.15506792\n",
      "Validation loss: 4.08639240\n",
      "Iteration: 7100\n",
      "Loss: 3.84102106\n",
      "Validation loss: 4.28511333\n",
      "Iteration: 7200\n",
      "Loss: 3.40163660\n",
      "Validation loss: 3.77152562\n",
      "Iteration: 7300\n",
      "Loss: 3.91749501\n",
      "Validation loss: 3.93195629\n",
      "Iteration: 7400\n",
      "Loss: 4.76510906\n",
      "Validation loss: 3.90872049\n",
      "Iteration: 7500\n",
      "Loss: 3.72059035\n",
      "Validation loss: 3.52112436\n",
      "Iteration: 7600\n",
      "Loss: 4.34086800\n",
      "Validation loss: 3.35659599\n",
      "Iteration: 7700\n",
      "Loss: 4.16174984\n",
      "Validation loss: 4.08742428\n",
      "Iteration: 7800\n",
      "Loss: 3.60956097\n",
      "Validation loss: 3.79180980\n",
      "Iteration: 7900\n",
      "Loss: 4.22902012\n",
      "Validation loss: 4.08749104\n",
      "Iteration: 8000\n",
      "Loss: 4.74254322\n",
      "Validation loss: 3.78764343\n",
      "Iteration: 8100\n",
      "Loss: 3.20514202\n",
      "Validation loss: 3.79247832\n",
      "Iteration: 8200\n",
      "Loss: 4.04472494\n",
      "Validation loss: 3.84664464\n",
      "Iteration: 8300\n",
      "Loss: 3.68433142\n",
      "Validation loss: 4.02558136\n",
      "Iteration: 8400\n",
      "Loss: 4.10586643\n",
      "Validation loss: 3.94429493\n",
      "Iteration: 8500\n",
      "Loss: 4.22795200\n",
      "Validation loss: 3.78988552\n",
      "Iteration: 8600\n",
      "Loss: 3.67885017\n",
      "Validation loss: 4.01694775\n",
      "Iteration: 8700\n",
      "Loss: 3.91259146\n",
      "Validation loss: 4.08661747\n",
      "Iteration: 8800\n",
      "Loss: 3.48386097\n",
      "Validation loss: 3.43440151\n",
      "Iteration: 8900\n",
      "Loss: 3.53458476\n",
      "Validation loss: 4.29904366\n",
      "Iteration: 9000\n",
      "Loss: 3.96882010\n",
      "Validation loss: 3.99370861\n",
      "Iteration: 9100\n",
      "Loss: 4.37639236\n",
      "Validation loss: 3.80892038\n",
      "Iteration: 9200\n",
      "Loss: 4.09134197\n",
      "Validation loss: 3.49280405\n",
      "Iteration: 9300\n",
      "Loss: 4.17453814\n",
      "Validation loss: 3.84119654\n",
      "Iteration: 9400\n",
      "Loss: 3.30152178\n",
      "Validation loss: 3.66464186\n",
      "Iteration: 9500\n",
      "Loss: 3.39400482\n",
      "Validation loss: 3.52900219\n",
      "Iteration: 9600\n",
      "Loss: 3.90335894\n",
      "Validation loss: 3.61220598\n",
      "Iteration: 9700\n",
      "Loss: 3.44674301\n",
      "Validation loss: 3.77937770\n",
      "Iteration: 9800\n",
      "Loss: 3.66880774\n",
      "Validation loss: 4.08049870\n",
      "Iteration: 9900\n",
      "Loss: 4.05560732\n",
      "Validation loss: 4.23148823\n",
      "Iteration: 10000\n",
      "Loss: 4.40550518\n",
      "Validation loss: 4.13718891\n",
      "Iteration: 10100\n",
      "Loss: 4.47720718\n",
      "Validation loss: 3.62154388\n",
      "Iteration: 10200\n",
      "Loss: 3.77763247\n",
      "Validation loss: 3.52283263\n",
      "Iteration: 10300\n",
      "Loss: 3.75094700\n",
      "Validation loss: 4.04001379\n",
      "Iteration: 10400\n",
      "Loss: 3.72217965\n",
      "Validation loss: 3.98510814\n",
      "Iteration: 10500\n",
      "Loss: 4.07977676\n",
      "Validation loss: 3.48834705\n",
      "Iteration: 10600\n",
      "Loss: 4.04080963\n",
      "Validation loss: 4.15785122\n",
      "Iteration: 10700\n",
      "Loss: 3.90692186\n",
      "Validation loss: 3.64265156\n",
      "Iteration: 10800\n",
      "Loss: 3.71338320\n",
      "Validation loss: 3.80933189\n",
      "Iteration: 10900\n",
      "Loss: 3.82371926\n",
      "Validation loss: 4.24206495\n",
      "Iteration: 11000\n",
      "Loss: 3.25247431\n",
      "Validation loss: 3.29231143\n",
      "Iteration: 11100\n",
      "Loss: 3.62344599\n",
      "Validation loss: 3.65822315\n",
      "Iteration: 11200\n",
      "Loss: 3.86487913\n",
      "Validation loss: 4.20817089\n",
      "Iteration: 11300\n",
      "Loss: 3.53572011\n",
      "Validation loss: 3.73480725\n",
      "Iteration: 11400\n",
      "Loss: 3.57879591\n",
      "Validation loss: 3.90017366\n",
      "Iteration: 11500\n",
      "Loss: 3.97638369\n",
      "Validation loss: 4.11776161\n",
      "Iteration: 11600\n",
      "Loss: 4.32253170\n",
      "Validation loss: 3.64929199\n",
      "Iteration: 11700\n",
      "Loss: 3.25612831\n",
      "Validation loss: 3.22064543\n",
      "Iteration: 11800\n",
      "Loss: 3.68324566\n",
      "Validation loss: 3.56073546\n",
      "Iteration: 11900\n",
      "Loss: 3.96060419\n",
      "Validation loss: 3.55450439\n",
      "Iteration: 12000\n",
      "Loss: 3.58410001\n",
      "Validation loss: 3.81227112\n",
      "Iteration: 12100\n",
      "Loss: 3.80716133\n",
      "Validation loss: 4.08145237\n",
      "Iteration: 12200\n",
      "Loss: 3.99339318\n",
      "Validation loss: 4.09332418\n",
      "Iteration: 12300\n",
      "Loss: 3.71346140\n",
      "Validation loss: 3.60345411\n",
      "Iteration: 12400\n",
      "Loss: 3.91483879\n",
      "Validation loss: 3.35093212\n",
      "Iteration: 12500\n",
      "Loss: 3.42925692\n",
      "Validation loss: 3.84672809\n",
      "Iteration: 12600\n",
      "Loss: 3.55850744\n",
      "Validation loss: 3.36437082\n",
      "Iteration: 12700\n",
      "Loss: 3.17401695\n",
      "Validation loss: 3.47217250\n",
      "Iteration: 12800\n",
      "Loss: 3.17750359\n",
      "Validation loss: 3.11922741\n",
      "Iteration: 12900\n",
      "Loss: 3.54466534\n",
      "Validation loss: 4.01100540\n",
      "Iteration: 13000\n",
      "Loss: 4.04336596\n",
      "Validation loss: 3.26097894\n",
      "Iteration: 13100\n",
      "Loss: 3.41452646\n",
      "Validation loss: 3.53294373\n",
      "Iteration: 13200\n",
      "Loss: 3.34074807\n",
      "Validation loss: 3.77225161\n",
      "Iteration: 13300\n",
      "Loss: 3.44056439\n",
      "Validation loss: 3.28738976\n",
      "Iteration: 13400\n",
      "Loss: 3.59216905\n",
      "Validation loss: 3.26083708\n",
      "Iteration: 13500\n",
      "Loss: 3.56247115\n",
      "Validation loss: 3.52478027\n",
      "Iteration: 13600\n",
      "Loss: 3.04970503\n",
      "Validation loss: 3.58055902\n",
      "Iteration: 13700\n",
      "Loss: 3.12849927\n",
      "Validation loss: 3.33802176\n",
      "Iteration: 13800\n",
      "Loss: 3.67373538\n",
      "Validation loss: 3.39858222\n",
      "Iteration: 13900\n",
      "Loss: 3.13433433\n",
      "Validation loss: 3.66802001\n",
      "Iteration: 14000\n",
      "Loss: 3.49145126\n",
      "Validation loss: 3.17805767\n",
      "Iteration: 14100\n",
      "Loss: 3.34963250\n",
      "Validation loss: 3.74044013\n",
      "Iteration: 14200\n",
      "Loss: 3.32068396\n",
      "Validation loss: 3.09307480\n",
      "Iteration: 14300\n",
      "Loss: 3.76066852\n",
      "Validation loss: 3.29416990\n",
      "Iteration: 14400\n",
      "Loss: 4.37254381\n",
      "Validation loss: 3.39994740\n",
      "Iteration: 14500\n",
      "Loss: 3.47912335\n",
      "Validation loss: 3.44028234\n",
      "Iteration: 14600\n",
      "Loss: 4.11102009\n",
      "Validation loss: 3.48363495\n",
      "Iteration: 14700\n",
      "Loss: 3.64288235\n",
      "Validation loss: 3.50898743\n",
      "Iteration: 14800\n",
      "Loss: 3.40228462\n",
      "Validation loss: 3.54535437\n",
      "Iteration: 14900\n",
      "Loss: 3.31416559\n",
      "Validation loss: 3.13746381\n",
      "Iteration: 15000\n",
      "Loss: 3.08067703\n",
      "Validation loss: 3.11159730\n",
      "Iteration: 15100\n",
      "Loss: 3.42163992\n",
      "Validation loss: 2.86341810\n",
      "Iteration: 15200\n",
      "Loss: 4.23310661\n",
      "Validation loss: 3.95865154\n",
      "Iteration: 15300\n",
      "Loss: 3.25702620\n",
      "Validation loss: 3.00269961\n",
      "Iteration: 15344\n",
      "Loss: 3.05392694\n",
      "Validation loss: 3.41323423\n",
      "Epoch: 2\n",
      "Iteration:    1\n",
      "Loss: 3.34604692\n",
      "Validation loss: 3.12295008\n",
      "Iteration:  100\n",
      "Loss: 3.27775097\n",
      "Validation loss: 2.81904125\n",
      "Iteration:  200\n",
      "Loss: 2.73642921\n",
      "Validation loss: 3.23641658\n",
      "Iteration:  300\n",
      "Loss: 3.09886789\n",
      "Validation loss: 3.53430820\n",
      "Iteration:  400\n",
      "Loss: 3.88010931\n",
      "Validation loss: 2.66534042\n",
      "Iteration:  500\n",
      "Loss: 2.88156366\n",
      "Validation loss: 2.96754384\n",
      "Iteration:  600\n",
      "Loss: 2.80915976\n",
      "Validation loss: 2.62900591\n",
      "Iteration:  700\n",
      "Loss: 2.75501490\n",
      "Validation loss: 3.07928419\n",
      "Iteration:  800\n",
      "Loss: 2.85451865\n",
      "Validation loss: 2.92259407\n",
      "Iteration:  900\n",
      "Loss: 3.08526945\n",
      "Validation loss: 1.86236548\n",
      "Iteration: 1000\n",
      "Loss: 1.81397378\n",
      "Validation loss: 1.68440664\n",
      "Iteration: 1100\n",
      "Loss: 2.11747932\n",
      "Validation loss: 2.50696731\n",
      "Iteration: 1200\n",
      "Loss: 2.58498669\n",
      "Validation loss: 2.55863857\n",
      "Iteration: 1300\n",
      "Loss: 2.26501131\n",
      "Validation loss: 2.69839573\n",
      "Iteration: 1400\n",
      "Loss: 1.87940526\n",
      "Validation loss: 1.79400980\n",
      "Iteration: 1500\n",
      "Loss: 2.20270991\n",
      "Validation loss: 2.42471433\n",
      "Iteration: 1600\n",
      "Loss: 1.70435500\n",
      "Validation loss: 1.59455013\n",
      "Iteration: 1700\n",
      "Loss: 2.58952236\n",
      "Validation loss: 1.45900607\n",
      "Iteration: 1800\n",
      "Loss: 2.74270463\n",
      "Validation loss: 1.47263265\n",
      "Iteration: 1900\n",
      "Loss: 3.12533379\n",
      "Validation loss: 1.86329973\n",
      "Iteration: 2000\n",
      "Loss: 2.37029648\n",
      "Validation loss: 2.33885384\n",
      "Iteration: 2100\n",
      "Loss: 1.75183821\n",
      "Validation loss: 2.11697245\n",
      "Iteration: 2200\n",
      "Loss: 1.25876892\n",
      "Validation loss: 1.94287515\n",
      "Iteration: 2300\n",
      "Loss: 1.60330784\n",
      "Validation loss: 2.44806790\n",
      "Iteration: 2400\n",
      "Loss: 2.00995183\n",
      "Validation loss: 1.33076096\n",
      "Iteration: 2500\n",
      "Loss: 1.67236662\n",
      "Validation loss: 2.22758055\n",
      "Iteration: 2600\n",
      "Loss: 1.43722868\n",
      "Validation loss: 1.60834980\n",
      "Iteration: 2700\n",
      "Loss: 2.37920451\n",
      "Validation loss: 1.54265201\n",
      "Iteration: 2800\n",
      "Loss: 1.55373418\n",
      "Validation loss: 1.49414957\n",
      "Iteration: 2900\n",
      "Loss: 1.37139046\n",
      "Validation loss: 1.75708103\n",
      "Iteration: 3000\n",
      "Loss: 1.30519474\n",
      "Validation loss: 1.23639917\n",
      "Iteration: 3100\n",
      "Loss: 0.71380293\n",
      "Validation loss: 0.94920570\n",
      "Iteration: 3200\n",
      "Loss: 1.61681533\n",
      "Validation loss: 1.17065465\n",
      "Iteration: 3300\n",
      "Loss: 1.18327498\n",
      "Validation loss: 2.05688739\n",
      "Iteration: 3400\n",
      "Loss: 1.49479270\n",
      "Validation loss: 0.96522218\n",
      "Iteration: 3500\n",
      "Loss: 1.16650033\n",
      "Validation loss: 1.46865702\n",
      "Iteration: 3600\n",
      "Loss: 1.74819398\n",
      "Validation loss: 0.98947251\n",
      "Iteration: 3700\n",
      "Loss: 0.81881154\n",
      "Validation loss: 0.59995997\n",
      "Iteration: 3800\n",
      "Loss: 1.62195325\n",
      "Validation loss: 1.80637717\n",
      "Iteration: 3900\n",
      "Loss: 1.07744730\n",
      "Validation loss: 1.39610338\n",
      "Iteration: 4000\n",
      "Loss: 0.49220991\n",
      "Validation loss: 1.51333964\n",
      "Iteration: 4100\n",
      "Loss: 1.67313385\n",
      "Validation loss: 1.55217052\n",
      "Iteration: 4200\n",
      "Loss: 0.63393420\n",
      "Validation loss: 0.90484208\n",
      "Iteration: 4300\n",
      "Loss: 0.56548566\n",
      "Validation loss: 1.11477137\n",
      "Iteration: 4400\n",
      "Loss: 1.43113756\n",
      "Validation loss: 0.88600129\n",
      "Iteration: 4500\n",
      "Loss: 2.63190198\n",
      "Validation loss: 0.84851140\n",
      "Iteration: 4600\n",
      "Loss: 2.01193190\n",
      "Validation loss: 1.30290067\n",
      "Iteration: 4700\n",
      "Loss: 1.49674666\n",
      "Validation loss: 1.37597573\n",
      "Iteration: 4800\n",
      "Loss: 1.00110650\n",
      "Validation loss: 0.88206154\n",
      "Iteration: 4900\n",
      "Loss: 0.83083677\n",
      "Validation loss: 0.73349249\n",
      "Iteration: 5000\n",
      "Loss: 0.66426438\n",
      "Validation loss: 0.73084688\n",
      "Iteration: 5100\n",
      "Loss: 0.84395349\n",
      "Validation loss: 0.64740211\n",
      "Iteration: 5200\n",
      "Loss: 2.05194283\n",
      "Validation loss: 0.95717114\n",
      "Iteration: 5300\n",
      "Loss: 2.31578279\n",
      "Validation loss: 0.74436742\n",
      "Iteration: 5400\n",
      "Loss: 1.44523406\n",
      "Validation loss: 0.98360670\n",
      "Iteration: 5500\n",
      "Loss: 1.12367094\n",
      "Validation loss: 1.43986988\n",
      "Iteration: 5600\n",
      "Loss: 1.20578289\n",
      "Validation loss: 1.59769416\n",
      "Iteration: 5700\n",
      "Loss: 2.64394188\n",
      "Validation loss: 3.17234516\n",
      "Iteration: 5800\n",
      "Loss: 0.91639024\n",
      "Validation loss: 1.55525100\n",
      "Iteration: 5900\n",
      "Loss: 1.11688626\n",
      "Validation loss: 1.54763412\n",
      "Iteration: 6000\n",
      "Loss: 1.35282457\n",
      "Validation loss: 1.04667699\n",
      "Iteration: 6100\n",
      "Loss: 0.69771487\n",
      "Validation loss: 0.58767223\n",
      "Iteration: 6200\n",
      "Loss: 0.61522663\n",
      "Validation loss: 1.19652593\n",
      "Iteration: 6300\n",
      "Loss: 0.68473083\n",
      "Validation loss: 2.01446581\n",
      "Iteration: 6400\n",
      "Loss: 1.65294206\n",
      "Validation loss: 1.16355157\n",
      "Iteration: 6500\n",
      "Loss: 1.04301155\n",
      "Validation loss: 1.58980656\n",
      "Iteration: 6600\n",
      "Loss: 0.86014968\n",
      "Validation loss: 1.74764109\n",
      "Iteration: 6700\n",
      "Loss: 1.07589161\n",
      "Validation loss: 1.26459098\n",
      "Iteration: 6800\n",
      "Loss: 1.23098874\n",
      "Validation loss: 1.06330776\n",
      "Iteration: 6900\n",
      "Loss: 1.39829648\n",
      "Validation loss: 0.92622268\n",
      "Iteration: 7000\n",
      "Loss: 0.91936046\n",
      "Validation loss: 0.71475071\n",
      "Iteration: 7100\n",
      "Loss: 1.10820472\n",
      "Validation loss: 0.85376501\n",
      "Iteration: 7200\n",
      "Loss: 0.50256801\n",
      "Validation loss: 0.84802848\n",
      "Iteration: 7300\n",
      "Loss: 1.11824656\n",
      "Validation loss: 0.55136406\n",
      "Iteration: 7400\n",
      "Loss: 1.20330691\n",
      "Validation loss: 1.22075248\n",
      "Iteration: 7500\n",
      "Loss: 0.62243378\n",
      "Validation loss: 1.34739959\n",
      "Iteration: 7600\n",
      "Loss: 1.30052221\n",
      "Validation loss: 2.40255761\n",
      "Iteration: 7700\n",
      "Loss: 0.67290103\n",
      "Validation loss: 1.76613557\n",
      "Iteration: 7800\n",
      "Loss: 0.52476633\n",
      "Validation loss: 1.11856699\n",
      "Iteration: 7900\n",
      "Loss: 0.47966042\n",
      "Validation loss: 0.72302538\n",
      "Iteration: 8000\n",
      "Loss: 1.20015240\n",
      "Validation loss: 0.60455745\n",
      "Iteration: 8100\n",
      "Loss: 0.79436785\n",
      "Validation loss: 0.52320570\n",
      "Iteration: 8200\n",
      "Loss: 1.40493727\n",
      "Validation loss: 1.32399344\n",
      "Iteration: 8300\n",
      "Loss: 0.76256162\n",
      "Validation loss: 1.27863503\n",
      "Iteration: 8400\n",
      "Loss: 0.59063250\n",
      "Validation loss: 0.68885165\n",
      "Iteration: 8500\n",
      "Loss: 1.67283571\n",
      "Validation loss: 1.16478455\n",
      "Iteration: 8600\n",
      "Loss: 0.80800378\n",
      "Validation loss: 0.69572520\n",
      "Iteration: 8700\n",
      "Loss: 0.55277818\n",
      "Validation loss: 1.98623395\n",
      "Iteration: 8800\n",
      "Loss: 0.50267017\n",
      "Validation loss: 1.26452494\n",
      "Iteration: 8900\n",
      "Loss: 0.74920011\n",
      "Validation loss: 0.89002210\n",
      "Iteration: 9000\n",
      "Loss: 0.87251353\n",
      "Validation loss: 1.21865129\n",
      "Iteration: 9100\n",
      "Loss: 0.78472656\n",
      "Validation loss: 0.80239779\n",
      "Iteration: 9200\n",
      "Loss: 0.78810596\n",
      "Validation loss: 1.04811966\n",
      "Iteration: 9300\n",
      "Loss: 0.93435127\n",
      "Validation loss: 1.11860073\n",
      "Iteration: 9400\n",
      "Loss: 0.80568212\n",
      "Validation loss: 0.73705673\n",
      "Iteration: 9500\n",
      "Loss: 1.28618670\n",
      "Validation loss: 1.34453821\n",
      "Iteration: 9600\n",
      "Loss: 0.88001239\n",
      "Validation loss: 0.87122035\n",
      "Iteration: 9700\n",
      "Loss: 1.10931313\n",
      "Validation loss: 2.36981010\n",
      "Iteration: 9800\n",
      "Loss: 0.38376170\n",
      "Validation loss: 0.81095791\n",
      "Iteration: 9900\n",
      "Loss: 0.94944423\n",
      "Validation loss: 0.47642884\n",
      "Iteration: 10000\n",
      "Loss: 0.31643653\n",
      "Validation loss: 0.67433536\n",
      "Iteration: 10100\n",
      "Loss: 1.34615827\n",
      "Validation loss: 0.39565372\n",
      "Iteration: 10200\n",
      "Loss: 0.73666942\n",
      "Validation loss: 0.75369084\n",
      "Iteration: 10300\n",
      "Loss: 0.65337181\n",
      "Validation loss: 1.08004642\n",
      "Iteration: 10400\n",
      "Loss: 1.07615626\n",
      "Validation loss: 0.45405218\n",
      "Iteration: 10500\n",
      "Loss: 1.67397904\n",
      "Validation loss: 1.04829049\n",
      "Iteration: 10600\n",
      "Loss: 0.81469542\n",
      "Validation loss: 0.65105641\n",
      "Iteration: 10700\n",
      "Loss: 1.04425180\n",
      "Validation loss: 1.02095330\n",
      "Iteration: 10800\n",
      "Loss: 0.79379439\n",
      "Validation loss: 0.70987695\n",
      "Iteration: 10900\n",
      "Loss: 0.81587005\n",
      "Validation loss: 1.18423533\n",
      "Iteration: 11000\n",
      "Loss: 0.90640116\n",
      "Validation loss: 0.65466535\n",
      "Iteration: 11100\n",
      "Loss: 0.87068504\n",
      "Validation loss: 0.47269034\n",
      "Iteration: 11200\n",
      "Loss: 0.53908163\n",
      "Validation loss: 0.57186151\n",
      "Iteration: 11300\n",
      "Loss: 0.48062426\n",
      "Validation loss: 0.62090158\n",
      "Iteration: 11400\n",
      "Loss: 0.55893975\n",
      "Validation loss: 2.32878232\n",
      "Iteration: 11500\n",
      "Loss: 0.73679590\n",
      "Validation loss: 0.88155127\n",
      "Iteration: 11600\n",
      "Loss: 1.30847669\n",
      "Validation loss: 0.36741197\n",
      "Iteration: 11700\n",
      "Loss: 0.92146856\n",
      "Validation loss: 0.84567809\n",
      "Iteration: 11800\n",
      "Loss: 0.48915195\n",
      "Validation loss: 1.12266254\n",
      "Iteration: 11900\n",
      "Loss: 0.42762730\n",
      "Validation loss: 0.65671313\n",
      "Iteration: 12000\n",
      "Loss: 0.32187402\n",
      "Validation loss: 0.64645565\n",
      "Iteration: 12100\n",
      "Loss: 0.90836072\n",
      "Validation loss: 0.71819592\n",
      "Iteration: 12200\n",
      "Loss: 0.88738179\n",
      "Validation loss: 1.11871469\n",
      "Iteration: 12300\n",
      "Loss: 1.02416706\n",
      "Validation loss: 0.71440208\n",
      "Iteration: 12400\n",
      "Loss: 0.95852387\n",
      "Validation loss: 0.32765457\n",
      "Iteration: 12500\n",
      "Loss: 0.66201103\n",
      "Validation loss: 0.84488821\n",
      "Iteration: 12600\n",
      "Loss: 0.19477679\n",
      "Validation loss: 1.02568722\n",
      "Iteration: 12700\n",
      "Loss: 0.40121359\n",
      "Validation loss: 0.42659324\n",
      "Iteration: 12800\n",
      "Loss: 1.04016697\n",
      "Validation loss: 0.60158110\n",
      "Iteration: 12900\n",
      "Loss: 0.62645078\n",
      "Validation loss: 0.49521604\n",
      "Iteration: 13000\n",
      "Loss: 1.30240798\n",
      "Validation loss: 0.47312707\n",
      "Iteration: 13100\n",
      "Loss: 0.75029486\n",
      "Validation loss: 0.73201430\n",
      "Iteration: 13200\n",
      "Loss: 0.70411694\n",
      "Validation loss: 0.69436848\n",
      "Iteration: 13300\n",
      "Loss: 0.54850638\n",
      "Validation loss: 0.79089433\n",
      "Iteration: 13400\n",
      "Loss: 2.05204511\n",
      "Validation loss: 1.15310264\n",
      "Iteration: 13500\n",
      "Loss: 0.49469149\n",
      "Validation loss: 0.93777031\n",
      "Iteration: 13600\n",
      "Loss: 0.69682866\n",
      "Validation loss: 0.43200433\n",
      "Iteration: 13700\n",
      "Loss: 0.64008522\n",
      "Validation loss: 0.35363179\n",
      "Iteration: 13800\n",
      "Loss: 0.63372612\n",
      "Validation loss: 0.99902523\n",
      "Iteration: 13900\n",
      "Loss: 0.25444630\n",
      "Validation loss: 0.60754752\n",
      "Iteration: 14000\n",
      "Loss: 0.75962007\n",
      "Validation loss: 0.47136676\n",
      "Iteration: 14100\n",
      "Loss: 0.52179885\n",
      "Validation loss: 1.04110765\n",
      "Iteration: 14200\n",
      "Loss: 0.76811641\n",
      "Validation loss: 0.42335856\n",
      "Iteration: 14300\n",
      "Loss: 1.46984470\n",
      "Validation loss: 0.67268819\n",
      "Iteration: 14400\n",
      "Loss: 0.49277872\n",
      "Validation loss: 0.33578935\n",
      "Iteration: 14500\n",
      "Loss: 0.39905867\n",
      "Validation loss: 0.27438906\n",
      "Iteration: 14600\n",
      "Loss: 0.75500727\n",
      "Validation loss: 0.56933618\n",
      "Iteration: 14700\n",
      "Loss: 0.47759575\n",
      "Validation loss: 0.86547816\n",
      "Iteration: 14800\n",
      "Loss: 1.46306264\n",
      "Validation loss: 0.94455832\n",
      "Iteration: 14900\n",
      "Loss: 0.26489350\n",
      "Validation loss: 0.48998210\n",
      "Iteration: 15000\n",
      "Loss: 1.19822228\n",
      "Validation loss: 0.43788457\n",
      "Iteration: 15100\n",
      "Loss: 0.70417565\n",
      "Validation loss: 0.41705894\n",
      "Iteration: 15200\n",
      "Loss: 0.59767699\n",
      "Validation loss: 0.33614478\n",
      "Iteration: 15300\n",
      "Loss: 0.62138081\n",
      "Validation loss: 0.53176582\n",
      "Iteration: 15344\n",
      "Loss: 0.55880040\n",
      "Validation loss: 1.48503554\n",
      "Epoch: 3\n",
      "Iteration:    1\n",
      "Loss: 1.25835061\n",
      "Validation loss: 0.62259710\n",
      "Iteration:  100\n",
      "Loss: 0.58855581\n",
      "Validation loss: 0.31327552\n",
      "Iteration:  200\n",
      "Loss: 0.38773537\n",
      "Validation loss: 0.97326422\n",
      "Iteration:  300\n",
      "Loss: 0.54658210\n",
      "Validation loss: 0.47169101\n",
      "Iteration:  400\n",
      "Loss: 0.32227248\n",
      "Validation loss: 0.94951439\n",
      "Iteration:  500\n",
      "Loss: 0.49381006\n",
      "Validation loss: 0.41129065\n",
      "Iteration:  600\n",
      "Loss: 0.75689387\n",
      "Validation loss: 0.65382862\n",
      "Iteration:  700\n",
      "Loss: 0.38754717\n",
      "Validation loss: 0.54906040\n",
      "Iteration:  800\n",
      "Loss: 0.33909151\n",
      "Validation loss: 0.86700296\n",
      "Iteration:  900\n",
      "Loss: 0.98617256\n",
      "Validation loss: 0.52463537\n",
      "Iteration: 1000\n",
      "Loss: 0.59663367\n",
      "Validation loss: 0.71433794\n",
      "Iteration: 1100\n",
      "Loss: 1.13665080\n",
      "Validation loss: 0.25287256\n",
      "Iteration: 1200\n",
      "Loss: 0.45937568\n",
      "Validation loss: 0.27857876\n",
      "Iteration: 1300\n",
      "Loss: 1.30436468\n",
      "Validation loss: 0.71248662\n",
      "Iteration: 1400\n",
      "Loss: 0.41037577\n",
      "Validation loss: 0.62271577\n",
      "Iteration: 1500\n",
      "Loss: 0.99746287\n",
      "Validation loss: 0.61478776\n",
      "Iteration: 1600\n",
      "Loss: 0.87397599\n",
      "Validation loss: 0.42604965\n",
      "Iteration: 1700\n",
      "Loss: 1.17650378\n",
      "Validation loss: 0.56068337\n",
      "Iteration: 1800\n",
      "Loss: 0.46807253\n",
      "Validation loss: 0.48937038\n",
      "Iteration: 1900\n",
      "Loss: 0.39881191\n",
      "Validation loss: 0.76079726\n",
      "Iteration: 2000\n",
      "Loss: 1.24173594\n",
      "Validation loss: 0.37212425\n",
      "Iteration: 2100\n",
      "Loss: 0.59699655\n",
      "Validation loss: 1.34387350\n",
      "Iteration: 2200\n",
      "Loss: 0.41200048\n",
      "Validation loss: 0.43001929\n",
      "Iteration: 2300\n",
      "Loss: 0.41763014\n",
      "Validation loss: 0.98947996\n",
      "Iteration: 2400\n",
      "Loss: 0.63279229\n",
      "Validation loss: 0.72213143\n",
      "Iteration: 2500\n",
      "Loss: 1.38607407\n",
      "Validation loss: 0.29357666\n",
      "Iteration: 2600\n",
      "Loss: 0.73746324\n",
      "Validation loss: 0.61168611\n",
      "Iteration: 2700\n",
      "Loss: 0.95897830\n",
      "Validation loss: 0.60169512\n",
      "Iteration: 2800\n",
      "Loss: 0.61019057\n",
      "Validation loss: 0.50060868\n",
      "Iteration: 2900\n",
      "Loss: 0.33191955\n",
      "Validation loss: 2.04144120\n",
      "Iteration: 3000\n",
      "Loss: 0.51148760\n",
      "Validation loss: 0.82478082\n",
      "Iteration: 3100\n",
      "Loss: 2.17701840\n",
      "Validation loss: 0.69336206\n",
      "Iteration: 3200\n",
      "Loss: 0.85688043\n",
      "Validation loss: 0.75128525\n",
      "Iteration: 3300\n",
      "Loss: 0.36995566\n",
      "Validation loss: 0.44829077\n",
      "Iteration: 3400\n",
      "Loss: 0.91246808\n",
      "Validation loss: 0.67774713\n",
      "Iteration: 3500\n",
      "Loss: 0.72529149\n",
      "Validation loss: 1.13220167\n",
      "Iteration: 3600\n",
      "Loss: 0.84576219\n",
      "Validation loss: 0.82505584\n",
      "Iteration: 3700\n",
      "Loss: 0.22225745\n",
      "Validation loss: 0.55655146\n",
      "Iteration: 3800\n",
      "Loss: 0.89259917\n",
      "Validation loss: 0.63786876\n",
      "Iteration: 3900\n",
      "Loss: 0.48786831\n",
      "Validation loss: 0.62203258\n",
      "Iteration: 4000\n",
      "Loss: 0.37049225\n",
      "Validation loss: 0.39304709\n",
      "Iteration: 4100\n",
      "Loss: 0.38674864\n",
      "Validation loss: 0.31684574\n",
      "Iteration: 4200\n",
      "Loss: 0.43108618\n",
      "Validation loss: 0.74714077\n",
      "Iteration: 4300\n",
      "Loss: 0.56717336\n",
      "Validation loss: 1.50714850\n",
      "Iteration: 4400\n",
      "Loss: 1.01937592\n",
      "Validation loss: 0.43615723\n",
      "Iteration: 4500\n",
      "Loss: 0.71660316\n",
      "Validation loss: 0.29881597\n",
      "Iteration: 4600\n",
      "Loss: 0.70370519\n",
      "Validation loss: 0.49805066\n",
      "Iteration: 4700\n",
      "Loss: 0.62122786\n",
      "Validation loss: 0.16409238\n",
      "Iteration: 4800\n",
      "Loss: 0.45549488\n",
      "Validation loss: 0.41959670\n",
      "Iteration: 4900\n",
      "Loss: 0.16848852\n",
      "Validation loss: 0.54202777\n",
      "Iteration: 5000\n",
      "Loss: 1.04969072\n",
      "Validation loss: 0.64011979\n",
      "Iteration: 5100\n",
      "Loss: 0.42122108\n",
      "Validation loss: 0.45686936\n",
      "Iteration: 5200\n",
      "Loss: 0.32253948\n",
      "Validation loss: 0.86896366\n",
      "Iteration: 5300\n",
      "Loss: 0.98265123\n",
      "Validation loss: 0.35190582\n",
      "Iteration: 5400\n",
      "Loss: 0.81002140\n",
      "Validation loss: 0.23143843\n",
      "Iteration: 5500\n",
      "Loss: 0.38842058\n",
      "Validation loss: 0.37412429\n",
      "Iteration: 5600\n",
      "Loss: 0.68439466\n",
      "Validation loss: 0.75618958\n",
      "Iteration: 5700\n",
      "Loss: 0.69286746\n",
      "Validation loss: 0.58099735\n",
      "Iteration: 5800\n",
      "Loss: 0.33366036\n",
      "Validation loss: 0.50303423\n",
      "Iteration: 5900\n",
      "Loss: 0.62104326\n",
      "Validation loss: 0.43256980\n",
      "Iteration: 6000\n",
      "Loss: 0.70065141\n",
      "Validation loss: 0.32166931\n",
      "Iteration: 6100\n",
      "Loss: 0.25715798\n",
      "Validation loss: 0.76412225\n",
      "Iteration: 6200\n",
      "Loss: 0.53238297\n",
      "Validation loss: 0.43494385\n",
      "Iteration: 6300\n",
      "Loss: 0.53912497\n",
      "Validation loss: 0.39211231\n",
      "Iteration: 6400\n",
      "Loss: 0.43801332\n",
      "Validation loss: 0.53494132\n",
      "Iteration: 6500\n",
      "Loss: 0.19795693\n",
      "Validation loss: 0.53180468\n",
      "Iteration: 6600\n",
      "Loss: 0.25843850\n",
      "Validation loss: 0.99650270\n",
      "Iteration: 6700\n",
      "Loss: 0.33233732\n",
      "Validation loss: 0.16778988\n",
      "Iteration: 6800\n",
      "Loss: 0.23563370\n",
      "Validation loss: 0.35709417\n",
      "Iteration: 6900\n",
      "Loss: 0.95043588\n",
      "Validation loss: 0.52324110\n",
      "Iteration: 7000\n",
      "Loss: 0.58413792\n",
      "Validation loss: 0.28242943\n",
      "Iteration: 7100\n",
      "Loss: 0.64579731\n",
      "Validation loss: 0.66081452\n",
      "Iteration: 7200\n",
      "Loss: 0.53395915\n",
      "Validation loss: 0.14995858\n",
      "Iteration: 7300\n",
      "Loss: 0.31619859\n",
      "Validation loss: 1.17958188\n",
      "Iteration: 7400\n",
      "Loss: 0.57289839\n",
      "Validation loss: 0.34078759\n",
      "Iteration: 7500\n",
      "Loss: 0.39305720\n",
      "Validation loss: 1.34232652\n",
      "Iteration: 7600\n",
      "Loss: 0.37611729\n",
      "Validation loss: 0.36320627\n",
      "Iteration: 7700\n",
      "Loss: 0.65609753\n",
      "Validation loss: 0.09462924\n",
      "Iteration: 7800\n",
      "Loss: 0.53970128\n",
      "Validation loss: 0.34213185\n",
      "Iteration: 7900\n",
      "Loss: 0.21230941\n",
      "Validation loss: 0.79373854\n",
      "Iteration: 8000\n",
      "Loss: 0.98253298\n",
      "Validation loss: 0.52537197\n",
      "Iteration: 8100\n",
      "Loss: 0.36256537\n",
      "Validation loss: 0.26633209\n",
      "Iteration: 8200\n",
      "Loss: 0.33069134\n",
      "Validation loss: 0.37819448\n",
      "Iteration: 8300\n",
      "Loss: 0.86557889\n",
      "Validation loss: 0.70685560\n",
      "Iteration: 8400\n",
      "Loss: 0.42515793\n",
      "Validation loss: 0.51655346\n",
      "Iteration: 8500\n",
      "Loss: 1.03341913\n",
      "Validation loss: 0.20673296\n",
      "Iteration: 8600\n",
      "Loss: 1.52933204\n",
      "Validation loss: 0.74630201\n",
      "Iteration: 8700\n",
      "Loss: 0.27826685\n",
      "Validation loss: 0.42042333\n",
      "Iteration: 8800\n",
      "Loss: 0.48608509\n",
      "Validation loss: 0.32035553\n",
      "Iteration: 8900\n",
      "Loss: 2.22575355\n",
      "Validation loss: 1.06228507\n",
      "Iteration: 9000\n",
      "Loss: 0.41216883\n",
      "Validation loss: 0.36745018\n",
      "Iteration: 9100\n",
      "Loss: 0.33885622\n",
      "Validation loss: 0.72271007\n",
      "Iteration: 9200\n",
      "Loss: 0.60254407\n",
      "Validation loss: 0.28736335\n",
      "Iteration: 9300\n",
      "Loss: 0.57150245\n",
      "Validation loss: 0.66027105\n",
      "Iteration: 9400\n",
      "Loss: 0.47260207\n",
      "Validation loss: 0.49436256\n",
      "Iteration: 9500\n",
      "Loss: 0.58393240\n",
      "Validation loss: 0.52416086\n",
      "Iteration: 9600\n",
      "Loss: 0.48499930\n",
      "Validation loss: 0.59575319\n",
      "Iteration: 9700\n",
      "Loss: 0.58289778\n",
      "Validation loss: 0.42809245\n",
      "Iteration: 9800\n",
      "Loss: 0.50141639\n",
      "Validation loss: 0.95530921\n",
      "Iteration: 9900\n",
      "Loss: 0.46663266\n",
      "Validation loss: 0.30944598\n",
      "Iteration: 10000\n",
      "Loss: 0.31520221\n",
      "Validation loss: 0.40859890\n",
      "Iteration: 10100\n",
      "Loss: 0.61240613\n",
      "Validation loss: 0.10831563\n",
      "Iteration: 10200\n",
      "Loss: 0.37841973\n",
      "Validation loss: 0.80619460\n",
      "Iteration: 10300\n",
      "Loss: 0.47675455\n",
      "Validation loss: 0.34422809\n",
      "Iteration: 10400\n",
      "Loss: 0.35863048\n",
      "Validation loss: 0.24589390\n",
      "Iteration: 10500\n",
      "Loss: 1.22714996\n",
      "Validation loss: 0.60244966\n",
      "Iteration: 10600\n",
      "Loss: 0.22383210\n",
      "Validation loss: 0.30671820\n",
      "Iteration: 10700\n",
      "Loss: 0.74398464\n",
      "Validation loss: 1.26684904\n",
      "Iteration: 10800\n",
      "Loss: 1.02010846\n",
      "Validation loss: 0.30283147\n",
      "Iteration: 10900\n",
      "Loss: 0.93976951\n",
      "Validation loss: 0.48469043\n",
      "Iteration: 11000\n",
      "Loss: 0.42241907\n",
      "Validation loss: 0.74698305\n",
      "Iteration: 11100\n",
      "Loss: 0.31872341\n",
      "Validation loss: 0.28998688\n",
      "Iteration: 11200\n",
      "Loss: 0.45188028\n",
      "Validation loss: 0.44653544\n",
      "Iteration: 11300\n",
      "Loss: 0.17799287\n",
      "Validation loss: 0.62566608\n",
      "Iteration: 11400\n",
      "Loss: 0.17065729\n",
      "Validation loss: 0.64959639\n",
      "Iteration: 11500\n",
      "Loss: 0.66525507\n",
      "Validation loss: 0.39178771\n",
      "Iteration: 11600\n",
      "Loss: 0.92806095\n",
      "Validation loss: 0.64325303\n",
      "Iteration: 11700\n",
      "Loss: 0.45568210\n",
      "Validation loss: 0.58535975\n",
      "Iteration: 11800\n",
      "Loss: 0.27491617\n",
      "Validation loss: 0.62591326\n",
      "Iteration: 11900\n",
      "Loss: 0.49674398\n",
      "Validation loss: 0.37570637\n",
      "Iteration: 12000\n",
      "Loss: 0.32108226\n",
      "Validation loss: 0.38878036\n",
      "Iteration: 12100\n",
      "Loss: 0.35447752\n",
      "Validation loss: 0.42882860\n",
      "Iteration: 12200\n",
      "Loss: 0.38865706\n",
      "Validation loss: 0.41113055\n",
      "Iteration: 12300\n",
      "Loss: 0.49383259\n",
      "Validation loss: 0.35599500\n",
      "Iteration: 12400\n",
      "Loss: 0.68008637\n",
      "Validation loss: 2.65504503\n",
      "Iteration: 12500\n",
      "Loss: 2.25977755\n",
      "Validation loss: 0.39240247\n",
      "Iteration: 12600\n",
      "Loss: 0.19278087\n",
      "Validation loss: 0.36011183\n",
      "Iteration: 12700\n",
      "Loss: 0.55549103\n",
      "Validation loss: 0.41864681\n",
      "Iteration: 12800\n",
      "Loss: 1.53409660\n",
      "Validation loss: 0.24187183\n",
      "Iteration: 12900\n",
      "Loss: 0.68421829\n",
      "Validation loss: 0.18959102\n",
      "Iteration: 13000\n",
      "Loss: 1.12597775\n",
      "Validation loss: 0.41642550\n",
      "Iteration: 13100\n",
      "Loss: 0.31655174\n",
      "Validation loss: 0.18458298\n",
      "Iteration: 13200\n",
      "Loss: 0.25411725\n",
      "Validation loss: 0.70940763\n",
      "Iteration: 13300\n",
      "Loss: 0.44419825\n",
      "Validation loss: 1.83265817\n",
      "Iteration: 13400\n",
      "Loss: 1.64042640\n",
      "Validation loss: 0.82570028\n",
      "Iteration: 13500\n",
      "Loss: 0.59537292\n",
      "Validation loss: 0.36741078\n",
      "Iteration: 13600\n",
      "Loss: 0.76726741\n",
      "Validation loss: 0.78460979\n",
      "Iteration: 13700\n",
      "Loss: 0.38117936\n",
      "Validation loss: 1.11670148\n",
      "Iteration: 13800\n",
      "Loss: 0.11626643\n",
      "Validation loss: 0.22291075\n",
      "Iteration: 13900\n",
      "Loss: 0.30638215\n",
      "Validation loss: 0.50929773\n",
      "Iteration: 14000\n",
      "Loss: 0.76571894\n",
      "Validation loss: 0.54751110\n",
      "Iteration: 14100\n",
      "Loss: 0.19266045\n",
      "Validation loss: 0.57018709\n",
      "Iteration: 14200\n",
      "Loss: 0.56728053\n",
      "Validation loss: 0.49570033\n",
      "Iteration: 14300\n",
      "Loss: 0.78579235\n",
      "Validation loss: 0.66054678\n",
      "Iteration: 14400\n",
      "Loss: 0.24307369\n",
      "Validation loss: 0.41278243\n",
      "Iteration: 14500\n",
      "Loss: 0.29520881\n",
      "Validation loss: 0.20320961\n",
      "Iteration: 14600\n",
      "Loss: 0.65065837\n",
      "Validation loss: 0.57806075\n",
      "Iteration: 14700\n",
      "Loss: 0.29910392\n",
      "Validation loss: 0.71990925\n",
      "Iteration: 14800\n",
      "Loss: 0.39959449\n",
      "Validation loss: 0.63461006\n",
      "Iteration: 14900\n",
      "Loss: 0.62353438\n",
      "Validation loss: 0.22145060\n",
      "Iteration: 15000\n",
      "Loss: 1.51247001\n",
      "Validation loss: 0.51554501\n",
      "Iteration: 15100\n",
      "Loss: 0.16953483\n",
      "Validation loss: 0.51822269\n",
      "Iteration: 15200\n",
      "Loss: 0.40520468\n",
      "Validation loss: 0.51353210\n",
      "Iteration: 15300\n",
      "Loss: 0.46071249\n",
      "Validation loss: 0.28112864\n",
      "Iteration: 15344\n",
      "Loss: 0.32001778\n",
      "Validation loss: 1.14630127\n",
      "Epoch: 4\n",
      "Iteration:    1\n",
      "Loss: 0.49117553\n",
      "Validation loss: 0.56091177\n",
      "Iteration:  100\n",
      "Loss: 0.50085664\n",
      "Validation loss: 0.37346274\n",
      "Iteration:  200\n",
      "Loss: 0.38054743\n",
      "Validation loss: 0.29126871\n",
      "Iteration:  300\n",
      "Loss: 0.41827860\n",
      "Validation loss: 0.65928531\n",
      "Iteration:  400\n",
      "Loss: 0.17933309\n",
      "Validation loss: 0.20685422\n",
      "Iteration:  500\n",
      "Loss: 0.42092973\n",
      "Validation loss: 0.47759908\n",
      "Iteration:  600\n",
      "Loss: 0.34709191\n",
      "Validation loss: 0.27884951\n",
      "Iteration:  700\n",
      "Loss: 0.45469210\n",
      "Validation loss: 0.31645301\n",
      "Iteration:  800\n",
      "Loss: 0.22953823\n",
      "Validation loss: 0.19582739\n",
      "Iteration:  900\n",
      "Loss: 0.37255025\n",
      "Validation loss: 0.18356791\n",
      "Iteration: 1000\n",
      "Loss: 0.33587378\n",
      "Validation loss: 0.68269724\n",
      "Iteration: 1100\n",
      "Loss: 0.42596114\n",
      "Validation loss: 0.12859982\n",
      "Iteration: 1200\n",
      "Loss: 0.46701962\n",
      "Validation loss: 0.14601222\n",
      "Iteration: 1300\n",
      "Loss: 0.65009326\n",
      "Validation loss: 0.64149410\n",
      "Iteration: 1400\n",
      "Loss: 0.64109892\n",
      "Validation loss: 0.70008725\n",
      "Iteration: 1500\n",
      "Loss: 0.33753699\n",
      "Validation loss: 0.27572438\n",
      "Iteration: 1600\n",
      "Loss: 0.85409951\n",
      "Validation loss: 0.32931519\n",
      "Iteration: 1700\n",
      "Loss: 1.08601213\n",
      "Validation loss: 0.23613021\n",
      "Iteration: 1800\n",
      "Loss: 0.61948282\n",
      "Validation loss: 0.56342798\n",
      "Iteration: 1900\n",
      "Loss: 0.28987783\n",
      "Validation loss: 0.25182539\n",
      "Iteration: 2000\n",
      "Loss: 0.38158759\n",
      "Validation loss: 0.36096415\n",
      "Iteration: 2100\n",
      "Loss: 0.53323102\n",
      "Validation loss: 0.48450541\n",
      "Iteration: 2200\n",
      "Loss: 0.26778686\n",
      "Validation loss: 0.37393987\n",
      "Iteration: 2300\n",
      "Loss: 0.41301322\n",
      "Validation loss: 1.01119554\n",
      "Iteration: 2400\n",
      "Loss: 0.43636566\n",
      "Validation loss: 0.43347090\n",
      "Iteration: 2500\n",
      "Loss: 1.54230499\n",
      "Validation loss: 0.17682141\n",
      "Iteration: 2600\n",
      "Loss: 0.37260500\n",
      "Validation loss: 1.28778410\n",
      "Iteration: 2700\n",
      "Loss: 1.01392186\n",
      "Validation loss: 0.21617937\n",
      "Iteration: 2800\n",
      "Loss: 0.46173704\n",
      "Validation loss: 1.58668840\n",
      "Iteration: 2900\n",
      "Loss: 0.27543187\n",
      "Validation loss: 0.27606225\n",
      "Iteration: 3000\n",
      "Loss: 0.38431638\n",
      "Validation loss: 0.43197960\n",
      "Iteration: 3100\n",
      "Loss: 1.77935159\n",
      "Validation loss: 0.45936954\n",
      "Iteration: 3200\n",
      "Loss: 0.71313953\n",
      "Validation loss: 0.15745984\n",
      "Iteration: 3300\n",
      "Loss: 0.24838501\n",
      "Validation loss: 0.19294322\n",
      "Iteration: 3400\n",
      "Loss: 0.68844688\n",
      "Validation loss: 0.40981615\n",
      "Iteration: 3500\n",
      "Loss: 0.42952678\n",
      "Validation loss: 0.60939366\n",
      "Iteration: 3600\n",
      "Loss: 0.98930109\n",
      "Validation loss: 0.75641680\n",
      "Iteration: 3700\n",
      "Loss: 0.11075288\n",
      "Validation loss: 0.53205955\n",
      "Iteration: 3800\n",
      "Loss: 0.41573888\n",
      "Validation loss: 0.34157565\n",
      "Iteration: 3900\n",
      "Loss: 1.07430863\n",
      "Validation loss: 0.46050495\n",
      "Iteration: 4000\n",
      "Loss: 0.24397162\n",
      "Validation loss: 0.42225280\n",
      "Iteration: 4100\n",
      "Loss: 0.44073150\n",
      "Validation loss: 0.82025194\n",
      "Iteration: 4200\n",
      "Loss: 0.42361644\n",
      "Validation loss: 0.36081535\n",
      "Iteration: 4300\n",
      "Loss: 0.95490056\n",
      "Validation loss: 0.41386187\n",
      "Iteration: 4400\n",
      "Loss: 1.03450251\n",
      "Validation loss: 0.68948591\n",
      "Iteration: 4500\n",
      "Loss: 0.29661542\n",
      "Validation loss: 0.36174971\n",
      "Iteration: 4600\n",
      "Loss: 0.33071834\n",
      "Validation loss: 0.12956941\n",
      "Iteration: 4700\n",
      "Loss: 0.49657834\n",
      "Validation loss: 0.35559684\n",
      "Iteration: 4800\n",
      "Loss: 0.35949332\n",
      "Validation loss: 0.18249083\n",
      "Iteration: 4900\n",
      "Loss: 0.23823032\n",
      "Validation loss: 0.19731428\n",
      "Iteration: 5000\n",
      "Loss: 1.01774549\n",
      "Validation loss: 0.14151144\n",
      "Iteration: 5100\n",
      "Loss: 0.49495047\n",
      "Validation loss: 0.28619292\n",
      "Iteration: 5200\n",
      "Loss: 0.44826767\n",
      "Validation loss: 0.28709555\n",
      "Iteration: 5300\n",
      "Loss: 0.62442303\n",
      "Validation loss: 0.34383434\n",
      "Iteration: 5400\n",
      "Loss: 0.63994056\n",
      "Validation loss: 0.23348254\n",
      "Iteration: 5500\n",
      "Loss: 0.43676227\n",
      "Validation loss: 0.29449961\n",
      "Iteration: 5600\n",
      "Loss: 0.23319238\n",
      "Validation loss: 0.11579435\n",
      "Iteration: 5700\n",
      "Loss: 0.77153349\n",
      "Validation loss: 0.66446936\n",
      "Iteration: 5800\n",
      "Loss: 0.72204971\n",
      "Validation loss: 1.01909363\n",
      "Iteration: 5900\n",
      "Loss: 0.86483240\n",
      "Validation loss: 0.36768553\n",
      "Iteration: 6000\n",
      "Loss: 0.29325047\n",
      "Validation loss: 0.47016576\n",
      "Iteration: 6100\n",
      "Loss: 0.31402028\n",
      "Validation loss: 0.27933380\n",
      "Iteration: 6200\n",
      "Loss: 0.40026659\n",
      "Validation loss: 0.31696218\n",
      "Iteration: 6300\n",
      "Loss: 0.31855077\n",
      "Validation loss: 0.44841665\n",
      "Iteration: 6400\n",
      "Loss: 0.33992320\n",
      "Validation loss: 0.16641195\n",
      "Iteration: 6500\n",
      "Loss: 0.28043306\n",
      "Validation loss: 0.19346288\n",
      "Iteration: 6600\n",
      "Loss: 0.33983108\n",
      "Validation loss: 1.43852282\n",
      "Iteration: 6700\n",
      "Loss: 0.28755289\n",
      "Validation loss: 0.28137630\n",
      "Iteration: 6800\n",
      "Loss: 0.36277294\n",
      "Validation loss: 0.21230993\n",
      "Iteration: 6900\n",
      "Loss: 0.38097042\n",
      "Validation loss: 0.26584154\n",
      "Iteration: 7000\n",
      "Loss: 0.39341757\n",
      "Validation loss: 0.27198762\n",
      "Iteration: 7100\n",
      "Loss: 0.10453727\n",
      "Validation loss: 0.45527610\n",
      "Iteration: 7200\n",
      "Loss: 0.74281716\n",
      "Validation loss: 0.44958860\n",
      "Iteration: 7300\n",
      "Loss: 0.35946849\n",
      "Validation loss: 0.60219431\n",
      "Iteration: 7400\n",
      "Loss: 0.31349546\n",
      "Validation loss: 0.58639115\n",
      "Iteration: 7500\n",
      "Loss: 0.37556708\n",
      "Validation loss: 0.18987973\n",
      "Iteration: 7600\n",
      "Loss: 0.47079092\n",
      "Validation loss: 0.46594495\n",
      "Iteration: 7700\n",
      "Loss: 0.54705584\n",
      "Validation loss: 0.74730319\n",
      "Iteration: 7800\n",
      "Loss: 0.47342992\n",
      "Validation loss: 0.27965042\n",
      "Iteration: 7900\n",
      "Loss: 0.16822436\n",
      "Validation loss: 0.28377345\n",
      "Iteration: 8000\n",
      "Loss: 0.60604572\n",
      "Validation loss: 0.19412069\n",
      "Iteration: 8100\n",
      "Loss: 0.48959121\n",
      "Validation loss: 0.38610753\n",
      "Iteration: 8200\n",
      "Loss: 0.26862180\n",
      "Validation loss: 0.42348880\n",
      "Iteration: 8300\n",
      "Loss: 0.39896584\n",
      "Validation loss: 0.20217799\n",
      "Iteration: 8400\n",
      "Loss: 0.27862757\n",
      "Validation loss: 1.28546810\n",
      "Iteration: 8500\n",
      "Loss: 0.68670344\n",
      "Validation loss: 0.77146888\n",
      "Iteration: 8600\n",
      "Loss: 1.11873055\n",
      "Validation loss: 0.60798180\n",
      "Iteration: 8700\n",
      "Loss: 0.20121056\n",
      "Validation loss: 1.00913966\n",
      "Iteration: 8800\n",
      "Loss: 0.35394093\n",
      "Validation loss: 0.29575816\n",
      "Iteration: 8900\n",
      "Loss: 0.14888746\n",
      "Validation loss: 0.20793349\n",
      "Iteration: 9000\n",
      "Loss: 0.29279611\n",
      "Validation loss: 0.64769495\n",
      "Iteration: 9100\n",
      "Loss: 0.11316511\n",
      "Validation loss: 0.64466822\n",
      "Iteration: 9200\n",
      "Loss: 0.46221158\n",
      "Validation loss: 0.55512959\n",
      "Iteration: 9300\n",
      "Loss: 0.34876513\n",
      "Validation loss: 0.41232091\n",
      "Iteration: 9400\n",
      "Loss: 0.32638159\n",
      "Validation loss: 0.37567109\n",
      "Iteration: 9500\n",
      "Loss: 0.52441084\n",
      "Validation loss: 0.25674707\n",
      "Iteration: 9600\n",
      "Loss: 0.29660869\n",
      "Validation loss: 0.45507085\n",
      "Iteration: 9700\n",
      "Loss: 0.39559457\n",
      "Validation loss: 0.60990244\n",
      "Iteration: 9800\n",
      "Loss: 0.66726208\n",
      "Validation loss: 0.23029983\n",
      "Iteration: 9900\n",
      "Loss: 0.13512090\n",
      "Validation loss: 0.38873941\n",
      "Iteration: 10000\n",
      "Loss: 0.35355699\n",
      "Validation loss: 0.34526131\n",
      "Iteration: 10100\n",
      "Loss: 0.35528207\n",
      "Validation loss: 0.64102733\n",
      "Iteration: 10200\n",
      "Loss: 0.20216867\n",
      "Validation loss: 0.51816535\n",
      "Iteration: 10300\n",
      "Loss: 0.26807511\n",
      "Validation loss: 0.26696777\n",
      "Iteration: 10400\n",
      "Loss: 0.40792358\n",
      "Validation loss: 0.08561738\n",
      "Iteration: 10500\n",
      "Loss: 0.26693201\n",
      "Validation loss: 0.48881450\n",
      "Iteration: 10600\n",
      "Loss: 0.19328743\n",
      "Validation loss: 1.49228895\n",
      "Iteration: 10700\n",
      "Loss: 0.58007967\n",
      "Validation loss: 0.39409700\n",
      "Iteration: 10800\n",
      "Loss: 0.97962159\n",
      "Validation loss: 0.23340493\n",
      "Iteration: 10900\n",
      "Loss: 1.52087569\n",
      "Validation loss: 1.44965625\n",
      "Iteration: 11000\n",
      "Loss: 0.27191320\n",
      "Validation loss: 0.25967616\n",
      "Iteration: 11100\n",
      "Loss: 0.20291770\n",
      "Validation loss: 0.30206198\n",
      "Iteration: 11200\n",
      "Loss: 0.56364465\n",
      "Validation loss: 0.21712585\n",
      "Iteration: 11300\n",
      "Loss: 0.94994915\n",
      "Validation loss: 0.34819055\n",
      "Iteration: 11400\n",
      "Loss: 0.21287979\n",
      "Validation loss: 0.51516366\n",
      "Iteration: 11500\n",
      "Loss: 0.73558140\n",
      "Validation loss: 0.32056376\n",
      "Iteration: 11600\n",
      "Loss: 0.67257714\n",
      "Validation loss: 0.30806330\n",
      "Iteration: 11700\n",
      "Loss: 0.56160754\n",
      "Validation loss: 0.52308810\n",
      "Iteration: 11800\n",
      "Loss: 0.30882177\n",
      "Validation loss: 0.30248472\n",
      "Iteration: 11900\n",
      "Loss: 0.57441103\n",
      "Validation loss: 0.27039269\n",
      "Iteration: 12000\n",
      "Loss: 0.17559338\n",
      "Validation loss: 0.25013465\n",
      "Iteration: 12100\n",
      "Loss: 0.65006185\n",
      "Validation loss: 0.36949459\n",
      "Iteration: 12200\n",
      "Loss: 0.47022626\n",
      "Validation loss: 0.33849818\n",
      "Iteration: 12300\n",
      "Loss: 0.14419872\n",
      "Validation loss: 0.71753788\n",
      "Iteration: 12400\n",
      "Loss: 0.58199561\n",
      "Validation loss: 0.25635630\n",
      "Iteration: 12500\n",
      "Loss: 0.47309583\n",
      "Validation loss: 0.50605315\n",
      "Iteration: 12600\n",
      "Loss: 0.26674548\n",
      "Validation loss: 0.37985116\n",
      "Iteration: 12700\n",
      "Loss: 0.64539176\n",
      "Validation loss: 0.94283825\n",
      "Iteration: 12800\n",
      "Loss: 0.46512192\n",
      "Validation loss: 0.38868591\n",
      "Iteration: 12900\n",
      "Loss: 0.34952879\n",
      "Validation loss: 0.42328128\n",
      "Iteration: 13000\n",
      "Loss: 0.88313752\n",
      "Validation loss: 0.25295714\n",
      "Iteration: 13100\n",
      "Loss: 0.32305419\n",
      "Validation loss: 0.13470066\n",
      "Iteration: 13200\n",
      "Loss: 0.13527907\n",
      "Validation loss: 0.23116171\n",
      "Iteration: 13300\n",
      "Loss: 0.30783296\n",
      "Validation loss: 0.83713430\n",
      "Iteration: 13400\n",
      "Loss: 0.69680077\n",
      "Validation loss: 0.51914704\n",
      "Iteration: 13500\n",
      "Loss: 0.43037635\n",
      "Validation loss: 0.12977701\n",
      "Iteration: 13600\n",
      "Loss: 0.67645055\n",
      "Validation loss: 0.40669584\n",
      "Iteration: 13700\n",
      "Loss: 0.26492906\n",
      "Validation loss: 0.13315625\n",
      "Iteration: 13800\n",
      "Loss: 0.26536661\n",
      "Validation loss: 0.25741053\n",
      "Iteration: 13900\n",
      "Loss: 0.40316546\n",
      "Validation loss: 1.10387623\n",
      "Iteration: 14000\n",
      "Loss: 0.36096984\n",
      "Validation loss: 0.21409674\n",
      "Iteration: 14100\n",
      "Loss: 0.32373476\n",
      "Validation loss: 0.41079244\n",
      "Iteration: 14200\n",
      "Loss: 0.31032443\n",
      "Validation loss: 0.79909861\n",
      "Iteration: 14300\n",
      "Loss: 0.36365354\n",
      "Validation loss: 0.32952210\n",
      "Iteration: 14400\n",
      "Loss: 0.34011647\n",
      "Validation loss: 0.31815875\n",
      "Iteration: 14500\n",
      "Loss: 0.23268458\n",
      "Validation loss: 0.23189065\n",
      "Iteration: 14600\n",
      "Loss: 0.81209779\n",
      "Validation loss: 0.58555609\n",
      "Iteration: 14700\n",
      "Loss: 0.95225459\n",
      "Validation loss: 0.38229758\n",
      "Iteration: 14800\n",
      "Loss: 0.28319031\n",
      "Validation loss: 0.32954001\n",
      "Iteration: 14900\n",
      "Loss: 0.47526255\n",
      "Validation loss: 0.18196459\n",
      "Iteration: 15000\n",
      "Loss: 0.38229114\n",
      "Validation loss: 0.12496263\n",
      "Iteration: 15100\n",
      "Loss: 0.17015943\n",
      "Validation loss: 0.66934264\n",
      "Iteration: 15200\n",
      "Loss: 0.78431571\n",
      "Validation loss: 0.53272229\n",
      "Iteration: 15300\n",
      "Loss: 0.36285508\n",
      "Validation loss: 0.28574634\n",
      "Iteration: 15344\n",
      "Loss: 0.43873978\n",
      "Validation loss: 0.34583467\n",
      "Epoch: 5\n",
      "Iteration:    1\n",
      "Loss: 0.43007281\n",
      "Validation loss: 0.25760278\n",
      "Iteration:  100\n",
      "Loss: 0.56200415\n",
      "Validation loss: 0.25335720\n",
      "Iteration:  200\n",
      "Loss: 0.24993402\n",
      "Validation loss: 0.31245944\n",
      "Iteration:  300\n",
      "Loss: 0.13907762\n",
      "Validation loss: 0.55013835\n",
      "Iteration:  400\n",
      "Loss: 0.46566099\n",
      "Validation loss: 0.17102247\n",
      "Iteration:  500\n",
      "Loss: 0.36001298\n",
      "Validation loss: 0.43361259\n",
      "Iteration:  600\n",
      "Loss: 0.34159344\n",
      "Validation loss: 0.15273480\n",
      "Iteration:  700\n",
      "Loss: 0.83876073\n",
      "Validation loss: 0.72695863\n",
      "Iteration:  800\n",
      "Loss: 0.43291771\n",
      "Validation loss: 0.49504799\n",
      "Iteration:  900\n",
      "Loss: 0.29186574\n",
      "Validation loss: 0.25395238\n",
      "Iteration: 1000\n",
      "Loss: 0.95910507\n",
      "Validation loss: 1.04369903\n",
      "Iteration: 1100\n",
      "Loss: 0.47347927\n",
      "Validation loss: 0.72357810\n",
      "Iteration: 1200\n",
      "Loss: 0.77870667\n",
      "Validation loss: 0.82711053\n",
      "Iteration: 1300\n",
      "Loss: 0.24007238\n",
      "Validation loss: 0.37013054\n",
      "Iteration: 1400\n",
      "Loss: 1.05683804\n",
      "Validation loss: 1.70433521\n",
      "Iteration: 1500\n",
      "Loss: 0.15859258\n",
      "Validation loss: 0.81042254\n",
      "Iteration: 1600\n",
      "Loss: 0.31492078\n",
      "Validation loss: 0.39427507\n",
      "Iteration: 1700\n",
      "Loss: 1.11678100\n",
      "Validation loss: 0.20913693\n",
      "Iteration: 1800\n",
      "Loss: 0.60354853\n",
      "Validation loss: 0.25009018\n",
      "Iteration: 1900\n",
      "Loss: 0.25563607\n",
      "Validation loss: 0.45205647\n",
      "Iteration: 2000\n",
      "Loss: 0.14204729\n",
      "Validation loss: 1.18464780\n",
      "Iteration: 2100\n",
      "Loss: 0.33332869\n",
      "Validation loss: 0.25406170\n",
      "Iteration: 2200\n",
      "Loss: 0.23467195\n",
      "Validation loss: 0.79443944\n",
      "Iteration: 2300\n",
      "Loss: 0.34660146\n",
      "Validation loss: 0.66691405\n",
      "Iteration: 2400\n",
      "Loss: 0.40286884\n",
      "Validation loss: 0.33816969\n",
      "Iteration: 2500\n",
      "Loss: 0.65702128\n",
      "Validation loss: 0.02882624\n",
      "Iteration: 2600\n",
      "Loss: 0.16656311\n",
      "Validation loss: 0.56564718\n",
      "Iteration: 2700\n",
      "Loss: 0.48277587\n",
      "Validation loss: 0.39917836\n",
      "Iteration: 2800\n",
      "Loss: 0.43082815\n",
      "Validation loss: 0.31232384\n",
      "Iteration: 2900\n",
      "Loss: 0.14850619\n",
      "Validation loss: 0.28431943\n",
      "Iteration: 3000\n",
      "Loss: 0.24319111\n",
      "Validation loss: 0.31482452\n",
      "Iteration: 3100\n",
      "Loss: 0.39220271\n",
      "Validation loss: 0.46196339\n",
      "Iteration: 3200\n",
      "Loss: 0.57413721\n",
      "Validation loss: 0.28129035\n",
      "Iteration: 3300\n",
      "Loss: 0.46446246\n",
      "Validation loss: 0.26208529\n",
      "Iteration: 3400\n",
      "Loss: 0.49056470\n",
      "Validation loss: 0.62097943\n",
      "Iteration: 3500\n",
      "Loss: 0.16441482\n",
      "Validation loss: 0.48507109\n",
      "Iteration: 3600\n",
      "Loss: 0.68623894\n",
      "Validation loss: 0.97990823\n",
      "Iteration: 3700\n",
      "Loss: 0.44187501\n",
      "Validation loss: 0.25299624\n",
      "Iteration: 3800\n",
      "Loss: 1.18422735\n",
      "Validation loss: 0.41303575\n",
      "Iteration: 3900\n",
      "Loss: 0.42559534\n",
      "Validation loss: 0.16811801\n",
      "Iteration: 4000\n",
      "Loss: 0.20952088\n",
      "Validation loss: 0.87122774\n",
      "Iteration: 4100\n",
      "Loss: 0.28546360\n",
      "Validation loss: 0.27844596\n",
      "Iteration: 4200\n",
      "Loss: 0.49793139\n",
      "Validation loss: 0.31478810\n",
      "Iteration: 4300\n",
      "Loss: 0.49899119\n",
      "Validation loss: 0.32980508\n",
      "Iteration: 4400\n",
      "Loss: 0.35076803\n",
      "Validation loss: 0.19746271\n",
      "Iteration: 4500\n",
      "Loss: 0.12851785\n",
      "Validation loss: 0.19519612\n",
      "Iteration: 4600\n",
      "Loss: 0.41959968\n",
      "Validation loss: 0.24337494\n",
      "Iteration: 4700\n",
      "Loss: 0.29063392\n",
      "Validation loss: 0.45347977\n",
      "Iteration: 4800\n",
      "Loss: 0.14860055\n",
      "Validation loss: 0.34838805\n",
      "Iteration: 4900\n",
      "Loss: 0.16883093\n",
      "Validation loss: 0.79411536\n",
      "Iteration: 5000\n",
      "Loss: 0.45737004\n",
      "Validation loss: 0.46233782\n",
      "Iteration: 5100\n",
      "Loss: 0.30436325\n",
      "Validation loss: 0.44077080\n",
      "Iteration: 5200\n",
      "Loss: 0.43831035\n",
      "Validation loss: 0.24664555\n",
      "Iteration: 5300\n",
      "Loss: 0.44438541\n",
      "Validation loss: 0.84716970\n",
      "Iteration: 5400\n",
      "Loss: 0.44456637\n",
      "Validation loss: 0.79715633\n",
      "Iteration: 5500\n",
      "Loss: 0.28343463\n",
      "Validation loss: 0.63398045\n",
      "Iteration: 5600\n",
      "Loss: 0.08023737\n",
      "Validation loss: 0.10743630\n",
      "Iteration: 5700\n",
      "Loss: 0.61564475\n",
      "Validation loss: 0.26981175\n",
      "Iteration: 5800\n",
      "Loss: 0.72780073\n",
      "Validation loss: 0.93095338\n",
      "Iteration: 5900\n",
      "Loss: 0.71307898\n",
      "Validation loss: 0.38911185\n",
      "Iteration: 6000\n",
      "Loss: 0.13961020\n",
      "Validation loss: 0.20762640\n",
      "Iteration: 6100\n",
      "Loss: 0.25862584\n",
      "Validation loss: 0.15847854\n",
      "Iteration: 6200\n",
      "Loss: 0.35388881\n",
      "Validation loss: 0.35123760\n",
      "Iteration: 6300\n",
      "Loss: 0.68111515\n",
      "Validation loss: 0.21703771\n",
      "Iteration: 6400\n",
      "Loss: 0.51197273\n",
      "Validation loss: 0.16871440\n",
      "Iteration: 6500\n",
      "Loss: 0.22497526\n",
      "Validation loss: 0.63916105\n",
      "Iteration: 6600\n",
      "Loss: 0.20024486\n",
      "Validation loss: 0.32530588\n",
      "Iteration: 6700\n",
      "Loss: 0.45393756\n",
      "Validation loss: 0.06826462\n",
      "Iteration: 6800\n",
      "Loss: 0.41433591\n",
      "Validation loss: 0.34692544\n",
      "Iteration: 6900\n",
      "Loss: 0.46986586\n",
      "Validation loss: 0.26102310\n",
      "Iteration: 7000\n",
      "Loss: 0.09865102\n",
      "Validation loss: 0.03504561\n",
      "Iteration: 7100\n",
      "Loss: 0.17500614\n",
      "Validation loss: 0.27313119\n",
      "Iteration: 7200\n",
      "Loss: 0.19634941\n",
      "Validation loss: 0.16710185\n",
      "Iteration: 7300\n",
      "Loss: 0.39516908\n",
      "Validation loss: 0.27599585\n",
      "Iteration: 7400\n",
      "Loss: 0.60785615\n",
      "Validation loss: 0.56564403\n",
      "Iteration: 7500\n",
      "Loss: 0.38585022\n",
      "Validation loss: 0.39055261\n",
      "Iteration: 7600\n",
      "Loss: 0.22909455\n",
      "Validation loss: 0.66991925\n",
      "Iteration: 7700\n",
      "Loss: 0.61163330\n",
      "Validation loss: 0.17989491\n",
      "Iteration: 7800\n",
      "Loss: 0.58108276\n",
      "Validation loss: 0.22940044\n",
      "Iteration: 7900\n",
      "Loss: 0.44567877\n",
      "Validation loss: 0.04653511\n",
      "Iteration: 8000\n",
      "Loss: 0.32276472\n",
      "Validation loss: 0.13920549\n",
      "Iteration: 8100\n",
      "Loss: 0.36056423\n",
      "Validation loss: 0.23234445\n",
      "Iteration: 8200\n",
      "Loss: 0.30499142\n",
      "Validation loss: 0.10073321\n",
      "Iteration: 8300\n",
      "Loss: 1.36365652\n",
      "Validation loss: 0.46200383\n",
      "Iteration: 8400\n",
      "Loss: 0.40857220\n",
      "Validation loss: 0.87237978\n",
      "Iteration: 8500\n",
      "Loss: 0.25759196\n",
      "Validation loss: 0.57194477\n",
      "Iteration: 8600\n",
      "Loss: 0.16228372\n",
      "Validation loss: 0.45941183\n",
      "Iteration: 8700\n",
      "Loss: 0.13311696\n",
      "Validation loss: 0.67617786\n",
      "Iteration: 8800\n",
      "Loss: 0.23522945\n",
      "Validation loss: 0.13557872\n",
      "Iteration: 8900\n",
      "Loss: 0.19748165\n",
      "Validation loss: 0.10915204\n",
      "Iteration: 9000\n",
      "Loss: 0.22590125\n",
      "Validation loss: 0.13759913\n",
      "Iteration: 9100\n",
      "Loss: 0.17320934\n",
      "Validation loss: 0.68808055\n",
      "Iteration: 9200\n",
      "Loss: 0.11810091\n",
      "Validation loss: 1.92794025\n",
      "Iteration: 9300\n",
      "Loss: 0.57180476\n",
      "Validation loss: 0.34741452\n",
      "Iteration: 9400\n",
      "Loss: 0.47519511\n",
      "Validation loss: 0.31838885\n",
      "Iteration: 9500\n",
      "Loss: 0.49577594\n",
      "Validation loss: 0.25057995\n",
      "Iteration: 9600\n",
      "Loss: 0.27165365\n",
      "Validation loss: 0.23846719\n",
      "Iteration: 9700\n",
      "Loss: 0.47856623\n",
      "Validation loss: 0.28253680\n",
      "Iteration: 9800\n",
      "Loss: 0.29251578\n",
      "Validation loss: 0.32346648\n",
      "Iteration: 9900\n",
      "Loss: 0.21123558\n",
      "Validation loss: 0.59121919\n",
      "Iteration: 10000\n",
      "Loss: 0.29008472\n",
      "Validation loss: 0.91038918\n",
      "Iteration: 10100\n",
      "Loss: 0.24571097\n",
      "Validation loss: 0.31689981\n",
      "Iteration: 10200\n",
      "Loss: 0.17022642\n",
      "Validation loss: 0.30183324\n",
      "Iteration: 10300\n",
      "Loss: 0.04629485\n",
      "Validation loss: 0.47468540\n",
      "Iteration: 10400\n",
      "Loss: 0.45033610\n",
      "Validation loss: 0.09903803\n",
      "Iteration: 10500\n",
      "Loss: 0.27984709\n",
      "Validation loss: 0.07580778\n",
      "Iteration: 10600\n",
      "Loss: 0.09902748\n",
      "Validation loss: 1.79106092\n",
      "Iteration: 10700\n",
      "Loss: 0.33835968\n",
      "Validation loss: 0.38234648\n",
      "Iteration: 10800\n",
      "Loss: 0.24053396\n",
      "Validation loss: 0.31467777\n",
      "Iteration: 10900\n",
      "Loss: 1.21625364\n",
      "Validation loss: 0.29558098\n",
      "Iteration: 11000\n",
      "Loss: 0.17275572\n",
      "Validation loss: 0.96286249\n",
      "Iteration: 11100\n",
      "Loss: 0.12755670\n",
      "Validation loss: 0.18164983\n",
      "Iteration: 11200\n",
      "Loss: 0.20460108\n",
      "Validation loss: 0.20009749\n",
      "Iteration: 11300\n",
      "Loss: 0.96063161\n",
      "Validation loss: 0.14583357\n",
      "Iteration: 11400\n",
      "Loss: 0.28506482\n",
      "Validation loss: 0.35108310\n",
      "Iteration: 11500\n",
      "Loss: 0.29906124\n",
      "Validation loss: 0.28777963\n",
      "Iteration: 11600\n",
      "Loss: 0.40996450\n",
      "Validation loss: 0.74520040\n",
      "Iteration: 11700\n",
      "Loss: 0.27462795\n",
      "Validation loss: 0.07784927\n",
      "Iteration: 11800\n",
      "Loss: 0.55441201\n",
      "Validation loss: 0.76510316\n",
      "Iteration: 11900\n",
      "Loss: 0.42204204\n",
      "Validation loss: 0.68956625\n",
      "Iteration: 12000\n",
      "Loss: 0.08054744\n",
      "Validation loss: 0.14441353\n",
      "Iteration: 12100\n",
      "Loss: 1.96036017\n",
      "Validation loss: 0.42735171\n",
      "Iteration: 12200\n",
      "Loss: 0.44464958\n",
      "Validation loss: 0.46284041\n",
      "Iteration: 12300\n",
      "Loss: 0.33996385\n",
      "Validation loss: 0.13874835\n",
      "Iteration: 12400\n",
      "Loss: 0.41441399\n",
      "Validation loss: 0.20026547\n",
      "Iteration: 12500\n",
      "Loss: 0.37731653\n",
      "Validation loss: 0.38171250\n",
      "Iteration: 12600\n",
      "Loss: 0.26082978\n",
      "Validation loss: 0.16290507\n",
      "Iteration: 12700\n",
      "Loss: 0.43167430\n",
      "Validation loss: 0.14123127\n",
      "Iteration: 12800\n",
      "Loss: 0.12449560\n",
      "Validation loss: 0.30552697\n",
      "Iteration: 12900\n",
      "Loss: 0.48196161\n",
      "Validation loss: 0.22000876\n",
      "Iteration: 13000\n",
      "Loss: 0.16658801\n",
      "Validation loss: 0.69070506\n",
      "Iteration: 13100\n",
      "Loss: 0.19225737\n",
      "Validation loss: 0.28095326\n",
      "Iteration: 13200\n",
      "Loss: 0.17222728\n",
      "Validation loss: 0.09063142\n",
      "Iteration: 13300\n",
      "Loss: 0.20908058\n",
      "Validation loss: 0.47845933\n",
      "Iteration: 13400\n",
      "Loss: 0.43777084\n",
      "Validation loss: 0.53837121\n",
      "Iteration: 13500\n",
      "Loss: 0.44689569\n",
      "Validation loss: 1.15653825\n",
      "Iteration: 13600\n",
      "Loss: 0.57385075\n",
      "Validation loss: 0.36776054\n",
      "Iteration: 13700\n",
      "Loss: 0.44755590\n",
      "Validation loss: 0.19909084\n",
      "Iteration: 13800\n",
      "Loss: 0.23255453\n",
      "Validation loss: 0.40072286\n",
      "Iteration: 13900\n",
      "Loss: 0.23006661\n",
      "Validation loss: 0.27040899\n",
      "Iteration: 14000\n",
      "Loss: 0.14761060\n",
      "Validation loss: 0.35977554\n",
      "Iteration: 14100\n",
      "Loss: 0.26833093\n",
      "Validation loss: 0.60452509\n",
      "Iteration: 14200\n",
      "Loss: 0.18677676\n",
      "Validation loss: 0.53251445\n",
      "Iteration: 14300\n",
      "Loss: 0.31284818\n",
      "Validation loss: 0.54986143\n",
      "Iteration: 14400\n",
      "Loss: 0.26687062\n",
      "Validation loss: 0.49246907\n",
      "Iteration: 14500\n",
      "Loss: 0.43055475\n",
      "Validation loss: 0.20380300\n",
      "Iteration: 14600\n",
      "Loss: 0.96924740\n",
      "Validation loss: 0.18893774\n",
      "Iteration: 14700\n",
      "Loss: 0.55666757\n",
      "Validation loss: 0.74659115\n",
      "Iteration: 14800\n",
      "Loss: 0.12135962\n",
      "Validation loss: 0.17938304\n",
      "Iteration: 14900\n",
      "Loss: 0.16601886\n",
      "Validation loss: 0.23483166\n",
      "Iteration: 15000\n",
      "Loss: 0.15100975\n",
      "Validation loss: 0.14558971\n",
      "Iteration: 15100\n",
      "Loss: 0.21233109\n",
      "Validation loss: 0.44571874\n",
      "Iteration: 15200\n",
      "Loss: 0.42792431\n",
      "Validation loss: 0.24020168\n",
      "Iteration: 15300\n",
      "Loss: 0.40737399\n",
      "Validation loss: 0.45840225\n",
      "Iteration: 15344\n",
      "Loss: 0.37601468\n",
      "Validation loss: 1.09223020\n"
     ]
    }
   ],
   "source": [
    "ctc_train, ctc_eval = train(model, optimizer, primus, epochs, save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = [100*x for x in range(len(ctc_train))]\n",
    "\n",
    "plt.plot(iterations, ctc_train, label='Training')\n",
    "plt.plot(iterations, ctc_eval, label='Validation')\n",
    "\n",
    "plt.title('CTC Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data, test_labels, test_label_lengths = primus.next_batch(\"test\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([864, 4, 1782])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(test_data).log_softmax(2)\n",
    "pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5579, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lengths = Variable(torch.IntTensor([pred.size(0)] * pred.size(1))).to(device)\n",
    "error = loss_function(pred, test_labels.detach(), pred_lengths.detach(), test_label_lengths.detach())\n",
    "                        \n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del error\n",
    "del pred_lengths\n",
    "del test_label_lengths\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0079277e+01, -2.2853819e+01, -1.9502142e+01, ...,\n",
       "        -2.9497711e+01, -3.4407841e+01, -1.3284331e+01],\n",
       "       [-2.0749731e+01, -2.7297729e+01, -2.1303400e+01, ...,\n",
       "        -1.5188000e+01, -2.3493229e+01, -1.3332623e+01],\n",
       "       [-2.5428980e+01, -2.9635818e+01, -2.6009523e+01, ...,\n",
       "        -6.7628396e-04, -9.3662891e+00, -1.8666414e+01],\n",
       "       ...,\n",
       "       [-1.2250779e+01, -2.1558317e+01, -4.0364513e+01, ...,\n",
       "        -3.7529530e+01, -3.6446743e+01, -5.4836123e-06],\n",
       "       [-1.2235142e+01, -2.0348257e+01, -3.8395840e+01, ...,\n",
       "        -3.4706066e+01, -3.3353859e+01, -8.4638241e-06],\n",
       "       [-9.7797403e+00, -2.0355101e+01, -4.0521023e+01, ...,\n",
       "        -3.7851166e+01, -3.7670063e+01, -5.8172442e-05]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_np = pred.cpu().detach().numpy()[:,0]\n",
    "pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4,  229, 1779,  262, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781,    0, 1781, 1727, 1781, 1781, 1781, 1280, 1781,  655,\n",
       "       1781, 1781, 1727, 1781, 1781,  655, 1781, 1781,  387, 1781,  668,\n",
       "          0, 1781, 1559, 1781, 1781, 1559, 1781, 1727, 1781, 1781, 1781,\n",
       "       1574, 1781, 1781,  480, 1781, 1781, 1781,  668,    0,   38, 1781,\n",
       "       1781,    0, 1781, 1781, 1781, 1781, 1722, 1781, 1781, 1781, 1781,\n",
       "       1781,    0, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "       1781, 1781, 1781, 1781, 1781, 1781], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_np = pred_np.argmax(1).flatten()\n",
    "pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_to_string(arr, int2word, blank):\n",
    "    result = []\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] != blank:\n",
    "            if i == 0 or arr[i] != arr[i-1]:\n",
    "                result.append(int2word[arr[i]])\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 74,  49,  58,  97, 101, 631, 577, 631, 577,  64, 465, 265, 359,\n",
       "       473, 251, 367,  64, 473, 253, 370, 465, 267, 362,  64], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = test_labels.cpu().detach().numpy()[]\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clef.G-L2',\n",
       " 'accidental.sharp-L5',\n",
       " 'accidental.sharp-S3',\n",
       " 'digit.6-L4',\n",
       " 'digit.8-L2',\n",
       " 'note.quarter-L4',\n",
       " 'note.eighth-S0',\n",
       " 'note.quarter-L4',\n",
       " 'note.eighth-S0',\n",
       " 'barline-L1',\n",
       " 'note.beamedRight1-L3',\n",
       " 'note.beamedBoth1-S2',\n",
       " 'note.beamedLeft1-L2',\n",
       " 'note.beamedRight1-S1',\n",
       " 'note.beamedBoth1-L1',\n",
       " 'note.beamedLeft1-S0',\n",
       " 'barline-L1',\n",
       " 'note.beamedRight1-S1',\n",
       " 'note.beamedBoth1-L2',\n",
       " 'note.beamedLeft1-S2',\n",
       " 'note.beamedRight1-L3',\n",
       " 'note.beamedBoth1-S3',\n",
       " 'note.beamedLeft1-L4',\n",
       " 'barline-L1']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_str = log_softmax_to_string(test_labels, int2word, vocab_size)\n",
    "label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clef-C4',\n",
       " 'keySignature-EbM',\n",
       " 'timeSignature-C',\n",
       " 'multirest-21',\n",
       " 'barline',\n",
       " 'rest-quarter',\n",
       " 'note-Eb4_eighth',\n",
       " 'note-Bb3_eighth',\n",
       " 'rest-quarter',\n",
       " 'note-Bb3_eighth',\n",
       " 'note-A3_sixteenth',\n",
       " 'note-Bb3_sixteenth',\n",
       " 'barline',\n",
       " 'note-G3_eighth',\n",
       " 'note-G3_eighth',\n",
       " 'rest-quarter',\n",
       " 'note-G3_quarter',\n",
       " 'note-Ab3_eighth.',\n",
       " 'note-Bb3_sixteenth',\n",
       " 'barline',\n",
       " 'gracenote-Ab3_eighth',\n",
       " 'barline',\n",
       " 'rest-half',\n",
       " 'barline']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_str = log_softmax_to_string(pred_np, int2word, vocab_size)\n",
    "pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9583333333333334, 0.03993055555555556)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein(label_str, pred_str), edit_distance(label_str, pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CTCLoss(blank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = torch.tensor([[[0.01,0.01,0.96,0.01,0.01]],\n",
    "            [[0.96,0.01,0.01,0.01,0.01]],\n",
    "            [[0.01,0.01,0.01,0.01,0.01]]])\n",
    "length = torch.tensor([[3]])\n",
    "label = torch.tensor([2])\n",
    "label_length = torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.8299)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(prob,label,length,label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a), len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bkai",
   "language": "python",
   "name": "bkai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
