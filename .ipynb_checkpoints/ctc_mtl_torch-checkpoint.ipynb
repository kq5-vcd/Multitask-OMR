{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#assert device == 'cuda:0' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return (255. - image)/255.\n",
    "\n",
    "\n",
    "def resize(image, height):\n",
    "    #new_width = old_width * (new_height/old_height)\n",
    "    width = int(float(height * image.shape[1]) / image.shape[0]) \n",
    "    sample_img = cv2.resize(image, (width, height))\n",
    "    \n",
    "    del width\n",
    "    \n",
    "    return sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(a,b):\n",
    "    \"Computes the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "\n",
    "    if n > m:\n",
    "        a,b = b,a\n",
    "        n,m = m,n\n",
    "\n",
    "    current = range(n+1)\n",
    "    for i in range(1,m+1):\n",
    "        previous, current = current, [i]+[0]*n\n",
    "        for j in range(1,n+1):\n",
    "            add, delete = previous[j]+1, current[j-1]+1\n",
    "            change = previous[j-1]\n",
    "            if a[j-1] != b[i-1]:\n",
    "                change = change + 1\n",
    "            current[j] = min(add, delete, change)\n",
    "\n",
    "    return current[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_pct(a,b):\n",
    "    max_len = max(len(a), len(b))\n",
    "    \n",
    "    return levenshtein(a,b)/max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_levenshtein(a,b):\n",
    "    n, m = len(a), len(b)\n",
    "    dist = 0\n",
    "\n",
    "    if n > m:\n",
    "        a,b = b,a\n",
    "        n,m = m,n\n",
    "        \n",
    "    for i in range(n):\n",
    "        word = a[i]\n",
    "        \n",
    "        b_seq = b[:len(b)-n+i+1]\n",
    "        \n",
    "        seq_dist = [levenshtein_pct(word, x) for x in b_seq]\n",
    "        best_val = min(seq_dist)\n",
    "        dest_idx = seq_dist.index(best_val)\n",
    "        \n",
    "        dist += dest_idx + best_val\n",
    "        \n",
    "        if i == n - 1:\n",
    "            dist += len(b) - dest_idx - 1\n",
    "            break\n",
    "            \n",
    "        b = b[dest_idx+1:]\n",
    "            \n",
    "\n",
    "    return dist/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_np_array(t):\n",
    "    t = t.cpu().detach().numpy()\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_string(arr, int2word, blank):\n",
    "    result = []\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] != blank:\n",
    "            if i == 0 or arr[i] != arr[i-1]:\n",
    "                result.append(int2word[arr[i]])\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_symbols(dict_path):\n",
    "    word2int = {} #map symbols to numeric values\n",
    "    int2word = {} #map numeric values to symbols\n",
    "\n",
    "    dict_file = open(dict_path,'r')\n",
    "    dict_list = dict_file.read().splitlines()\n",
    "    \n",
    "    for word in dict_list:\n",
    "        if not word in word2int:\n",
    "            word_idx = len(word2int)\n",
    "            word2int[word] = word_idx\n",
    "            int2word[word_idx] = word\n",
    "\n",
    "    dict_file.close()\n",
    "    \n",
    "    del dict_file\n",
    "    \n",
    "    return (word2int, int2word, len(int2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriMuS:\n",
    "    # Data preprocessor/loader for the model\n",
    "    gt_element_separator = '-'\n",
    "    PAD_COLUMN = 0\n",
    "\n",
    "\n",
    "    def __init__(self, corpus_dirpath, corpus_path, word2int_sem, word2int_agn, \n",
    "                 img_height, batch_size, img_channels, \n",
    "                 train_split=0.5, test_split=0.5, \n",
    "                 shuffle=True, distorted=False):\n",
    "        corpus_file = open(corpus_path,'r')\n",
    "        corpus_list = corpus_file.read().splitlines()\n",
    "        corpus_file.close()\n",
    "        \n",
    "        del corpus_file\n",
    "\n",
    "        # Train and validation split\n",
    "        if shuffle:\n",
    "            random.shuffle(corpus_list) \n",
    "            \n",
    "        train_idx = int(len(corpus_list) * train_split) \n",
    "        test_idx = int(len(corpus_list) * test_split) \n",
    "\n",
    "        self.training_list = corpus_list[:train_idx]\n",
    "        self.validation_list = corpus_list[train_idx:-test_idx]\n",
    "        self.test_list = corpus_list[-test_idx:]\n",
    "        \n",
    "        del train_idx\n",
    "        del test_idx\n",
    "\n",
    "        print ('Training with ' + str(len(self.training_list)) + ', validating with ' \n",
    "               + str(len(self.validation_list)) + ', and testing with ' + str(len(self.test_list)))\n",
    "        \n",
    "        self.corpus_dirpath = corpus_dirpath\n",
    "        self.distorted = distorted\n",
    "        \n",
    "        self.current_idx = 0 #identify current index in list of samples\n",
    "        self.current_eval_idx = 0\n",
    "        self.current_test_idx = 0\n",
    "\n",
    "        # Dictionary\n",
    "        self.word2int_sem = word2int_sem #map symbols to numeric values\n",
    "        self.word2int_agn = word2int_agn\n",
    "        \n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "        self.img_channels = img_channels\n",
    "        \n",
    "        self.training_iterations = int(len(self.training_list)/self.batch_size) + 1\n",
    "        self.eval_iterations = int(len(self.validation_list)/self.batch_size) + 1\n",
    "        self.test_iterations = int(len(self.test_list)/self.batch_size) + 1\n",
    "        \n",
    "    \n",
    "    def load_data(self, filepath):\n",
    "        sample_fullpath = self.corpus_dirpath + '/' + filepath + '/' + filepath\n",
    "        #print(sample_filepath)\n",
    "\n",
    "        # IMAGE\n",
    "        img_src = sample_fullpath + '_distorted.jpg' if self.distorted else sample_fullpath + '.png'\n",
    "        image = cv2.imread(img_src, False)  # Grayscale is assumed!\n",
    "        \n",
    "        del img_src\n",
    "        \n",
    "        image = resize(image, self.img_height)\n",
    "        image = normalize(image)\n",
    "        \n",
    "\n",
    "        # GROUND TRUTH\n",
    "        sample_gt_file_sem = open(sample_fullpath + '.semantic', 'r')\n",
    "        sample_gt_plain_sem = sample_gt_file_sem.readline().rstrip().split('\\t')\n",
    "        sample_gt_file_sem.close()\n",
    "        \n",
    "        del sample_gt_file_sem\n",
    "        \n",
    "        sample_gt_file_agn = open(sample_fullpath + '.agnostic', 'r')\n",
    "        sample_gt_plain_agn = sample_gt_file_agn.readline().rstrip().split('\\t')\n",
    "        sample_gt_file_agn.close()\n",
    "        \n",
    "        del sample_fullpath\n",
    "        del sample_gt_file_agn\n",
    "\n",
    "        label_sem = [self.word2int_sem[lab] for lab in sample_gt_plain_sem] #label: list of numeric values\n",
    "        del sample_gt_plain_sem\n",
    "        \n",
    "        label_agn = [self.word2int_agn[lab] for lab in sample_gt_plain_agn] \n",
    "        del sample_gt_plain_agn\n",
    "        \n",
    "        return (image, label_sem, label_agn)\n",
    "    \n",
    "    \n",
    "    def transform_to_batch(self, images):\n",
    "        # Extend all images to match the longest in the batch\n",
    "        image_widths = [img.shape[1] for img in images]\n",
    "        max_image_width = max(image_widths)\n",
    "        \n",
    "        del image_widths\n",
    "\n",
    "        batch_images = np.ones(shape=[len(images),\n",
    "                                      self.img_channels,\n",
    "                                      self.img_height,\n",
    "                                      max_image_width], dtype=np.float32)*self.PAD_COLUMN\n",
    "        # batch shape: (b, c, h, w)\n",
    "        \n",
    "        del max_image_width\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            batch_images[i, 0, 0:img.shape[0], 0:img.shape[1]] = img\n",
    "        # shorter images will be padded\n",
    "\n",
    "        return batch_images\n",
    "\n",
    "        \n",
    "    def next_batch(self, phase=\"train\"):\n",
    "        # Create a batch\n",
    "        images = [] \n",
    "        labels_sem = []\n",
    "        labels_agn = []\n",
    "\n",
    "        if phase == \"train\":\n",
    "            for _ in range(self.batch_size):\n",
    "                image, label_sem, label_agn = self.load_data(self.training_list[self.current_idx])\n",
    "                images.append(image)\n",
    "                labels_sem.append(label_sem)\n",
    "                labels_agn.append(label_agn)\n",
    "\n",
    "                self.current_idx = (self.current_idx + 1) % len(self.training_list) #increment index, turn back to beginning if overflow\n",
    "        elif phase == \"eval\":\n",
    "            for _ in range(self.batch_size):\n",
    "                image, label_sem, label_agn = self.load_data(self.validation_list[self.current_eval_idx])\n",
    "                images.append(image)\n",
    "                labels_sem.append(label_sem)\n",
    "                labels_agn.append(label_agn)\n",
    "                \n",
    "                self.current_eval_idx = (self.current_eval_idx + 1) % len(self.validation_list)\n",
    "        elif phase == \"test\":\n",
    "            for _ in range(self.batch_size):\n",
    "                image, label_sem, label_agn = self.load_data(self.test_list[self.current_test_idx])\n",
    "                images.append(image)\n",
    "                labels_sem.append(label_sem)\n",
    "                labels_agn.append(label_agn)\n",
    "            \n",
    "                self.current_test_idx = (self.current_test_idx + 1) % len(self.test_list)\n",
    "\n",
    "        # Transform to batch\n",
    "        batch_images = self.transform_to_batch(images)\n",
    "        target_lengths_sem = [len(x) for x in labels_sem]\n",
    "        target_lengths_agn = [len(x) for x in labels_agn]\n",
    "        \n",
    "        flattened_labels_sem = []\n",
    "        for label in labels_sem:\n",
    "            flattened_labels_sem += label\n",
    "        del labels_sem\n",
    "        \n",
    "        flattened_labels_agn = []\n",
    "        for label in labels_agn:\n",
    "            flattened_labels_agn += label\n",
    "        del labels_agn\n",
    "        \n",
    "        batch_images = torch.from_numpy(batch_images).to(device)\n",
    "        labels_sem = torch.tensor(flattened_labels_sem).to(device)\n",
    "        target_lengths_sem = torch.tensor(target_lengths_sem, dtype=torch.int32).to(device)\n",
    "        labels_agn = torch.tensor(flattened_labels_agn).to(device)\n",
    "        target_lengths_agn = torch.tensor(target_lengths_agn, dtype=torch.int32).to(device)\n",
    "\n",
    "        return (batch_images, labels_sem, target_lengths_sem, labels_agn, target_lengths_agn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut, dropout=0):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True, dropout=dropout)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        \n",
    "        del h\n",
    "        del recurrent\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        \n",
    "        del T\n",
    "        del b\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, img_height, nclass_sem, nclass_agn, conv_layers, rnn_hidden_states, dropout=0.5, kernel_size=3, pooling_size=2):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert img_height % 16 == 0, 'img_height has to be a multiple of 16'\n",
    "\n",
    "        self.cnn = nn.Sequential()\n",
    "        for i in range(len(conv_layers)):\n",
    "            input_channels = input_size if i == 0 else conv_layers[i-1]\n",
    "            output_channels = conv_layers[i]\n",
    "\n",
    "            self.cnn.add_module('conv{0}'.format(i),\n",
    "                                nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, padding = 1))\n",
    "            self.cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(output_channels))\n",
    "            self.cnn.add_module('relu{0}'.format(i), nn.LeakyReLU(0.2, inplace=True))\n",
    "            self.cnn.add_module('pooling{0}'.format(i), nn.MaxPool2d(pooling_size, 2))\n",
    "            \n",
    "            del input_channels\n",
    "            del output_channels\n",
    "            \n",
    "        self.rnn_sem = nn.Sequential()\n",
    "        for i in range(len(rnn_hidden_states)):\n",
    "            inC = conv_layers[-1] if i == 0 else rnn_hidden_states[i-1]\n",
    "\n",
    "            if i < len(rnn_hidden_states) - 1:\n",
    "                self.rnn_sem.add_module('BiLSTM{0}'.format(i), \n",
    "                                    BidirectionalLSTM(inC, rnn_hidden_states[i], \n",
    "                                                      rnn_hidden_states[i], dropout=dropout))\n",
    "            else:\n",
    "                self.rnn_sem.add_module('BiLSTM{0}'.format(i), \n",
    "                                    BidirectionalLSTM(inC, rnn_hidden_states[i], nclass_sem))\n",
    "\n",
    "            del inC\n",
    "\n",
    "        self.rnn_agn = nn.Sequential()\n",
    "        for i in range(len(rnn_hidden_states)):\n",
    "            inC = conv_layers[-1] if i == 0 else rnn_hidden_states[i-1]\n",
    "\n",
    "            if i < len(rnn_hidden_states) - 1:\n",
    "                self.rnn_agn.add_module('BiLSTM{0}'.format(i), \n",
    "                                    BidirectionalLSTM(inC, rnn_hidden_states[i], \n",
    "                                                      rnn_hidden_states[i], dropout=dropout))\n",
    "            else:\n",
    "                self.rnn_agn.add_module('BiLSTM{0}'.format(i), \n",
    "                                    BidirectionalLSTM(inC, rnn_hidden_states[i], nclass_agn))\n",
    "\n",
    "            del inC\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        conv = conv.view(b, c, -1)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        #print(conv.size())\n",
    "        \n",
    "        del b\n",
    "        del c\n",
    "        del h\n",
    "        del w\n",
    "\n",
    "        # rnn features\n",
    "        output1 = self.rnn_sem(conv)\n",
    "        output2 = self.rnn_agn(conv)\n",
    "\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multitask_loss(loss_func1, loss_func2,\n",
    "                   batch_sem, batch_agn, \n",
    "                   labels_sem, labels_agn, \n",
    "                   batch_lengths, label_lengths_sem, label_lengths_agn):\n",
    "    l1 = loss_func1(batch_sem, labels_sem, batch_lengths, label_lengths_sem)\n",
    "    l2 = loss_func2(batch_agn, labels_agn, batch_lengths, label_lengths_agn)\n",
    "    return   l1, l2, l1+l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "corpus = 'Data/Corpus' #Path to the corpus\n",
    "set_path = 'Data/Corpus_list.txt' #Path to the set file\n",
    "save_model = 'Models/trained_multitask_distorted_model.pth' #Path to save the model\n",
    "vocabulary_sem = 'Data/vocabulary_semantic.txt' #Path to the vocabulary file\n",
    "vocabulary_agn = 'Data/vocabulary_agnostic.txt'\n",
    "\n",
    "train_split = 0.7\n",
    "test_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int_sem, int2word_sem, vocab_size_sem = load_symbols(vocabulary_sem)\n",
    "word2int_agn, int2word_agn, vocab_size_agn = load_symbols(vocabulary_agn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 128\n",
    "img_width = None\n",
    "batch_size = 4\n",
    "img_channels = 1\n",
    "conv_channels = [32, 64, 128, 256]\n",
    "conv_filter_size = 3\n",
    "conv_pooling_size = 2\n",
    "rnn_hidden_states = [256,256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterization\n",
    "epochs = 5\n",
    "dropout = 0.5\n",
    "nclass_sem = vocab_size_sem + 1\n",
    "nclass_agn = vocab_size_agn + 1\n",
    "input_size = 1\n",
    "\n",
    "prepared = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\envs\\bkai\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (cnn): Sequential(\n",
      "    (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (pooling2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (pooling3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rnn_sem): Sequential(\n",
      "    (BiLSTM0): BidirectionalLSTM(\n",
      "      (rnn): LSTM(256, 256, dropout=0.5, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (BiLSTM1): BidirectionalLSTM(\n",
      "      (rnn): LSTM(256, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=1782, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rnn_agn): Sequential(\n",
      "    (BiLSTM0): BidirectionalLSTM(\n",
      "      (rnn): LSTM(256, 256, dropout=0.5, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (BiLSTM1): BidirectionalLSTM(\n",
      "      (rnn): LSTM(256, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=759, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CRNN(input_size, img_height, \n",
    "             nclass_sem, nclass_agn, \n",
    "             conv_channels, \n",
    "             rnn_hidden_states, \n",
    "             dropout=dropout)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "    \n",
    "if prepared:\n",
    "    model.load_state_dict(torch.load(save_model))\n",
    "    \n",
    "loss_function_sem = nn.CTCLoss(blank=vocab_size_sem)\n",
    "loss_function_agn = nn.CTCLoss(blank=vocab_size_agn)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.99)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 61374,validating with 17537, and testing with 8767\n"
     ]
    }
   ],
   "source": [
    "# Load primus\n",
    "primus = PriMuS(corpus, set_path, word2int_sem, word2int_agn, img_height, \n",
    "                batch_size, img_channels, train_split=train_split, test_split=test_split, \n",
    "                shuffle=False, distorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, dataloader, epochs, checkpoint):\n",
    "    ctc_train = []\n",
    "    ctc_train_sem = []\n",
    "    ctc_train_agn = []\n",
    "    \n",
    "    ctc_eval = []\n",
    "    ctc_eval_sem = []\n",
    "    ctc_eval_agn = []\n",
    "    \n",
    "    best_val = 10000\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        print(\"Epoch:\", epoch+1)\n",
    "        \n",
    "        for i in range(dataloader.training_iterations):\n",
    "            batch, labels_sem, label_lengths_sem, labels_agn, label_lengths_agn = dataloader.next_batch()\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds_sem, preds_agn = model(batch)\n",
    "            del batch\n",
    "            \n",
    "            preds_sem = preds_sem.log_softmax(2)\n",
    "            preds_agn = preds_agn.log_softmax(2)\n",
    "            \n",
    "            batch_lengths = Variable(torch.IntTensor([preds_sem.size(0)] * preds_sem.size(1))).to(device)\n",
    "            \n",
    "            loss_sem, loss_agn, loss = multitask_loss(loss_function_sem, loss_function_agn,\n",
    "                                                      preds_sem, preds_agn,\n",
    "                                                      labels_sem.detach(), labels_agn.detach(), \n",
    "                                                      batch_lengths.detach(), \n",
    "                                                      label_lengths_sem.detach(), label_lengths_agn.detach())\n",
    "\n",
    "            del labels_sem\n",
    "            del labels_agn\n",
    "            del batch_lengths\n",
    "            del label_lengths_sem\n",
    "            del label_lengths_agn\n",
    "            \n",
    "            del preds_sem\n",
    "            del preds_agn\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "            if i % 100 == 99 or i == dataloader.training_iterations - 1:\n",
    "                ctc_train.append(loss.item()) \n",
    "                ctc_train_sem.append(loss_sem.item()) \n",
    "                ctc_train_agn.append(loss_agn.item()) \n",
    "                print(f'Iteration: {i+1:4}')\n",
    "                print(f'Semantic loss: {loss_sem.item():10.8f}')\n",
    "                print(f'Agnostic loss: {loss_agn.item():10.8f}')\n",
    "                print(f'Total loss: {loss.item():10.8f}')\n",
    "            del loss\n",
    "            del loss_sem\n",
    "            del loss_agn\n",
    "\n",
    "            if i % 100 == 99 or i == dataloader.training_iterations - 1:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    eval_data, eval_labels_sem, eval_label_lengths_sem, eval_labels_agn, eval_label_lengths_agn = dataloader.next_batch(\"eval\")\n",
    "                    eval_preds_sem, eval_preds_agn = model(eval_data)\n",
    "                    del eval_data\n",
    "                    \n",
    "                    eval_preds_sem = eval_preds_sem.log_softmax(2)\n",
    "                    eval_preds_agn = eval_preds_agn.log_softmax(2)\n",
    "\n",
    "                    eval_lengths = Variable(torch.IntTensor([eval_preds_sem.size(0)] * eval_preds_sem.size(1))).to(device)\n",
    "                    error_sem, error_agn, error = multitask_loss(loss_function_sem, loss_function_agn,\n",
    "                                                                 eval_preds_sem, eval_preds_agn,\n",
    "                                                                 eval_labels_sem.detach(), eval_labels_agn.detach(), \n",
    "                                                                 eval_lengths.detach(), \n",
    "                                                                 eval_label_lengths_sem.detach(), eval_label_lengths_agn.detach())\n",
    "                    \n",
    "                    del eval_labels_sem\n",
    "                    del eval_labels_agn\n",
    "                    del eval_lengths\n",
    "                    del eval_label_lengths_sem\n",
    "                    del eval_label_lengths_agn\n",
    "                    del eval_preds_sem\n",
    "                    del eval_preds_agn\n",
    "\n",
    "                    err = error.item()\n",
    "                    del error\n",
    "                        \n",
    "                    ctc_eval.append(err)\n",
    "                    ctc_eval_sem.append(error_sem.item()) \n",
    "                    ctc_eval_agn.append(error_agn.item()) \n",
    "\n",
    "                    if err < best_val:\n",
    "                        best_val = err\n",
    "                        torch.save(model.state_dict(), checkpoint)\n",
    "\n",
    "                print(f'Validation loss: {err:10.8f}')\n",
    "                print(f'Semantic loss: {error_sem.item():10.8f}')\n",
    "                print(f'Agnostic loss: {error_agn.item():10.8f}')\n",
    "                \n",
    "                del err\n",
    "                del error_sem\n",
    "                del error_agn\n",
    "        \n",
    "    return (ctc_train, ctc_train_sem, ctc_train_agn, ctc_eval, ctc_eval_sem, ctc_eval_agn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Iteration:  100\n",
      "Semantic loss: 4.77673721\n",
      "Agnostic loss: 5.37628651\n",
      "Total loss: 10.15302372\n",
      "Validation loss: 12.50626087\n",
      "Semantic loss: 6.08666754\n",
      "Agnostic loss: 6.41959333\n",
      "Iteration:  200\n",
      "Semantic loss: 5.96919632\n",
      "Agnostic loss: 5.60197067\n",
      "Total loss: 11.57116699\n",
      "Validation loss: 10.50467682\n",
      "Semantic loss: 5.37535667\n",
      "Agnostic loss: 5.12932014\n",
      "Iteration:  300\n",
      "Semantic loss: 5.89659786\n",
      "Agnostic loss: 6.31306410\n",
      "Total loss: 12.20966148\n",
      "Validation loss: 11.89163208\n",
      "Semantic loss: 6.24059772\n",
      "Agnostic loss: 5.65103388\n",
      "Iteration:  400\n",
      "Semantic loss: 5.30295753\n",
      "Agnostic loss: 5.66948175\n",
      "Total loss: 10.97243881\n",
      "Validation loss: 9.88813400\n",
      "Semantic loss: 5.04666138\n",
      "Agnostic loss: 4.84147263\n",
      "Iteration:  500\n",
      "Semantic loss: 5.53014374\n",
      "Agnostic loss: 5.38388157\n",
      "Total loss: 10.91402531\n",
      "Validation loss: 9.96750259\n",
      "Semantic loss: 4.87744713\n",
      "Agnostic loss: 5.09005547\n",
      "Iteration:  600\n",
      "Semantic loss: 4.56166267\n",
      "Agnostic loss: 5.15135574\n",
      "Total loss: 9.71301842\n",
      "Validation loss: 11.47172737\n",
      "Semantic loss: 5.76544285\n",
      "Agnostic loss: 5.70628452\n",
      "Iteration:  700\n",
      "Semantic loss: 4.75803757\n",
      "Agnostic loss: 5.18385696\n",
      "Total loss: 9.94189453\n",
      "Validation loss: 10.14757729\n",
      "Semantic loss: 5.00400162\n",
      "Agnostic loss: 5.14357567\n",
      "Iteration:  800\n",
      "Semantic loss: 5.21012306\n",
      "Agnostic loss: 5.56886435\n",
      "Total loss: 10.77898788\n",
      "Validation loss: 9.81737900\n",
      "Semantic loss: 4.65143013\n",
      "Agnostic loss: 5.16594839\n",
      "Iteration:  900\n",
      "Semantic loss: 5.15780544\n",
      "Agnostic loss: 5.58036423\n",
      "Total loss: 10.73816967\n",
      "Validation loss: 9.35722923\n",
      "Semantic loss: 4.74532890\n",
      "Agnostic loss: 4.61190033\n",
      "Iteration: 1000\n",
      "Semantic loss: 4.78767204\n",
      "Agnostic loss: 4.87849474\n",
      "Total loss: 9.66616631\n",
      "Validation loss: 10.57195568\n",
      "Semantic loss: 5.69350243\n",
      "Agnostic loss: 4.87845325\n",
      "Iteration: 1100\n",
      "Semantic loss: 4.64106560\n",
      "Agnostic loss: 4.67836952\n",
      "Total loss: 9.31943512\n",
      "Validation loss: 9.00072098\n",
      "Semantic loss: 4.56822729\n",
      "Agnostic loss: 4.43249416\n",
      "Iteration: 1200\n",
      "Semantic loss: 5.68267536\n",
      "Agnostic loss: 5.70486212\n",
      "Total loss: 11.38753700\n",
      "Validation loss: 10.08193016\n",
      "Semantic loss: 5.00489902\n",
      "Agnostic loss: 5.07703114\n",
      "Iteration: 1300\n",
      "Semantic loss: 5.24232578\n",
      "Agnostic loss: 5.22703838\n",
      "Total loss: 10.46936417\n",
      "Validation loss: 9.69167519\n",
      "Semantic loss: 4.94806671\n",
      "Agnostic loss: 4.74360847\n",
      "Iteration: 1400\n",
      "Semantic loss: 5.51458979\n",
      "Agnostic loss: 5.01433945\n",
      "Total loss: 10.52892876\n",
      "Validation loss: 10.50574875\n",
      "Semantic loss: 5.14620399\n",
      "Agnostic loss: 5.35954475\n",
      "Iteration: 1500\n",
      "Semantic loss: 5.02341413\n",
      "Agnostic loss: 5.08794212\n",
      "Total loss: 10.11135674\n",
      "Validation loss: 9.05286026\n",
      "Semantic loss: 4.75000000\n",
      "Agnostic loss: 4.30286026\n",
      "Iteration: 1600\n",
      "Semantic loss: 4.98097324\n",
      "Agnostic loss: 4.77671242\n",
      "Total loss: 9.75768566\n",
      "Validation loss: 11.43134499\n",
      "Semantic loss: 5.79736614\n",
      "Agnostic loss: 5.63397837\n",
      "Iteration: 1700\n",
      "Semantic loss: 5.30810165\n",
      "Agnostic loss: 4.19564056\n",
      "Total loss: 9.50374222\n",
      "Validation loss: 9.69897461\n",
      "Semantic loss: 5.37615681\n",
      "Agnostic loss: 4.32281780\n",
      "Iteration: 1800\n",
      "Semantic loss: 4.69729519\n",
      "Agnostic loss: 4.33829403\n",
      "Total loss: 9.03558922\n",
      "Validation loss: 9.71227169\n",
      "Semantic loss: 5.33317089\n",
      "Agnostic loss: 4.37910080\n",
      "Iteration: 1900\n",
      "Semantic loss: 5.48602057\n",
      "Agnostic loss: 4.62007236\n",
      "Total loss: 10.10609245\n",
      "Validation loss: 9.86056519\n",
      "Semantic loss: 5.16235352\n",
      "Agnostic loss: 4.69821167\n",
      "Iteration: 2000\n",
      "Semantic loss: 5.05905628\n",
      "Agnostic loss: 4.33447075\n",
      "Total loss: 9.39352703\n",
      "Validation loss: 10.18903255\n",
      "Semantic loss: 5.41373158\n",
      "Agnostic loss: 4.77530098\n",
      "Iteration: 2100\n",
      "Semantic loss: 4.99193573\n",
      "Agnostic loss: 4.96439791\n",
      "Total loss: 9.95633316\n",
      "Validation loss: 9.46231270\n",
      "Semantic loss: 5.14693117\n",
      "Agnostic loss: 4.31538200\n",
      "Iteration: 2200\n",
      "Semantic loss: 4.59976006\n",
      "Agnostic loss: 4.63842297\n",
      "Total loss: 9.23818302\n",
      "Validation loss: 9.40425968\n",
      "Semantic loss: 4.65689945\n",
      "Agnostic loss: 4.74736023\n",
      "Iteration: 2300\n",
      "Semantic loss: 5.08716440\n",
      "Agnostic loss: 4.95276928\n",
      "Total loss: 10.03993416\n",
      "Validation loss: 10.86563683\n",
      "Semantic loss: 5.80932283\n",
      "Agnostic loss: 5.05631351\n",
      "Iteration: 2400\n",
      "Semantic loss: 4.23140621\n",
      "Agnostic loss: 4.29003668\n",
      "Total loss: 8.52144241\n",
      "Validation loss: 9.35644436\n",
      "Semantic loss: 4.56584930\n",
      "Agnostic loss: 4.79059505\n",
      "Iteration: 2500\n",
      "Semantic loss: 4.32344723\n",
      "Agnostic loss: 4.17614460\n",
      "Total loss: 8.49959183\n",
      "Validation loss: 9.06847763\n",
      "Semantic loss: 4.65429115\n",
      "Agnostic loss: 4.41418648\n",
      "Iteration: 2600\n",
      "Semantic loss: 4.30518055\n",
      "Agnostic loss: 4.27455616\n",
      "Total loss: 8.57973671\n",
      "Validation loss: 8.92726707\n",
      "Semantic loss: 4.37810469\n",
      "Agnostic loss: 4.54916191\n",
      "Iteration: 2700\n",
      "Semantic loss: 5.51977158\n",
      "Agnostic loss: 4.82909441\n",
      "Total loss: 10.34886551\n",
      "Validation loss: 9.12289619\n",
      "Semantic loss: 4.54762697\n",
      "Agnostic loss: 4.57526970\n",
      "Iteration: 2800\n",
      "Semantic loss: 3.83361387\n",
      "Agnostic loss: 4.23682785\n",
      "Total loss: 8.07044220\n",
      "Validation loss: 9.45651817\n",
      "Semantic loss: 4.59164762\n",
      "Agnostic loss: 4.86487055\n",
      "Iteration: 2900\n",
      "Semantic loss: 4.59669590\n",
      "Agnostic loss: 4.39436436\n",
      "Total loss: 8.99106026\n",
      "Validation loss: 8.32607079\n",
      "Semantic loss: 4.16007566\n",
      "Agnostic loss: 4.16599560\n",
      "Iteration: 3000\n",
      "Semantic loss: 5.34598827\n",
      "Agnostic loss: 4.91508675\n",
      "Total loss: 10.26107502\n",
      "Validation loss: 9.51650810\n",
      "Semantic loss: 4.54153872\n",
      "Agnostic loss: 4.97496891\n",
      "Iteration: 3100\n",
      "Semantic loss: 5.15488291\n",
      "Agnostic loss: 5.50184917\n",
      "Total loss: 10.65673256\n",
      "Validation loss: 9.43399048\n",
      "Semantic loss: 5.16429615\n",
      "Agnostic loss: 4.26969433\n",
      "Iteration: 3200\n",
      "Semantic loss: 4.07881260\n",
      "Agnostic loss: 3.90815115\n",
      "Total loss: 7.98696375\n",
      "Validation loss: 9.04325104\n",
      "Semantic loss: 4.57312965\n",
      "Agnostic loss: 4.47012138\n",
      "Iteration: 3300\n",
      "Semantic loss: 4.30774689\n",
      "Agnostic loss: 4.50673103\n",
      "Total loss: 8.81447792\n",
      "Validation loss: 9.22937012\n",
      "Semantic loss: 4.71959162\n",
      "Agnostic loss: 4.50977898\n",
      "Iteration: 3400\n",
      "Semantic loss: 4.94648838\n",
      "Agnostic loss: 4.56079674\n",
      "Total loss: 9.50728512\n",
      "Validation loss: 8.84644032\n",
      "Semantic loss: 4.57041550\n",
      "Agnostic loss: 4.27602482\n",
      "Iteration: 3500\n",
      "Semantic loss: 4.61625576\n",
      "Agnostic loss: 3.98583555\n",
      "Total loss: 8.60209084\n",
      "Validation loss: 8.79682446\n",
      "Semantic loss: 4.33266687\n",
      "Agnostic loss: 4.46415758\n",
      "Iteration: 3600\n",
      "Semantic loss: 4.58641863\n",
      "Agnostic loss: 4.47887230\n",
      "Total loss: 9.06529045\n",
      "Validation loss: 9.95225620\n",
      "Semantic loss: 5.62556553\n",
      "Agnostic loss: 4.32669067\n",
      "Iteration: 3700\n",
      "Semantic loss: 4.41040659\n",
      "Agnostic loss: 4.30266857\n",
      "Total loss: 8.71307564\n",
      "Validation loss: 8.81529045\n",
      "Semantic loss: 4.48771572\n",
      "Agnostic loss: 4.32757473\n",
      "Iteration: 3800\n",
      "Semantic loss: 4.28305435\n",
      "Agnostic loss: 4.28822994\n",
      "Total loss: 8.57128429\n",
      "Validation loss: 9.33569908\n",
      "Semantic loss: 4.39553308\n",
      "Agnostic loss: 4.94016600\n",
      "Iteration: 3900\n",
      "Semantic loss: 4.46361256\n",
      "Agnostic loss: 4.70883894\n",
      "Total loss: 9.17245102\n",
      "Validation loss: 8.31554794\n",
      "Semantic loss: 4.20711756\n",
      "Agnostic loss: 4.10842991\n",
      "Iteration: 4000\n",
      "Semantic loss: 4.40350151\n",
      "Agnostic loss: 4.23109770\n",
      "Total loss: 8.63459969\n",
      "Validation loss: 8.33994770\n",
      "Semantic loss: 4.40279818\n",
      "Agnostic loss: 3.93714952\n",
      "Iteration: 4100\n",
      "Semantic loss: 4.25647926\n",
      "Agnostic loss: 4.61105299\n",
      "Total loss: 8.86753273\n",
      "Validation loss: 8.38558197\n",
      "Semantic loss: 4.35940361\n",
      "Agnostic loss: 4.02617836\n",
      "Iteration: 4200\n",
      "Semantic loss: 4.05690002\n",
      "Agnostic loss: 3.76134562\n",
      "Total loss: 7.81824589\n",
      "Validation loss: 8.47559834\n",
      "Semantic loss: 4.31246376\n",
      "Agnostic loss: 4.16313457\n",
      "Iteration: 4300\n",
      "Semantic loss: 3.99420404\n",
      "Agnostic loss: 4.14158106\n",
      "Total loss: 8.13578510\n",
      "Validation loss: 9.23943710\n",
      "Semantic loss: 4.63126278\n",
      "Agnostic loss: 4.60817432\n",
      "Iteration: 4400\n",
      "Semantic loss: 4.73578882\n",
      "Agnostic loss: 4.09549952\n",
      "Total loss: 8.83128834\n",
      "Validation loss: 8.66225719\n",
      "Semantic loss: 4.03585958\n",
      "Agnostic loss: 4.62639761\n",
      "Iteration: 4500\n",
      "Semantic loss: 4.06409836\n",
      "Agnostic loss: 4.46066284\n",
      "Total loss: 8.52476120\n",
      "Validation loss: 7.69062710\n",
      "Semantic loss: 3.97593522\n",
      "Agnostic loss: 3.71469164\n",
      "Iteration: 4600\n",
      "Semantic loss: 4.29094934\n",
      "Agnostic loss: 4.12339878\n",
      "Total loss: 8.41434860\n",
      "Validation loss: 8.45202255\n",
      "Semantic loss: 3.98329186\n",
      "Agnostic loss: 4.46873093\n",
      "Iteration: 4700\n",
      "Semantic loss: 4.11583519\n",
      "Agnostic loss: 3.92248201\n",
      "Total loss: 8.03831673\n",
      "Validation loss: 7.55155134\n",
      "Semantic loss: 3.77596664\n",
      "Agnostic loss: 3.77558470\n",
      "Iteration: 4800\n",
      "Semantic loss: 4.13582230\n",
      "Agnostic loss: 4.49977493\n",
      "Total loss: 8.63559723\n",
      "Validation loss: 9.84200668\n",
      "Semantic loss: 4.75979328\n",
      "Agnostic loss: 5.08221340\n",
      "Iteration: 4900\n",
      "Semantic loss: 3.75133848\n",
      "Agnostic loss: 4.09678555\n",
      "Total loss: 7.84812403\n",
      "Validation loss: 8.19324493\n",
      "Semantic loss: 3.93001223\n",
      "Agnostic loss: 4.26323318\n",
      "Iteration: 5000\n",
      "Semantic loss: 3.80793095\n",
      "Agnostic loss: 3.87352467\n",
      "Total loss: 7.68145561\n",
      "Validation loss: 8.27653217\n",
      "Semantic loss: 4.17599773\n",
      "Agnostic loss: 4.10053444\n",
      "Iteration: 5100\n",
      "Semantic loss: 3.93700695\n",
      "Agnostic loss: 4.19198036\n",
      "Total loss: 8.12898731\n",
      "Validation loss: 8.55325508\n",
      "Semantic loss: 4.27844620\n",
      "Agnostic loss: 4.27480936\n",
      "Iteration: 5200\n",
      "Semantic loss: 4.31966496\n",
      "Agnostic loss: 4.48750114\n",
      "Total loss: 8.80716610\n",
      "Validation loss: 7.50940561\n",
      "Semantic loss: 3.84260726\n",
      "Agnostic loss: 3.66679835\n",
      "Iteration: 5300\n",
      "Semantic loss: 4.32200670\n",
      "Agnostic loss: 4.32452583\n",
      "Total loss: 8.64653206\n",
      "Validation loss: 7.87425089\n",
      "Semantic loss: 4.27815342\n",
      "Agnostic loss: 3.59609747\n",
      "Iteration: 5400\n",
      "Semantic loss: 4.34768486\n",
      "Agnostic loss: 4.15555763\n",
      "Total loss: 8.50324249\n",
      "Validation loss: 8.05668449\n",
      "Semantic loss: 4.00388384\n",
      "Agnostic loss: 4.05280018\n",
      "Iteration: 5500\n",
      "Semantic loss: 4.28816509\n",
      "Agnostic loss: 3.72813416\n",
      "Total loss: 8.01629925\n",
      "Validation loss: 7.95708656\n",
      "Semantic loss: 3.94006753\n",
      "Agnostic loss: 4.01701927\n",
      "Iteration: 5600\n",
      "Semantic loss: 3.72040105\n",
      "Agnostic loss: 3.64999104\n",
      "Total loss: 7.37039185\n",
      "Validation loss: 7.92180395\n",
      "Semantic loss: 3.74861193\n",
      "Agnostic loss: 4.17319202\n",
      "Iteration: 5700\n",
      "Semantic loss: 4.30913544\n",
      "Agnostic loss: 4.10771084\n",
      "Total loss: 8.41684628\n",
      "Validation loss: 8.57944775\n",
      "Semantic loss: 4.52671909\n",
      "Agnostic loss: 4.05272865\n",
      "Iteration: 5800\n",
      "Semantic loss: 3.88562584\n",
      "Agnostic loss: 3.60962701\n",
      "Total loss: 7.49525261\n",
      "Validation loss: 8.30298042\n",
      "Semantic loss: 4.40380049\n",
      "Agnostic loss: 3.89918041\n",
      "Iteration: 5900\n",
      "Semantic loss: 4.46316814\n",
      "Agnostic loss: 4.20529652\n",
      "Total loss: 8.66846466\n",
      "Validation loss: 8.22918892\n",
      "Semantic loss: 4.10585642\n",
      "Agnostic loss: 4.12333298\n",
      "Iteration: 6000\n",
      "Semantic loss: 4.74007130\n",
      "Agnostic loss: 4.42782736\n",
      "Total loss: 9.16789818\n",
      "Validation loss: 8.24319363\n",
      "Semantic loss: 4.28958988\n",
      "Agnostic loss: 3.95360374\n",
      "Iteration: 6100\n",
      "Semantic loss: 4.01311874\n",
      "Agnostic loss: 3.91085887\n",
      "Total loss: 7.92397785\n",
      "Validation loss: 7.99561787\n",
      "Semantic loss: 3.88745046\n",
      "Agnostic loss: 4.10816765\n",
      "Iteration: 6200\n",
      "Semantic loss: 4.23380470\n",
      "Agnostic loss: 4.12121153\n",
      "Total loss: 8.35501671\n",
      "Validation loss: 7.43202972\n",
      "Semantic loss: 3.72190070\n",
      "Agnostic loss: 3.71012878\n",
      "Iteration: 6300\n",
      "Semantic loss: 3.91574430\n",
      "Agnostic loss: 3.96059990\n",
      "Total loss: 7.87634420\n",
      "Validation loss: 8.20582390\n",
      "Semantic loss: 4.02435207\n",
      "Agnostic loss: 4.18147230\n",
      "Iteration: 6400\n",
      "Semantic loss: 4.61580086\n",
      "Agnostic loss: 3.90438151\n",
      "Total loss: 8.52018261\n",
      "Validation loss: 8.42227364\n",
      "Semantic loss: 4.10693598\n",
      "Agnostic loss: 4.31533718\n",
      "Iteration: 6500\n",
      "Semantic loss: 3.85930490\n",
      "Agnostic loss: 4.24701977\n",
      "Total loss: 8.10632515\n",
      "Validation loss: 8.69708824\n",
      "Semantic loss: 4.45211792\n",
      "Agnostic loss: 4.24496984\n",
      "Iteration: 6600\n",
      "Semantic loss: 4.36555290\n",
      "Agnostic loss: 3.61716914\n",
      "Total loss: 7.98272228\n",
      "Validation loss: 7.55931854\n",
      "Semantic loss: 3.90676141\n",
      "Agnostic loss: 3.65255690\n",
      "Iteration: 6700\n",
      "Semantic loss: 4.01944065\n",
      "Agnostic loss: 3.68983483\n",
      "Total loss: 7.70927525\n",
      "Validation loss: 8.04575443\n",
      "Semantic loss: 3.87739468\n",
      "Agnostic loss: 4.16835976\n",
      "Iteration: 6800\n",
      "Semantic loss: 3.76348734\n",
      "Agnostic loss: 3.66183901\n",
      "Total loss: 7.42532635\n",
      "Validation loss: 7.87493277\n",
      "Semantic loss: 4.04527855\n",
      "Agnostic loss: 3.82965422\n",
      "Iteration: 6900\n",
      "Semantic loss: 4.20288849\n",
      "Agnostic loss: 4.60498762\n",
      "Total loss: 8.80787659\n",
      "Validation loss: 8.19194221\n",
      "Semantic loss: 4.19388723\n",
      "Agnostic loss: 3.99805450\n",
      "Iteration: 7000\n",
      "Semantic loss: 3.75019002\n",
      "Agnostic loss: 3.79187894\n",
      "Total loss: 7.54206896\n",
      "Validation loss: 7.85036087\n",
      "Semantic loss: 4.00762749\n",
      "Agnostic loss: 3.84273314\n",
      "Iteration: 7100\n",
      "Semantic loss: 3.99509668\n",
      "Agnostic loss: 3.78939915\n",
      "Total loss: 7.78449583\n",
      "Validation loss: 10.71120930\n",
      "Semantic loss: 5.40607643\n",
      "Agnostic loss: 5.30513287\n",
      "Iteration: 7200\n",
      "Semantic loss: 3.68487072\n",
      "Agnostic loss: 3.63107848\n",
      "Total loss: 7.31594944\n",
      "Validation loss: 7.52173805\n",
      "Semantic loss: 3.92697430\n",
      "Agnostic loss: 3.59476376\n",
      "Iteration: 7300\n",
      "Semantic loss: 3.44545865\n",
      "Agnostic loss: 3.49554157\n",
      "Total loss: 6.94099998\n",
      "Validation loss: 8.27753830\n",
      "Semantic loss: 4.19522381\n",
      "Agnostic loss: 4.08231449\n",
      "Iteration: 7400\n",
      "Semantic loss: 5.47079039\n",
      "Agnostic loss: 4.99026108\n",
      "Total loss: 10.46105194\n",
      "Validation loss: 8.31022835\n",
      "Semantic loss: 4.32669783\n",
      "Agnostic loss: 3.98353028\n",
      "Iteration: 7500\n",
      "Semantic loss: 3.76797104\n",
      "Agnostic loss: 3.88436532\n",
      "Total loss: 7.65233612\n",
      "Validation loss: 7.53732300\n",
      "Semantic loss: 3.56615520\n",
      "Agnostic loss: 3.97116804\n",
      "Iteration: 7600\n",
      "Semantic loss: 3.77059221\n",
      "Agnostic loss: 3.82831001\n",
      "Total loss: 7.59890223\n",
      "Validation loss: 8.18674946\n",
      "Semantic loss: 4.01021767\n",
      "Agnostic loss: 4.17653179\n",
      "Iteration: 7700\n",
      "Semantic loss: 3.79079723\n",
      "Agnostic loss: 3.66452456\n",
      "Total loss: 7.45532179\n",
      "Validation loss: 7.29949617\n",
      "Semantic loss: 3.94773293\n",
      "Agnostic loss: 3.35176325\n",
      "Iteration: 7800\n",
      "Semantic loss: 3.96798015\n",
      "Agnostic loss: 3.99458218\n",
      "Total loss: 7.96256256\n",
      "Validation loss: 7.89640665\n",
      "Semantic loss: 3.76311064\n",
      "Agnostic loss: 4.13329601\n",
      "Iteration: 7900\n",
      "Semantic loss: 3.78591251\n",
      "Agnostic loss: 3.70942640\n",
      "Total loss: 7.49533892\n",
      "Validation loss: 7.62214947\n",
      "Semantic loss: 3.98327780\n",
      "Agnostic loss: 3.63887143\n",
      "Iteration: 8000\n",
      "Semantic loss: 3.31579065\n",
      "Agnostic loss: 3.29404831\n",
      "Total loss: 6.60983896\n",
      "Validation loss: 9.18548584\n",
      "Semantic loss: 4.48869324\n",
      "Agnostic loss: 4.69679260\n",
      "Iteration: 8100\n",
      "Semantic loss: 3.95967340\n",
      "Agnostic loss: 4.10270262\n",
      "Total loss: 8.06237602\n",
      "Validation loss: 7.16797447\n",
      "Semantic loss: 3.64795208\n",
      "Agnostic loss: 3.52002239\n",
      "Iteration: 8200\n",
      "Semantic loss: 3.49050975\n",
      "Agnostic loss: 3.61473608\n",
      "Total loss: 7.10524559\n",
      "Validation loss: 7.14648008\n",
      "Semantic loss: 3.75606513\n",
      "Agnostic loss: 3.39041495\n",
      "Iteration: 8300\n",
      "Semantic loss: 3.81966138\n",
      "Agnostic loss: 3.83272791\n",
      "Total loss: 7.65238953\n",
      "Validation loss: 7.52564907\n",
      "Semantic loss: 3.62951994\n",
      "Agnostic loss: 3.89612937\n",
      "Iteration: 8400\n",
      "Semantic loss: 4.12628222\n",
      "Agnostic loss: 3.50153756\n",
      "Total loss: 7.62782001\n",
      "Validation loss: 7.39056396\n",
      "Semantic loss: 3.77096534\n",
      "Agnostic loss: 3.61959863\n",
      "Iteration: 8500\n",
      "Semantic loss: 4.10044003\n",
      "Agnostic loss: 3.64310932\n",
      "Total loss: 7.74354935\n",
      "Validation loss: 7.22321701\n",
      "Semantic loss: 3.67225409\n",
      "Agnostic loss: 3.55096292\n",
      "Iteration: 8600\n",
      "Semantic loss: 4.19502687\n",
      "Agnostic loss: 3.82181263\n",
      "Total loss: 8.01683998\n",
      "Validation loss: 7.02155876\n",
      "Semantic loss: 3.71307802\n",
      "Agnostic loss: 3.30848050\n",
      "Iteration: 8700\n",
      "Semantic loss: 3.93528152\n",
      "Agnostic loss: 3.21852183\n",
      "Total loss: 7.15380335\n",
      "Validation loss: 6.78335190\n",
      "Semantic loss: 3.76851368\n",
      "Agnostic loss: 3.01483798\n",
      "Iteration: 8800\n",
      "Semantic loss: 3.33515120\n",
      "Agnostic loss: 2.97282290\n",
      "Total loss: 6.30797386\n",
      "Validation loss: 8.05630589\n",
      "Semantic loss: 4.25401115\n",
      "Agnostic loss: 3.80229473\n",
      "Iteration: 8900\n",
      "Semantic loss: 3.82281303\n",
      "Agnostic loss: 3.37110376\n",
      "Total loss: 7.19391680\n",
      "Validation loss: 7.65074682\n",
      "Semantic loss: 4.22226572\n",
      "Agnostic loss: 3.42848110\n",
      "Iteration: 9000\n",
      "Semantic loss: 4.12447214\n",
      "Agnostic loss: 2.42837715\n",
      "Total loss: 6.55284929\n",
      "Validation loss: 7.35976505\n",
      "Semantic loss: 4.04584742\n",
      "Agnostic loss: 3.31391764\n",
      "Iteration: 9100\n",
      "Semantic loss: 3.74812198\n",
      "Agnostic loss: 2.53111935\n",
      "Total loss: 6.27924156\n",
      "Validation loss: 6.30436468\n",
      "Semantic loss: 3.40021515\n",
      "Agnostic loss: 2.90414953\n",
      "Iteration: 9200\n",
      "Semantic loss: 3.58473349\n",
      "Agnostic loss: 2.80071378\n",
      "Total loss: 6.38544750\n",
      "Validation loss: 6.04883003\n",
      "Semantic loss: 3.46286869\n",
      "Agnostic loss: 2.58596134\n",
      "Iteration: 9300\n",
      "Semantic loss: 3.27622890\n",
      "Agnostic loss: 2.88583875\n",
      "Total loss: 6.16206741\n",
      "Validation loss: 6.50355625\n",
      "Semantic loss: 3.57207584\n",
      "Agnostic loss: 2.93148041\n",
      "Iteration: 9400\n",
      "Semantic loss: 4.20418167\n",
      "Agnostic loss: 2.31379128\n",
      "Total loss: 6.51797295\n",
      "Validation loss: 6.38684893\n",
      "Semantic loss: 3.66898060\n",
      "Agnostic loss: 2.71786833\n",
      "Iteration: 9500\n",
      "Semantic loss: 4.25059414\n",
      "Agnostic loss: 2.39513540\n",
      "Total loss: 6.64572954\n",
      "Validation loss: 7.28268623\n",
      "Semantic loss: 3.92740345\n",
      "Agnostic loss: 3.35528278\n",
      "Iteration: 9600\n",
      "Semantic loss: 3.65069151\n",
      "Agnostic loss: 2.47425890\n",
      "Total loss: 6.12495041\n",
      "Validation loss: 7.29617310\n",
      "Semantic loss: 4.13481045\n",
      "Agnostic loss: 3.16136265\n",
      "Iteration: 9700\n",
      "Semantic loss: 2.79070759\n",
      "Agnostic loss: 2.05515337\n",
      "Total loss: 4.84586096\n",
      "Validation loss: 5.43564796\n",
      "Semantic loss: 3.53094435\n",
      "Agnostic loss: 1.90470350\n",
      "Iteration: 9800\n",
      "Semantic loss: 3.39460516\n",
      "Agnostic loss: 1.80276084\n",
      "Total loss: 5.19736576\n",
      "Validation loss: 6.66750717\n",
      "Semantic loss: 3.90572119\n",
      "Agnostic loss: 2.76178598\n",
      "Iteration: 9900\n",
      "Semantic loss: 3.72715044\n",
      "Agnostic loss: 1.87917638\n",
      "Total loss: 5.60632706\n",
      "Validation loss: 5.99544907\n",
      "Semantic loss: 3.59426761\n",
      "Agnostic loss: 2.40118170\n",
      "Iteration: 10000\n",
      "Semantic loss: 3.68507004\n",
      "Agnostic loss: 2.10266614\n",
      "Total loss: 5.78773594\n",
      "Validation loss: 6.66035843\n",
      "Semantic loss: 3.99487734\n",
      "Agnostic loss: 2.66548109\n",
      "Iteration: 10100\n",
      "Semantic loss: 3.81692982\n",
      "Agnostic loss: 2.48966789\n",
      "Total loss: 6.30659771\n",
      "Validation loss: 5.83375740\n",
      "Semantic loss: 3.39975357\n",
      "Agnostic loss: 2.43400359\n",
      "Iteration: 10200\n",
      "Semantic loss: 3.58417201\n",
      "Agnostic loss: 1.78591967\n",
      "Total loss: 5.37009144\n",
      "Validation loss: 4.66905546\n",
      "Semantic loss: 2.94843888\n",
      "Agnostic loss: 1.72061658\n",
      "Iteration: 10300\n",
      "Semantic loss: 3.29756260\n",
      "Agnostic loss: 2.16024804\n",
      "Total loss: 5.45781040\n",
      "Validation loss: 5.41291523\n",
      "Semantic loss: 3.18114018\n",
      "Agnostic loss: 2.23177481\n",
      "Iteration: 10400\n",
      "Semantic loss: 3.22758389\n",
      "Agnostic loss: 1.97721601\n",
      "Total loss: 5.20479965\n",
      "Validation loss: 5.91123581\n",
      "Semantic loss: 3.54765797\n",
      "Agnostic loss: 2.36357808\n",
      "Iteration: 10500\n",
      "Semantic loss: 2.95505071\n",
      "Agnostic loss: 1.63117194\n",
      "Total loss: 4.58622265\n",
      "Validation loss: 5.36659575\n",
      "Semantic loss: 3.48043680\n",
      "Agnostic loss: 1.88615894\n",
      "Iteration: 10600\n",
      "Semantic loss: 3.47311926\n",
      "Agnostic loss: 2.01555800\n",
      "Total loss: 5.48867702\n",
      "Validation loss: 6.66885328\n",
      "Semantic loss: 3.55331540\n",
      "Agnostic loss: 3.11553788\n",
      "Iteration: 10700\n",
      "Semantic loss: 3.22603917\n",
      "Agnostic loss: 1.65952754\n",
      "Total loss: 4.88556671\n",
      "Validation loss: 4.83590031\n",
      "Semantic loss: 2.90824461\n",
      "Agnostic loss: 1.92765570\n",
      "Iteration: 10800\n",
      "Semantic loss: 2.52574062\n",
      "Agnostic loss: 1.25599432\n",
      "Total loss: 3.78173494\n",
      "Validation loss: 5.63909435\n",
      "Semantic loss: 3.62932658\n",
      "Agnostic loss: 2.00976753\n",
      "Iteration: 10900\n",
      "Semantic loss: 2.59987473\n",
      "Agnostic loss: 1.11101735\n",
      "Total loss: 3.71089220\n",
      "Validation loss: 6.65395164\n",
      "Semantic loss: 3.69201422\n",
      "Agnostic loss: 2.96193767\n",
      "Iteration: 11000\n",
      "Semantic loss: 3.04363060\n",
      "Agnostic loss: 2.55706406\n",
      "Total loss: 5.60069466\n",
      "Validation loss: 6.51890373\n",
      "Semantic loss: 4.17226887\n",
      "Agnostic loss: 2.34663486\n",
      "Iteration: 11100\n",
      "Semantic loss: 3.67023516\n",
      "Agnostic loss: 3.45159554\n",
      "Total loss: 7.12183094\n",
      "Validation loss: 5.11465836\n",
      "Semantic loss: 3.38768315\n",
      "Agnostic loss: 1.72697520\n",
      "Iteration: 11200\n",
      "Semantic loss: 2.62817550\n",
      "Agnostic loss: 1.86228538\n",
      "Total loss: 4.49046087\n",
      "Validation loss: 3.27528429\n",
      "Semantic loss: 2.07969308\n",
      "Agnostic loss: 1.19559121\n",
      "Iteration: 11300\n",
      "Semantic loss: 2.40910387\n",
      "Agnostic loss: 1.54340053\n",
      "Total loss: 3.95250440\n",
      "Validation loss: 3.97470379\n",
      "Semantic loss: 2.65176010\n",
      "Agnostic loss: 1.32294357\n",
      "Iteration: 11400\n",
      "Semantic loss: 2.92278528\n",
      "Agnostic loss: 1.27257013\n",
      "Total loss: 4.19535542\n",
      "Validation loss: 4.19413948\n",
      "Semantic loss: 2.35305071\n",
      "Agnostic loss: 1.84108853\n",
      "Iteration: 11500\n",
      "Semantic loss: 2.29641557\n",
      "Agnostic loss: 1.10646677\n",
      "Total loss: 3.40288234\n",
      "Validation loss: 4.39317513\n",
      "Semantic loss: 2.52424383\n",
      "Agnostic loss: 1.86893106\n",
      "Iteration: 11600\n",
      "Semantic loss: 2.64469719\n",
      "Agnostic loss: 1.80732763\n",
      "Total loss: 4.45202494\n",
      "Validation loss: 5.52751780\n",
      "Semantic loss: 3.26888037\n",
      "Agnostic loss: 2.25863743\n",
      "Iteration: 11700\n",
      "Semantic loss: 2.27667427\n",
      "Agnostic loss: 1.24444318\n",
      "Total loss: 3.52111745\n",
      "Validation loss: 4.72157240\n",
      "Semantic loss: 2.75340819\n",
      "Agnostic loss: 1.96816432\n",
      "Iteration: 11800\n",
      "Semantic loss: 2.02279711\n",
      "Agnostic loss: 1.54441977\n",
      "Total loss: 3.56721687\n",
      "Validation loss: 4.16320324\n",
      "Semantic loss: 2.25538540\n",
      "Agnostic loss: 1.90781808\n",
      "Iteration: 11900\n",
      "Semantic loss: 2.56056643\n",
      "Agnostic loss: 1.66152263\n",
      "Total loss: 4.22208881\n",
      "Validation loss: 4.00527859\n",
      "Semantic loss: 2.48371649\n",
      "Agnostic loss: 1.52156198\n",
      "Iteration: 12000\n",
      "Semantic loss: 2.71452856\n",
      "Agnostic loss: 1.85503483\n",
      "Total loss: 4.56956339\n",
      "Validation loss: 4.48763180\n",
      "Semantic loss: 2.42066431\n",
      "Agnostic loss: 2.06696773\n",
      "Iteration: 12100\n",
      "Semantic loss: 2.05724549\n",
      "Agnostic loss: 1.43111348\n",
      "Total loss: 3.48835897\n",
      "Validation loss: 3.28899717\n",
      "Semantic loss: 2.21136236\n",
      "Agnostic loss: 1.07763469\n",
      "Iteration: 12200\n",
      "Semantic loss: 2.39042473\n",
      "Agnostic loss: 1.41920292\n",
      "Total loss: 3.80962753\n",
      "Validation loss: 2.49756217\n",
      "Semantic loss: 1.58283949\n",
      "Agnostic loss: 0.91472274\n",
      "Iteration: 12300\n",
      "Semantic loss: 2.07893467\n",
      "Agnostic loss: 1.58261228\n",
      "Total loss: 3.66154695\n",
      "Validation loss: 3.68636560\n",
      "Semantic loss: 2.31866908\n",
      "Agnostic loss: 1.36769652\n",
      "Iteration: 12400\n",
      "Semantic loss: 3.55735993\n",
      "Agnostic loss: 2.40221739\n",
      "Total loss: 5.95957756\n",
      "Validation loss: 4.73126698\n",
      "Semantic loss: 2.57300425\n",
      "Agnostic loss: 2.15826297\n",
      "Iteration: 12500\n",
      "Semantic loss: 2.60748315\n",
      "Agnostic loss: 1.20368767\n",
      "Total loss: 3.81117082\n",
      "Validation loss: 4.25269556\n",
      "Semantic loss: 2.35981607\n",
      "Agnostic loss: 1.89287949\n",
      "Iteration: 12600\n",
      "Semantic loss: 2.42755985\n",
      "Agnostic loss: 1.44482648\n",
      "Total loss: 3.87238646\n",
      "Validation loss: 2.90702510\n",
      "Semantic loss: 2.02976060\n",
      "Agnostic loss: 0.87726450\n",
      "Iteration: 12700\n",
      "Semantic loss: 3.21834707\n",
      "Agnostic loss: 2.68873215\n",
      "Total loss: 5.90707922\n",
      "Validation loss: 3.81532288\n",
      "Semantic loss: 2.76105404\n",
      "Agnostic loss: 1.05426884\n",
      "Iteration: 12800\n",
      "Semantic loss: 2.93135166\n",
      "Agnostic loss: 1.71612358\n",
      "Total loss: 4.64747524\n",
      "Validation loss: 3.03959584\n",
      "Semantic loss: 2.13028884\n",
      "Agnostic loss: 0.90930700\n",
      "Iteration: 12900\n",
      "Semantic loss: 1.53920460\n",
      "Agnostic loss: 0.92490423\n",
      "Total loss: 2.46410894\n",
      "Validation loss: 3.42424965\n",
      "Semantic loss: 2.13146186\n",
      "Agnostic loss: 1.29278791\n",
      "Iteration: 13000\n",
      "Semantic loss: 2.68035388\n",
      "Agnostic loss: 1.48785281\n",
      "Total loss: 4.16820669\n",
      "Validation loss: 3.43217468\n",
      "Semantic loss: 1.93667138\n",
      "Agnostic loss: 1.49550319\n",
      "Iteration: 13100\n",
      "Semantic loss: 1.82421780\n",
      "Agnostic loss: 1.29966247\n",
      "Total loss: 3.12388039\n",
      "Validation loss: 3.24960899\n",
      "Semantic loss: 1.99819887\n",
      "Agnostic loss: 1.25141025\n",
      "Iteration: 13200\n",
      "Semantic loss: 2.89806890\n",
      "Agnostic loss: 1.96085584\n",
      "Total loss: 4.85892487\n",
      "Validation loss: 2.51002288\n",
      "Semantic loss: 1.58035612\n",
      "Agnostic loss: 0.92966676\n",
      "Iteration: 13300\n",
      "Semantic loss: 3.41322541\n",
      "Agnostic loss: 2.25999928\n",
      "Total loss: 5.67322445\n",
      "Validation loss: 3.80241036\n",
      "Semantic loss: 2.17242622\n",
      "Agnostic loss: 1.62998414\n",
      "Iteration: 13400\n",
      "Semantic loss: 1.94381571\n",
      "Agnostic loss: 1.24927378\n",
      "Total loss: 3.19308949\n",
      "Validation loss: 3.06727552\n",
      "Semantic loss: 1.70269108\n",
      "Agnostic loss: 1.36458433\n",
      "Iteration: 13500\n",
      "Semantic loss: 1.51947403\n",
      "Agnostic loss: 0.94681597\n",
      "Total loss: 2.46629000\n",
      "Validation loss: 6.87732887\n",
      "Semantic loss: 3.84409857\n",
      "Agnostic loss: 3.03323030\n",
      "Iteration: 13600\n",
      "Semantic loss: 2.67534709\n",
      "Agnostic loss: 2.58462620\n",
      "Total loss: 5.25997353\n",
      "Validation loss: 2.31595349\n",
      "Semantic loss: 1.27607751\n",
      "Agnostic loss: 1.03987598\n",
      "Iteration: 13700\n",
      "Semantic loss: 1.38978028\n",
      "Agnostic loss: 0.73206520\n",
      "Total loss: 2.12184548\n",
      "Validation loss: 2.45275331\n",
      "Semantic loss: 1.45459938\n",
      "Agnostic loss: 0.99815392\n",
      "Iteration: 13800\n",
      "Semantic loss: 1.91355872\n",
      "Agnostic loss: 1.13298833\n",
      "Total loss: 3.04654694\n",
      "Validation loss: 3.65740204\n",
      "Semantic loss: 2.32221961\n",
      "Agnostic loss: 1.33518243\n",
      "Iteration: 13900\n",
      "Semantic loss: 1.72892165\n",
      "Agnostic loss: 1.27458262\n",
      "Total loss: 3.00350428\n",
      "Validation loss: 2.70255756\n",
      "Semantic loss: 1.37122917\n",
      "Agnostic loss: 1.33132839\n",
      "Iteration: 14000\n",
      "Semantic loss: 1.26992178\n",
      "Agnostic loss: 0.90887052\n",
      "Total loss: 2.17879224\n",
      "Validation loss: 2.78737545\n",
      "Semantic loss: 1.69126093\n",
      "Agnostic loss: 1.09611440\n",
      "Iteration: 14100\n",
      "Semantic loss: 1.51070786\n",
      "Agnostic loss: 0.92474967\n",
      "Total loss: 2.43545747\n",
      "Validation loss: 3.83462000\n",
      "Semantic loss: 2.39970064\n",
      "Agnostic loss: 1.43491936\n",
      "Iteration: 14200\n",
      "Semantic loss: 1.40639043\n",
      "Agnostic loss: 0.97156221\n",
      "Total loss: 2.37795258\n",
      "Validation loss: 2.46795106\n",
      "Semantic loss: 1.53590965\n",
      "Agnostic loss: 0.93204135\n",
      "Iteration: 14300\n",
      "Semantic loss: 1.92367816\n",
      "Agnostic loss: 1.26186562\n",
      "Total loss: 3.18554378\n",
      "Validation loss: 3.54404402\n",
      "Semantic loss: 2.02578402\n",
      "Agnostic loss: 1.51826000\n",
      "Iteration: 14400\n",
      "Semantic loss: 3.73474932\n",
      "Agnostic loss: 2.19463587\n",
      "Total loss: 5.92938519\n",
      "Validation loss: 4.03422737\n",
      "Semantic loss: 2.56056833\n",
      "Agnostic loss: 1.47365892\n",
      "Iteration: 14500\n",
      "Semantic loss: 2.15036440\n",
      "Agnostic loss: 1.43150353\n",
      "Total loss: 3.58186793\n",
      "Validation loss: 4.62858486\n",
      "Semantic loss: 2.92767048\n",
      "Agnostic loss: 1.70091438\n",
      "Iteration: 14600\n",
      "Semantic loss: 1.96338558\n",
      "Agnostic loss: 1.30203366\n",
      "Total loss: 3.26541924\n",
      "Validation loss: 2.52484465\n",
      "Semantic loss: 1.45504904\n",
      "Agnostic loss: 1.06979573\n",
      "Iteration: 14700\n",
      "Semantic loss: 1.39894652\n",
      "Agnostic loss: 1.48643076\n",
      "Total loss: 2.88537741\n",
      "Validation loss: 2.46712494\n",
      "Semantic loss: 1.65076852\n",
      "Agnostic loss: 0.81635636\n",
      "Iteration: 14800\n",
      "Semantic loss: 1.57889986\n",
      "Agnostic loss: 1.17650723\n",
      "Total loss: 2.75540709\n",
      "Validation loss: 2.11973572\n",
      "Semantic loss: 1.39804912\n",
      "Agnostic loss: 0.72168672\n",
      "Iteration: 14900\n",
      "Semantic loss: 0.93268383\n",
      "Agnostic loss: 0.59423149\n",
      "Total loss: 1.52691531\n",
      "Validation loss: 3.34066486\n",
      "Semantic loss: 1.94987094\n",
      "Agnostic loss: 1.39079404\n",
      "Iteration: 15000\n",
      "Semantic loss: 1.96765268\n",
      "Agnostic loss: 1.67983162\n",
      "Total loss: 3.64748430\n",
      "Validation loss: 1.94622970\n",
      "Semantic loss: 1.12345076\n",
      "Agnostic loss: 0.82277894\n",
      "Iteration: 15100\n",
      "Semantic loss: 1.01092160\n",
      "Agnostic loss: 0.66192222\n",
      "Total loss: 1.67284381\n",
      "Validation loss: 3.10156727\n",
      "Semantic loss: 1.82542849\n",
      "Agnostic loss: 1.27613878\n",
      "Iteration: 15200\n",
      "Semantic loss: 0.98777431\n",
      "Agnostic loss: 0.77490234\n",
      "Total loss: 1.76267672\n",
      "Validation loss: 3.01949978\n",
      "Semantic loss: 1.51677990\n",
      "Agnostic loss: 1.50271988\n",
      "Iteration: 15300\n",
      "Semantic loss: 1.15824246\n",
      "Agnostic loss: 1.29064000\n",
      "Total loss: 2.44888258\n",
      "Validation loss: 2.53606725\n",
      "Semantic loss: 1.50442052\n",
      "Agnostic loss: 1.03164673\n",
      "Iteration: 15344\n",
      "Semantic loss: 0.97374618\n",
      "Agnostic loss: 0.42081690\n",
      "Total loss: 1.39456308\n",
      "Validation loss: 1.96876049\n",
      "Semantic loss: 1.13045359\n",
      "Agnostic loss: 0.83830684\n",
      "Epoch: 2\n",
      "Iteration:  100\n",
      "Semantic loss: 1.23765469\n",
      "Agnostic loss: 0.92579949\n",
      "Total loss: 2.16345406\n",
      "Validation loss: 2.51560783\n",
      "Semantic loss: 1.38098621\n",
      "Agnostic loss: 1.13462162\n",
      "Iteration:  200\n",
      "Semantic loss: 1.96596408\n",
      "Agnostic loss: 1.34083652\n",
      "Total loss: 3.30680060\n",
      "Validation loss: 2.41413784\n",
      "Semantic loss: 1.40189421\n",
      "Agnostic loss: 1.01224351\n",
      "Iteration:  300\n",
      "Semantic loss: 1.72243118\n",
      "Agnostic loss: 1.37779880\n",
      "Total loss: 3.10022998\n",
      "Validation loss: 1.89805865\n",
      "Semantic loss: 1.03500342\n",
      "Agnostic loss: 0.86305523\n",
      "Iteration:  400\n",
      "Semantic loss: 1.62639129\n",
      "Agnostic loss: 1.15560365\n",
      "Total loss: 2.78199482\n",
      "Validation loss: 2.50570536\n",
      "Semantic loss: 1.27402449\n",
      "Agnostic loss: 1.23168087\n",
      "Iteration:  500\n",
      "Semantic loss: 2.43636513\n",
      "Agnostic loss: 2.04882193\n",
      "Total loss: 4.48518705\n",
      "Validation loss: 4.17691469\n",
      "Semantic loss: 2.61591196\n",
      "Agnostic loss: 1.56100261\n",
      "Iteration:  600\n",
      "Semantic loss: 0.83189684\n",
      "Agnostic loss: 1.02357662\n",
      "Total loss: 1.85547352\n",
      "Validation loss: 3.59382057\n",
      "Semantic loss: 2.45842552\n",
      "Agnostic loss: 1.13539505\n",
      "Iteration:  700\n",
      "Semantic loss: 1.70820904\n",
      "Agnostic loss: 1.01653743\n",
      "Total loss: 2.72474647\n",
      "Validation loss: 3.11729217\n",
      "Semantic loss: 1.85985374\n",
      "Agnostic loss: 1.25743842\n",
      "Iteration:  800\n",
      "Semantic loss: 1.22614610\n",
      "Agnostic loss: 1.13765252\n",
      "Total loss: 2.36379862\n",
      "Validation loss: 2.38903236\n",
      "Semantic loss: 1.47799659\n",
      "Agnostic loss: 0.91103566\n",
      "Iteration:  900\n",
      "Semantic loss: 1.48352122\n",
      "Agnostic loss: 1.84053636\n",
      "Total loss: 3.32405758\n",
      "Validation loss: 2.38677955\n",
      "Semantic loss: 1.36858988\n",
      "Agnostic loss: 1.01818967\n",
      "Iteration: 1000\n",
      "Semantic loss: 2.02726126\n",
      "Agnostic loss: 1.23247111\n",
      "Total loss: 3.25973225\n",
      "Validation loss: 2.94825792\n",
      "Semantic loss: 1.68923795\n",
      "Agnostic loss: 1.25901997\n",
      "Iteration: 1100\n",
      "Semantic loss: 2.19265962\n",
      "Agnostic loss: 1.56716013\n",
      "Total loss: 3.75981975\n",
      "Validation loss: 3.62750387\n",
      "Semantic loss: 1.89689279\n",
      "Agnostic loss: 1.73061109\n",
      "Iteration: 1200\n",
      "Semantic loss: 2.08819175\n",
      "Agnostic loss: 1.32870412\n",
      "Total loss: 3.41689587\n",
      "Validation loss: 3.99683619\n",
      "Semantic loss: 2.10456944\n",
      "Agnostic loss: 1.89226687\n",
      "Iteration: 1300\n",
      "Semantic loss: 1.60684752\n",
      "Agnostic loss: 1.88558924\n",
      "Total loss: 3.49243689\n",
      "Validation loss: 3.14737415\n",
      "Semantic loss: 1.88891995\n",
      "Agnostic loss: 1.25845408\n",
      "Iteration: 1400\n",
      "Semantic loss: 1.44128513\n",
      "Agnostic loss: 1.12363636\n",
      "Total loss: 2.56492138\n",
      "Validation loss: 3.08513641\n",
      "Semantic loss: 1.75613928\n",
      "Agnostic loss: 1.32899714\n",
      "Iteration: 1500\n",
      "Semantic loss: 1.14659476\n",
      "Agnostic loss: 0.89619660\n",
      "Total loss: 2.04279137\n",
      "Validation loss: 2.43099403\n",
      "Semantic loss: 1.40473223\n",
      "Agnostic loss: 1.02626193\n",
      "Iteration: 1600\n",
      "Semantic loss: 1.70379531\n",
      "Agnostic loss: 1.10242891\n",
      "Total loss: 2.80622435\n",
      "Validation loss: 2.94101071\n",
      "Semantic loss: 1.38351536\n",
      "Agnostic loss: 1.55749536\n",
      "Iteration: 1700\n",
      "Semantic loss: 1.33207893\n",
      "Agnostic loss: 0.97838902\n",
      "Total loss: 2.31046796\n",
      "Validation loss: 1.65274811\n",
      "Semantic loss: 0.93046516\n",
      "Agnostic loss: 0.72228295\n",
      "Iteration: 1800\n",
      "Semantic loss: 1.77771103\n",
      "Agnostic loss: 0.95277905\n",
      "Total loss: 2.73049021\n",
      "Validation loss: 1.98661506\n",
      "Semantic loss: 1.39323688\n",
      "Agnostic loss: 0.59337819\n",
      "Iteration: 1900\n",
      "Semantic loss: 1.20993316\n",
      "Agnostic loss: 0.85765606\n",
      "Total loss: 2.06758928\n",
      "Validation loss: 2.91470814\n",
      "Semantic loss: 1.59128869\n",
      "Agnostic loss: 1.32341957\n",
      "Iteration: 2000\n",
      "Semantic loss: 1.72918630\n",
      "Agnostic loss: 1.28758931\n",
      "Total loss: 3.01677561\n",
      "Validation loss: 3.23023939\n",
      "Semantic loss: 1.72477651\n",
      "Agnostic loss: 1.50546288\n",
      "Iteration: 2100\n",
      "Semantic loss: 1.61088419\n",
      "Agnostic loss: 1.39209425\n",
      "Total loss: 3.00297832\n",
      "Validation loss: 1.92140424\n",
      "Semantic loss: 1.05010366\n",
      "Agnostic loss: 0.87130058\n",
      "Iteration: 2200\n",
      "Semantic loss: 1.60301352\n",
      "Agnostic loss: 0.70224589\n",
      "Total loss: 2.30525947\n",
      "Validation loss: 2.69849682\n",
      "Semantic loss: 1.70847964\n",
      "Agnostic loss: 0.99001712\n",
      "Iteration: 2300\n",
      "Semantic loss: 1.36546075\n",
      "Agnostic loss: 0.69586366\n",
      "Total loss: 2.06132436\n",
      "Validation loss: 2.44104242\n",
      "Semantic loss: 1.33788621\n",
      "Agnostic loss: 1.10315609\n",
      "Iteration: 2400\n",
      "Semantic loss: 1.43629909\n",
      "Agnostic loss: 1.24567473\n",
      "Total loss: 2.68197393\n",
      "Validation loss: 1.72693193\n",
      "Semantic loss: 0.99437684\n",
      "Agnostic loss: 0.73255509\n",
      "Iteration: 2500\n",
      "Semantic loss: 1.54709876\n",
      "Agnostic loss: 1.57401991\n",
      "Total loss: 3.12111855\n",
      "Validation loss: 2.18956327\n",
      "Semantic loss: 1.14315581\n",
      "Agnostic loss: 1.04640734\n",
      "Iteration: 2600\n",
      "Semantic loss: 1.15938604\n",
      "Agnostic loss: 0.65514123\n",
      "Total loss: 1.81452727\n",
      "Validation loss: 3.44160461\n",
      "Semantic loss: 1.71288264\n",
      "Agnostic loss: 1.72872210\n",
      "Iteration: 2700\n",
      "Semantic loss: 2.41671276\n",
      "Agnostic loss: 2.52688313\n",
      "Total loss: 4.94359589\n",
      "Validation loss: 2.11204243\n",
      "Semantic loss: 1.18659711\n",
      "Agnostic loss: 0.92544520\n",
      "Iteration: 2800\n",
      "Semantic loss: 0.96637630\n",
      "Agnostic loss: 0.95621812\n",
      "Total loss: 1.92259443\n",
      "Validation loss: 2.70510530\n",
      "Semantic loss: 1.40347946\n",
      "Agnostic loss: 1.30162585\n",
      "Iteration: 2900\n",
      "Semantic loss: 0.76155162\n",
      "Agnostic loss: 1.01548588\n",
      "Total loss: 1.77703750\n",
      "Validation loss: 1.82122469\n",
      "Semantic loss: 1.16728914\n",
      "Agnostic loss: 0.65393555\n",
      "Iteration: 3000\n",
      "Semantic loss: 2.22792482\n",
      "Agnostic loss: 1.56694460\n",
      "Total loss: 3.79486942\n",
      "Validation loss: 2.35018587\n",
      "Semantic loss: 1.43060386\n",
      "Agnostic loss: 0.91958201\n",
      "Iteration: 3100\n",
      "Semantic loss: 2.04036045\n",
      "Agnostic loss: 1.69398820\n",
      "Total loss: 3.73434877\n",
      "Validation loss: 2.52233887\n",
      "Semantic loss: 1.41074729\n",
      "Agnostic loss: 1.11159146\n",
      "Iteration: 3200\n",
      "Semantic loss: 1.11645162\n",
      "Agnostic loss: 1.30655789\n",
      "Total loss: 2.42300940\n",
      "Validation loss: 4.08538246\n",
      "Semantic loss: 2.40075397\n",
      "Agnostic loss: 1.68462873\n",
      "Iteration: 3300\n",
      "Semantic loss: 0.79184282\n",
      "Agnostic loss: 0.88282883\n",
      "Total loss: 1.67467165\n",
      "Validation loss: 2.52066851\n",
      "Semantic loss: 1.50422072\n",
      "Agnostic loss: 1.01644790\n",
      "Iteration: 3400\n",
      "Semantic loss: 1.43780303\n",
      "Agnostic loss: 1.08623266\n",
      "Total loss: 2.52403569\n",
      "Validation loss: 3.13458204\n",
      "Semantic loss: 1.59437144\n",
      "Agnostic loss: 1.54021072\n",
      "Iteration: 3500\n",
      "Semantic loss: 0.86453658\n",
      "Agnostic loss: 0.50651884\n",
      "Total loss: 1.37105536\n",
      "Validation loss: 1.88380480\n",
      "Semantic loss: 1.08168578\n",
      "Agnostic loss: 0.80211908\n",
      "Iteration: 3600\n",
      "Semantic loss: 1.18022728\n",
      "Agnostic loss: 1.08409905\n",
      "Total loss: 2.26432633\n",
      "Validation loss: 2.56174755\n",
      "Semantic loss: 1.51604748\n",
      "Agnostic loss: 1.04570007\n",
      "Iteration: 3700\n",
      "Semantic loss: 1.95757079\n",
      "Agnostic loss: 2.11037874\n",
      "Total loss: 4.06794930\n",
      "Validation loss: 3.97222853\n",
      "Semantic loss: 2.13197899\n",
      "Agnostic loss: 1.84024954\n",
      "Iteration: 3800\n",
      "Semantic loss: 1.33306730\n",
      "Agnostic loss: 0.94437295\n",
      "Total loss: 2.27744031\n",
      "Validation loss: 3.34663248\n",
      "Semantic loss: 1.78904963\n",
      "Agnostic loss: 1.55758274\n",
      "Iteration: 3900\n",
      "Semantic loss: 0.79601568\n",
      "Agnostic loss: 0.88719893\n",
      "Total loss: 1.68321466\n",
      "Validation loss: 1.02286589\n",
      "Semantic loss: 0.58372355\n",
      "Agnostic loss: 0.43914232\n",
      "Iteration: 4000\n",
      "Semantic loss: 1.78288555\n",
      "Agnostic loss: 1.36254382\n",
      "Total loss: 3.14542937\n",
      "Validation loss: 2.08016682\n",
      "Semantic loss: 1.17562211\n",
      "Agnostic loss: 0.90454459\n",
      "Iteration: 4100\n",
      "Semantic loss: 1.39925051\n",
      "Agnostic loss: 0.77673316\n",
      "Total loss: 2.17598367\n",
      "Validation loss: 2.05110312\n",
      "Semantic loss: 1.14515102\n",
      "Agnostic loss: 0.90595222\n",
      "Iteration: 4200\n",
      "Semantic loss: 1.32984376\n",
      "Agnostic loss: 0.90180910\n",
      "Total loss: 2.23165274\n",
      "Validation loss: 1.45821953\n",
      "Semantic loss: 0.75231421\n",
      "Agnostic loss: 0.70590532\n",
      "Iteration: 4300\n",
      "Semantic loss: 1.40942478\n",
      "Agnostic loss: 0.92772424\n",
      "Total loss: 2.33714914\n",
      "Validation loss: 3.18239450\n",
      "Semantic loss: 1.93142080\n",
      "Agnostic loss: 1.25097358\n",
      "Iteration: 4400\n",
      "Semantic loss: 1.70423269\n",
      "Agnostic loss: 1.27151299\n",
      "Total loss: 2.97574568\n",
      "Validation loss: 2.01845741\n",
      "Semantic loss: 1.17834687\n",
      "Agnostic loss: 0.84011054\n",
      "Iteration: 4500\n",
      "Semantic loss: 1.03654420\n",
      "Agnostic loss: 0.92058182\n",
      "Total loss: 1.95712602\n",
      "Validation loss: 2.42289567\n",
      "Semantic loss: 1.32913220\n",
      "Agnostic loss: 1.09376347\n",
      "Iteration: 4600\n",
      "Semantic loss: 0.84768867\n",
      "Agnostic loss: 0.78229463\n",
      "Total loss: 1.62998331\n",
      "Validation loss: 1.77925897\n",
      "Semantic loss: 0.88904709\n",
      "Agnostic loss: 0.89021194\n",
      "Iteration: 4700\n",
      "Semantic loss: 1.95350122\n",
      "Agnostic loss: 1.78771102\n",
      "Total loss: 3.74121237\n",
      "Validation loss: 1.37448955\n",
      "Semantic loss: 0.75050229\n",
      "Agnostic loss: 0.62398732\n",
      "Iteration: 4800\n",
      "Semantic loss: 1.55980253\n",
      "Agnostic loss: 1.01627171\n",
      "Total loss: 2.57607412\n",
      "Validation loss: 1.53344858\n",
      "Semantic loss: 0.75826156\n",
      "Agnostic loss: 0.77518702\n",
      "Iteration: 4900\n",
      "Semantic loss: 0.89854759\n",
      "Agnostic loss: 0.60016286\n",
      "Total loss: 1.49871039\n",
      "Validation loss: 2.09885168\n",
      "Semantic loss: 1.33477235\n",
      "Agnostic loss: 0.76407933\n",
      "Iteration: 5000\n",
      "Semantic loss: 0.82223260\n",
      "Agnostic loss: 0.68271637\n",
      "Total loss: 1.50494897\n",
      "Validation loss: 2.19959879\n",
      "Semantic loss: 1.24186051\n",
      "Agnostic loss: 0.95773828\n",
      "Iteration: 5100\n",
      "Semantic loss: 1.24563861\n",
      "Agnostic loss: 0.99314451\n",
      "Total loss: 2.23878312\n",
      "Validation loss: 1.73663485\n",
      "Semantic loss: 0.97624767\n",
      "Agnostic loss: 0.76038718\n",
      "Iteration: 5200\n",
      "Semantic loss: 0.86006486\n",
      "Agnostic loss: 0.65016294\n",
      "Total loss: 1.51022780\n",
      "Validation loss: 1.45742989\n",
      "Semantic loss: 0.85338163\n",
      "Agnostic loss: 0.60404831\n",
      "Iteration: 5300\n",
      "Semantic loss: 1.67689610\n",
      "Agnostic loss: 0.81867862\n",
      "Total loss: 2.49557471\n",
      "Validation loss: 3.26479530\n",
      "Semantic loss: 1.53343523\n",
      "Agnostic loss: 1.73135996\n",
      "Iteration: 5400\n",
      "Semantic loss: 1.32007253\n",
      "Agnostic loss: 0.88523996\n",
      "Total loss: 2.20531249\n",
      "Validation loss: 2.93284512\n",
      "Semantic loss: 1.48774242\n",
      "Agnostic loss: 1.44510269\n",
      "Iteration: 5500\n",
      "Semantic loss: 1.04163980\n",
      "Agnostic loss: 0.96584493\n",
      "Total loss: 2.00748467\n",
      "Validation loss: 2.11315942\n",
      "Semantic loss: 1.10755658\n",
      "Agnostic loss: 1.00560284\n",
      "Iteration: 5600\n",
      "Semantic loss: 1.14246452\n",
      "Agnostic loss: 0.64756715\n",
      "Total loss: 1.79003167\n",
      "Validation loss: 2.11627340\n",
      "Semantic loss: 1.35159183\n",
      "Agnostic loss: 0.76468164\n",
      "Iteration: 5700\n",
      "Semantic loss: 1.55427289\n",
      "Agnostic loss: 0.95697641\n",
      "Total loss: 2.51124930\n",
      "Validation loss: 1.18278396\n",
      "Semantic loss: 0.76898021\n",
      "Agnostic loss: 0.41380376\n",
      "Iteration: 5800\n",
      "Semantic loss: 0.99987537\n",
      "Agnostic loss: 0.68076956\n",
      "Total loss: 1.68064499\n",
      "Validation loss: 1.38824368\n",
      "Semantic loss: 0.79231256\n",
      "Agnostic loss: 0.59593105\n",
      "Iteration: 5900\n",
      "Semantic loss: 1.93788421\n",
      "Agnostic loss: 1.71675062\n",
      "Total loss: 3.65463495\n",
      "Validation loss: 1.42927074\n",
      "Semantic loss: 0.68396616\n",
      "Agnostic loss: 0.74530458\n",
      "Iteration: 6000\n",
      "Semantic loss: 1.64581239\n",
      "Agnostic loss: 1.57861114\n",
      "Total loss: 3.22442341\n",
      "Validation loss: 1.93893325\n",
      "Semantic loss: 1.05286777\n",
      "Agnostic loss: 0.88606548\n",
      "Iteration: 6100\n",
      "Semantic loss: 1.27549493\n",
      "Agnostic loss: 0.86071372\n",
      "Total loss: 2.13620853\n",
      "Validation loss: 1.61586213\n",
      "Semantic loss: 1.04449296\n",
      "Agnostic loss: 0.57136923\n",
      "Iteration: 6200\n",
      "Semantic loss: 1.36829758\n",
      "Agnostic loss: 0.67642701\n",
      "Total loss: 2.04472446\n",
      "Validation loss: 1.65915167\n",
      "Semantic loss: 1.00609708\n",
      "Agnostic loss: 0.65305459\n",
      "Iteration: 6300\n",
      "Semantic loss: 0.83435202\n",
      "Agnostic loss: 0.54676938\n",
      "Total loss: 1.38112140\n",
      "Validation loss: 1.95872378\n",
      "Semantic loss: 0.99995393\n",
      "Agnostic loss: 0.95876986\n",
      "Iteration: 6400\n",
      "Semantic loss: 1.66959453\n",
      "Agnostic loss: 1.37473631\n",
      "Total loss: 3.04433084\n",
      "Validation loss: 3.12323046\n",
      "Semantic loss: 1.67922819\n",
      "Agnostic loss: 1.44400215\n",
      "Iteration: 6500\n",
      "Semantic loss: 0.89165419\n",
      "Agnostic loss: 0.76254755\n",
      "Total loss: 1.65420175\n",
      "Validation loss: 3.24071598\n",
      "Semantic loss: 1.59972537\n",
      "Agnostic loss: 1.64099050\n",
      "Iteration: 6600\n",
      "Semantic loss: 1.40093589\n",
      "Agnostic loss: 0.84956622\n",
      "Total loss: 2.25050211\n",
      "Validation loss: 2.12530422\n",
      "Semantic loss: 1.33270264\n",
      "Agnostic loss: 0.79260153\n",
      "Iteration: 6700\n",
      "Semantic loss: 0.83167893\n",
      "Agnostic loss: 0.59098721\n",
      "Total loss: 1.42266607\n",
      "Validation loss: 1.76617348\n",
      "Semantic loss: 1.01849580\n",
      "Agnostic loss: 0.74767768\n",
      "Iteration: 6800\n",
      "Semantic loss: 0.96769547\n",
      "Agnostic loss: 0.88264799\n",
      "Total loss: 1.85034347\n",
      "Validation loss: 1.77579832\n",
      "Semantic loss: 0.98804528\n",
      "Agnostic loss: 0.78775299\n",
      "Iteration: 6900\n",
      "Semantic loss: 1.05473900\n",
      "Agnostic loss: 0.87200594\n",
      "Total loss: 1.92674494\n",
      "Validation loss: 2.41331434\n",
      "Semantic loss: 1.75400710\n",
      "Agnostic loss: 0.65930724\n",
      "Iteration: 7000\n",
      "Semantic loss: 2.69463158\n",
      "Agnostic loss: 0.76331794\n",
      "Total loss: 3.45794964\n",
      "Validation loss: 3.97943258\n",
      "Semantic loss: 3.24253345\n",
      "Agnostic loss: 0.73689920\n",
      "Iteration: 7100\n",
      "Semantic loss: 1.62730026\n",
      "Agnostic loss: 0.85649031\n",
      "Total loss: 2.48379064\n",
      "Validation loss: 2.93916988\n",
      "Semantic loss: 1.94262552\n",
      "Agnostic loss: 0.99654430\n",
      "Iteration: 7200\n",
      "Semantic loss: 1.71206522\n",
      "Agnostic loss: 1.40186191\n",
      "Total loss: 3.11392713\n",
      "Validation loss: 3.47636127\n",
      "Semantic loss: 1.81794834\n",
      "Agnostic loss: 1.65841293\n",
      "Iteration: 7300\n",
      "Semantic loss: 1.17634797\n",
      "Agnostic loss: 0.81686813\n",
      "Total loss: 1.99321604\n",
      "Validation loss: 1.01677585\n",
      "Semantic loss: 0.56320584\n",
      "Agnostic loss: 0.45357001\n",
      "Iteration: 7400\n",
      "Semantic loss: 1.26378334\n",
      "Agnostic loss: 1.03768325\n",
      "Total loss: 2.30146646\n",
      "Validation loss: 1.56438982\n",
      "Semantic loss: 0.85376298\n",
      "Agnostic loss: 0.71062684\n",
      "Iteration: 7500\n",
      "Semantic loss: 1.26695096\n",
      "Agnostic loss: 0.78515857\n",
      "Total loss: 2.05210948\n",
      "Validation loss: 1.49011636\n",
      "Semantic loss: 0.93071187\n",
      "Agnostic loss: 0.55940455\n",
      "Iteration: 7600\n",
      "Semantic loss: 1.70807862\n",
      "Agnostic loss: 0.54192322\n",
      "Total loss: 2.25000191\n",
      "Validation loss: 1.70271468\n",
      "Semantic loss: 0.71689421\n",
      "Agnostic loss: 0.98582053\n",
      "Iteration: 7700\n",
      "Semantic loss: 2.50329089\n",
      "Agnostic loss: 1.93778121\n",
      "Total loss: 4.44107199\n",
      "Validation loss: 2.59612656\n",
      "Semantic loss: 1.47484028\n",
      "Agnostic loss: 1.12128639\n",
      "Iteration: 7800\n",
      "Semantic loss: 1.27137542\n",
      "Agnostic loss: 0.87238884\n",
      "Total loss: 2.14376426\n",
      "Validation loss: 2.57212162\n",
      "Semantic loss: 1.80427337\n",
      "Agnostic loss: 0.76784837\n",
      "Iteration: 7900\n",
      "Semantic loss: 0.50076503\n",
      "Agnostic loss: 0.42285180\n",
      "Total loss: 0.92361683\n",
      "Validation loss: 1.66717160\n",
      "Semantic loss: 0.78892159\n",
      "Agnostic loss: 0.87825000\n",
      "Iteration: 8000\n",
      "Semantic loss: 1.18661582\n",
      "Agnostic loss: 1.25389183\n",
      "Total loss: 2.44050765\n",
      "Validation loss: 1.65783739\n",
      "Semantic loss: 1.00100911\n",
      "Agnostic loss: 0.65682834\n",
      "Iteration: 8100\n",
      "Semantic loss: 1.06644964\n",
      "Agnostic loss: 1.00359535\n",
      "Total loss: 2.07004499\n",
      "Validation loss: 0.84542286\n",
      "Semantic loss: 0.53956074\n",
      "Agnostic loss: 0.30586210\n",
      "Iteration: 8200\n",
      "Semantic loss: 1.30320096\n",
      "Agnostic loss: 1.16011477\n",
      "Total loss: 2.46331573\n",
      "Validation loss: 2.08671808\n",
      "Semantic loss: 1.70513725\n",
      "Agnostic loss: 0.38158077\n",
      "Iteration: 8300\n",
      "Semantic loss: 0.83817405\n",
      "Agnostic loss: 0.65620023\n",
      "Total loss: 1.49437428\n",
      "Validation loss: 1.98636246\n",
      "Semantic loss: 1.34877205\n",
      "Agnostic loss: 0.63759041\n",
      "Iteration: 8400\n",
      "Semantic loss: 0.96408439\n",
      "Agnostic loss: 0.86785126\n",
      "Total loss: 1.83193564\n",
      "Validation loss: 1.94393265\n",
      "Semantic loss: 0.89914036\n",
      "Agnostic loss: 1.04479229\n",
      "Iteration: 8500\n",
      "Semantic loss: 2.14487934\n",
      "Agnostic loss: 0.81115699\n",
      "Total loss: 2.95603633\n",
      "Validation loss: 3.16030407\n",
      "Semantic loss: 2.06250453\n",
      "Agnostic loss: 1.09779966\n",
      "Iteration: 8600\n",
      "Semantic loss: 1.67090929\n",
      "Agnostic loss: 1.53382158\n",
      "Total loss: 3.20473099\n",
      "Validation loss: 2.69911909\n",
      "Semantic loss: 1.30027652\n",
      "Agnostic loss: 1.39884269\n",
      "Iteration: 8700\n",
      "Semantic loss: 1.35973048\n",
      "Agnostic loss: 0.82286441\n",
      "Total loss: 2.18259478\n",
      "Validation loss: 1.92259121\n",
      "Semantic loss: 1.03991342\n",
      "Agnostic loss: 0.88267785\n",
      "Iteration: 8800\n",
      "Semantic loss: 1.43150580\n",
      "Agnostic loss: 1.08837676\n",
      "Total loss: 2.51988268\n",
      "Validation loss: 1.69472802\n",
      "Semantic loss: 0.90369105\n",
      "Agnostic loss: 0.79103696\n",
      "Iteration: 8900\n",
      "Semantic loss: 0.92948949\n",
      "Agnostic loss: 0.95730698\n",
      "Total loss: 1.88679647\n",
      "Validation loss: 2.16060710\n",
      "Semantic loss: 1.01699114\n",
      "Agnostic loss: 1.14361596\n",
      "Iteration: 9000\n",
      "Semantic loss: 1.19179440\n",
      "Agnostic loss: 0.60124916\n",
      "Total loss: 1.79304361\n",
      "Validation loss: 1.94359612\n",
      "Semantic loss: 1.22866547\n",
      "Agnostic loss: 0.71493065\n",
      "Iteration: 9100\n",
      "Semantic loss: 0.66250777\n",
      "Agnostic loss: 0.29459453\n",
      "Total loss: 0.95710230\n",
      "Validation loss: 1.74606740\n",
      "Semantic loss: 1.01342094\n",
      "Agnostic loss: 0.73264647\n",
      "Iteration: 9200\n",
      "Semantic loss: 0.72225714\n",
      "Agnostic loss: 0.36958820\n",
      "Total loss: 1.09184527\n",
      "Validation loss: 3.44047213\n",
      "Semantic loss: 2.21090317\n",
      "Agnostic loss: 1.22956896\n",
      "Iteration: 9300\n",
      "Semantic loss: 1.62067842\n",
      "Agnostic loss: 0.86758763\n",
      "Total loss: 2.48826599\n",
      "Validation loss: 1.80417001\n",
      "Semantic loss: 1.13847816\n",
      "Agnostic loss: 0.66569185\n",
      "Iteration: 9400\n",
      "Semantic loss: 1.80818748\n",
      "Agnostic loss: 1.06024337\n",
      "Total loss: 2.86843085\n",
      "Validation loss: 1.30350924\n",
      "Semantic loss: 0.74197525\n",
      "Agnostic loss: 0.56153405\n",
      "Iteration: 9500\n",
      "Semantic loss: 0.47783279\n",
      "Agnostic loss: 0.52519757\n",
      "Total loss: 1.00303030\n",
      "Validation loss: 1.66283059\n",
      "Semantic loss: 0.72621238\n",
      "Agnostic loss: 0.93661815\n",
      "Iteration: 9600\n",
      "Semantic loss: 1.04532862\n",
      "Agnostic loss: 1.11633611\n",
      "Total loss: 2.16166472\n",
      "Validation loss: 1.29459894\n",
      "Semantic loss: 0.65548813\n",
      "Agnostic loss: 0.63911080\n",
      "Iteration: 9700\n",
      "Semantic loss: 0.82873333\n",
      "Agnostic loss: 0.55525160\n",
      "Total loss: 1.38398492\n",
      "Validation loss: 2.69247341\n",
      "Semantic loss: 1.17987072\n",
      "Agnostic loss: 1.51260281\n",
      "Iteration: 9800\n",
      "Semantic loss: 1.05914307\n",
      "Agnostic loss: 0.79313743\n",
      "Total loss: 1.85228050\n",
      "Validation loss: 2.18253946\n",
      "Semantic loss: 1.32701898\n",
      "Agnostic loss: 0.85552049\n",
      "Iteration: 9900\n",
      "Semantic loss: 1.36313510\n",
      "Agnostic loss: 1.17922807\n",
      "Total loss: 2.54236317\n",
      "Validation loss: 2.29491186\n",
      "Semantic loss: 1.00569606\n",
      "Agnostic loss: 1.28921592\n",
      "Iteration: 10000\n",
      "Semantic loss: 1.18916798\n",
      "Agnostic loss: 0.44101843\n",
      "Total loss: 1.63018644\n",
      "Validation loss: 2.13315153\n",
      "Semantic loss: 1.40704513\n",
      "Agnostic loss: 0.72610629\n",
      "Iteration: 10100\n",
      "Semantic loss: 1.31908727\n",
      "Agnostic loss: 0.63854599\n",
      "Total loss: 1.95763326\n",
      "Validation loss: 2.22929955\n",
      "Semantic loss: 1.52926338\n",
      "Agnostic loss: 0.70003605\n",
      "Iteration: 10200\n",
      "Semantic loss: 1.63211656\n",
      "Agnostic loss: 0.90290952\n",
      "Total loss: 2.53502607\n",
      "Validation loss: 3.40386748\n",
      "Semantic loss: 2.55178475\n",
      "Agnostic loss: 0.85208267\n",
      "Iteration: 10300\n",
      "Semantic loss: 1.55354285\n",
      "Agnostic loss: 0.95226365\n",
      "Total loss: 2.50580645\n",
      "Validation loss: 2.66409278\n",
      "Semantic loss: 1.69817114\n",
      "Agnostic loss: 0.96592158\n",
      "Iteration: 10400\n",
      "Semantic loss: 1.21273160\n",
      "Agnostic loss: 0.77169490\n",
      "Total loss: 1.98442650\n",
      "Validation loss: 2.68528461\n",
      "Semantic loss: 1.40164852\n",
      "Agnostic loss: 1.28363621\n",
      "Iteration: 10500\n",
      "Semantic loss: 0.79785359\n",
      "Agnostic loss: 0.35949874\n",
      "Total loss: 1.15735233\n",
      "Validation loss: 3.50460768\n",
      "Semantic loss: 1.94013071\n",
      "Agnostic loss: 1.56447685\n",
      "Iteration: 10600\n",
      "Semantic loss: 1.32928026\n",
      "Agnostic loss: 0.53766096\n",
      "Total loss: 1.86694121\n",
      "Validation loss: 1.55123258\n",
      "Semantic loss: 0.86647165\n",
      "Agnostic loss: 0.68476099\n",
      "Iteration: 10700\n",
      "Semantic loss: 1.29709446\n",
      "Agnostic loss: 1.03601372\n",
      "Total loss: 2.33310819\n",
      "Validation loss: 1.31395531\n",
      "Semantic loss: 0.71510410\n",
      "Agnostic loss: 0.59885114\n",
      "Iteration: 10800\n",
      "Semantic loss: 1.76026320\n",
      "Agnostic loss: 0.63394260\n",
      "Total loss: 2.39420581\n",
      "Validation loss: 0.84105098\n",
      "Semantic loss: 0.35323840\n",
      "Agnostic loss: 0.48781258\n",
      "Iteration: 10900\n",
      "Semantic loss: 0.47904563\n",
      "Agnostic loss: 0.42695248\n",
      "Total loss: 0.90599811\n",
      "Validation loss: 1.81813502\n",
      "Semantic loss: 0.78126001\n",
      "Agnostic loss: 1.03687501\n",
      "Iteration: 11000\n",
      "Semantic loss: 1.47814429\n",
      "Agnostic loss: 1.48506129\n",
      "Total loss: 2.96320558\n",
      "Validation loss: 1.77397227\n",
      "Semantic loss: 1.23054183\n",
      "Agnostic loss: 0.54343045\n",
      "Iteration: 11100\n",
      "Semantic loss: 1.09287190\n",
      "Agnostic loss: 0.65681803\n",
      "Total loss: 1.74968994\n",
      "Validation loss: 1.33165979\n",
      "Semantic loss: 0.87830901\n",
      "Agnostic loss: 0.45335072\n",
      "Iteration: 11200\n",
      "Semantic loss: 0.92405474\n",
      "Agnostic loss: 0.50414413\n",
      "Total loss: 1.42819881\n",
      "Validation loss: 2.97497749\n",
      "Semantic loss: 1.46679306\n",
      "Agnostic loss: 1.50818443\n",
      "Iteration: 11300\n",
      "Semantic loss: 2.14424658\n",
      "Agnostic loss: 1.58540702\n",
      "Total loss: 3.72965360\n",
      "Validation loss: 2.17587948\n",
      "Semantic loss: 1.13673043\n",
      "Agnostic loss: 1.03914905\n",
      "Iteration: 11400\n",
      "Semantic loss: 1.30531907\n",
      "Agnostic loss: 0.98178071\n",
      "Total loss: 2.28709984\n",
      "Validation loss: 3.06732368\n",
      "Semantic loss: 1.97672772\n",
      "Agnostic loss: 1.09059596\n",
      "Iteration: 11500\n",
      "Semantic loss: 0.93047065\n",
      "Agnostic loss: 0.45697403\n",
      "Total loss: 1.38744473\n",
      "Validation loss: 1.86212206\n",
      "Semantic loss: 0.99558812\n",
      "Agnostic loss: 0.86653399\n",
      "Iteration: 11600\n",
      "Semantic loss: 1.53521311\n",
      "Agnostic loss: 1.06659639\n",
      "Total loss: 2.60180950\n",
      "Validation loss: 2.75656748\n",
      "Semantic loss: 1.65661609\n",
      "Agnostic loss: 1.09995127\n",
      "Iteration: 11700\n",
      "Semantic loss: 0.96448159\n",
      "Agnostic loss: 1.02712059\n",
      "Total loss: 1.99160218\n",
      "Validation loss: 3.14559174\n",
      "Semantic loss: 1.65277970\n",
      "Agnostic loss: 1.49281192\n",
      "Iteration: 11800\n",
      "Semantic loss: 0.94917804\n",
      "Agnostic loss: 0.60281241\n",
      "Total loss: 1.55199051\n",
      "Validation loss: 1.85053909\n",
      "Semantic loss: 0.75176525\n",
      "Agnostic loss: 1.09877384\n",
      "Iteration: 11900\n",
      "Semantic loss: 1.04194260\n",
      "Agnostic loss: 0.55298519\n",
      "Total loss: 1.59492779\n",
      "Validation loss: 1.66686726\n",
      "Semantic loss: 0.99471408\n",
      "Agnostic loss: 0.67215323\n",
      "Iteration: 12000\n",
      "Semantic loss: 1.27274382\n",
      "Agnostic loss: 0.79730237\n",
      "Total loss: 2.07004619\n",
      "Validation loss: 2.74159336\n",
      "Semantic loss: 1.52925634\n",
      "Agnostic loss: 1.21233702\n",
      "Iteration: 12100\n",
      "Semantic loss: 0.75874406\n",
      "Agnostic loss: 0.46922976\n",
      "Total loss: 1.22797382\n",
      "Validation loss: 1.81937993\n",
      "Semantic loss: 0.86601210\n",
      "Agnostic loss: 0.95336783\n",
      "Iteration: 12200\n",
      "Semantic loss: 0.52201796\n",
      "Agnostic loss: 0.35077119\n",
      "Total loss: 0.87278914\n",
      "Validation loss: 1.35257888\n",
      "Semantic loss: 0.94663489\n",
      "Agnostic loss: 0.40594405\n",
      "Iteration: 12300\n",
      "Semantic loss: 0.74012613\n",
      "Agnostic loss: 0.77543366\n",
      "Total loss: 1.51555979\n",
      "Validation loss: 1.53718615\n",
      "Semantic loss: 0.96207547\n",
      "Agnostic loss: 0.57511073\n",
      "Iteration: 12400\n",
      "Semantic loss: 1.14302397\n",
      "Agnostic loss: 0.71678346\n",
      "Total loss: 1.85980749\n",
      "Validation loss: 1.01421225\n",
      "Semantic loss: 0.62638545\n",
      "Agnostic loss: 0.38782680\n",
      "Iteration: 12500\n",
      "Semantic loss: 0.97838020\n",
      "Agnostic loss: 0.84497726\n",
      "Total loss: 1.82335746\n",
      "Validation loss: 1.09830344\n",
      "Semantic loss: 0.43859291\n",
      "Agnostic loss: 0.65971053\n",
      "Iteration: 12600\n",
      "Semantic loss: 0.77379847\n",
      "Agnostic loss: 0.53535926\n",
      "Total loss: 1.30915773\n",
      "Validation loss: 1.54528964\n",
      "Semantic loss: 1.01595473\n",
      "Agnostic loss: 0.52933490\n",
      "Iteration: 12700\n",
      "Semantic loss: 0.91677165\n",
      "Agnostic loss: 1.15461373\n",
      "Total loss: 2.07138538\n",
      "Validation loss: 2.49081802\n",
      "Semantic loss: 1.17602837\n",
      "Agnostic loss: 1.31478965\n",
      "Iteration: 12800\n",
      "Semantic loss: 1.70174670\n",
      "Agnostic loss: 1.05957031\n",
      "Total loss: 2.76131701\n",
      "Validation loss: 1.32465720\n",
      "Semantic loss: 0.65771592\n",
      "Agnostic loss: 0.66694129\n",
      "Iteration: 12900\n",
      "Semantic loss: 0.73050117\n",
      "Agnostic loss: 0.86428142\n",
      "Total loss: 1.59478259\n",
      "Validation loss: 1.68632293\n",
      "Semantic loss: 0.87536347\n",
      "Agnostic loss: 0.81095946\n",
      "Iteration: 13000\n",
      "Semantic loss: 0.39749324\n",
      "Agnostic loss: 0.39827141\n",
      "Total loss: 0.79576468\n",
      "Validation loss: 1.09884524\n",
      "Semantic loss: 0.60838938\n",
      "Agnostic loss: 0.49045581\n",
      "Iteration: 13100\n",
      "Semantic loss: 0.70957839\n",
      "Agnostic loss: 0.58462727\n",
      "Total loss: 1.29420567\n",
      "Validation loss: 1.59917617\n",
      "Semantic loss: 1.16142082\n",
      "Agnostic loss: 0.43775541\n",
      "Iteration: 13200\n",
      "Semantic loss: 1.22166800\n",
      "Agnostic loss: 0.96026659\n",
      "Total loss: 2.18193460\n",
      "Validation loss: 1.76796448\n",
      "Semantic loss: 1.05194247\n",
      "Agnostic loss: 0.71602201\n",
      "Iteration: 13300\n",
      "Semantic loss: 1.44965410\n",
      "Agnostic loss: 1.18146741\n",
      "Total loss: 2.63112164\n",
      "Validation loss: 0.96459025\n",
      "Semantic loss: 0.62140644\n",
      "Agnostic loss: 0.34318382\n",
      "Iteration: 13400\n",
      "Semantic loss: 0.93128860\n",
      "Agnostic loss: 0.61361057\n",
      "Total loss: 1.54489923\n",
      "Validation loss: 2.89561725\n",
      "Semantic loss: 1.74053550\n",
      "Agnostic loss: 1.15508175\n",
      "Iteration: 13500\n",
      "Semantic loss: 1.39119148\n",
      "Agnostic loss: 0.91340202\n",
      "Total loss: 2.30459356\n",
      "Validation loss: 1.84461045\n",
      "Semantic loss: 1.07836008\n",
      "Agnostic loss: 0.76625037\n",
      "Iteration: 13600\n",
      "Semantic loss: 1.40118051\n",
      "Agnostic loss: 1.22707820\n",
      "Total loss: 2.62825871\n",
      "Validation loss: 1.03642976\n",
      "Semantic loss: 0.69984066\n",
      "Agnostic loss: 0.33658913\n",
      "Iteration: 13700\n",
      "Semantic loss: 0.70357674\n",
      "Agnostic loss: 0.48372096\n",
      "Total loss: 1.18729770\n",
      "Validation loss: 1.94117510\n",
      "Semantic loss: 1.17279172\n",
      "Agnostic loss: 0.76838338\n",
      "Iteration: 13800\n",
      "Semantic loss: 1.76664901\n",
      "Agnostic loss: 1.75843477\n",
      "Total loss: 3.52508378\n",
      "Validation loss: 0.87955129\n",
      "Semantic loss: 0.64939737\n",
      "Agnostic loss: 0.23015395\n",
      "Iteration: 13900\n",
      "Semantic loss: 0.65840995\n",
      "Agnostic loss: 0.72093737\n",
      "Total loss: 1.37934732\n",
      "Validation loss: 1.09523892\n",
      "Semantic loss: 0.48242182\n",
      "Agnostic loss: 0.61281705\n",
      "Iteration: 14000\n",
      "Semantic loss: 0.73306435\n",
      "Agnostic loss: 0.51908314\n",
      "Total loss: 1.25214744\n",
      "Validation loss: 1.22473192\n",
      "Semantic loss: 0.63955331\n",
      "Agnostic loss: 0.58517861\n",
      "Iteration: 14100\n",
      "Semantic loss: 1.02793181\n",
      "Agnostic loss: 0.53799444\n",
      "Total loss: 1.56592631\n",
      "Validation loss: 2.85592890\n",
      "Semantic loss: 1.51231432\n",
      "Agnostic loss: 1.34361458\n",
      "Iteration: 14200\n",
      "Semantic loss: 0.84823984\n",
      "Agnostic loss: 0.44272885\n",
      "Total loss: 1.29096866\n",
      "Validation loss: 1.87746596\n",
      "Semantic loss: 1.06062520\n",
      "Agnostic loss: 0.81684077\n",
      "Iteration: 14300\n",
      "Semantic loss: 0.89220041\n",
      "Agnostic loss: 0.33557904\n",
      "Total loss: 1.22777939\n",
      "Validation loss: 1.42496812\n",
      "Semantic loss: 0.95910728\n",
      "Agnostic loss: 0.46586084\n",
      "Iteration: 14400\n",
      "Semantic loss: 0.99220741\n",
      "Agnostic loss: 0.76618183\n",
      "Total loss: 1.75838923\n",
      "Validation loss: 2.42947912\n",
      "Semantic loss: 1.54648781\n",
      "Agnostic loss: 0.88299131\n",
      "Iteration: 14500\n",
      "Semantic loss: 0.52724016\n",
      "Agnostic loss: 0.41496378\n",
      "Total loss: 0.94220394\n",
      "Validation loss: 1.65363586\n",
      "Semantic loss: 1.01478195\n",
      "Agnostic loss: 0.63885391\n",
      "Iteration: 14600\n",
      "Semantic loss: 0.41137415\n",
      "Agnostic loss: 0.37065142\n",
      "Total loss: 0.78202558\n",
      "Validation loss: 1.96161377\n",
      "Semantic loss: 1.07391095\n",
      "Agnostic loss: 0.88770282\n",
      "Iteration: 14700\n",
      "Semantic loss: 1.33905077\n",
      "Agnostic loss: 0.95232785\n",
      "Total loss: 2.29137850\n",
      "Validation loss: 1.97778964\n",
      "Semantic loss: 1.25209475\n",
      "Agnostic loss: 0.72569489\n",
      "Iteration: 14800\n",
      "Semantic loss: 0.62783468\n",
      "Agnostic loss: 0.72376335\n",
      "Total loss: 1.35159802\n",
      "Validation loss: 1.19849038\n",
      "Semantic loss: 0.73886395\n",
      "Agnostic loss: 0.45962644\n",
      "Iteration: 14900\n",
      "Semantic loss: 0.60081112\n",
      "Agnostic loss: 0.24466062\n",
      "Total loss: 0.84547174\n",
      "Validation loss: 1.19683957\n",
      "Semantic loss: 0.65434211\n",
      "Agnostic loss: 0.54249752\n",
      "Iteration: 15000\n",
      "Semantic loss: 1.48118329\n",
      "Agnostic loss: 1.05637288\n",
      "Total loss: 2.53755617\n",
      "Validation loss: 2.44334316\n",
      "Semantic loss: 1.50955689\n",
      "Agnostic loss: 0.93378615\n",
      "Iteration: 15100\n",
      "Semantic loss: 0.80516505\n",
      "Agnostic loss: 0.57206506\n",
      "Total loss: 1.37723017\n",
      "Validation loss: 0.82661033\n",
      "Semantic loss: 0.51530367\n",
      "Agnostic loss: 0.31130666\n",
      "Iteration: 15200\n",
      "Semantic loss: 0.55035222\n",
      "Agnostic loss: 0.46133202\n",
      "Total loss: 1.01168418\n",
      "Validation loss: 1.83466327\n",
      "Semantic loss: 1.00496244\n",
      "Agnostic loss: 0.82970083\n",
      "Iteration: 15300\n",
      "Semantic loss: 1.03064477\n",
      "Agnostic loss: 0.99943316\n",
      "Total loss: 2.03007793\n",
      "Validation loss: 1.87427008\n",
      "Semantic loss: 1.01703906\n",
      "Agnostic loss: 0.85723102\n",
      "Iteration: 15344\n",
      "Semantic loss: 0.68978709\n",
      "Agnostic loss: 0.61759377\n",
      "Total loss: 1.30738091\n",
      "Validation loss: 1.70600986\n",
      "Semantic loss: 1.04422879\n",
      "Agnostic loss: 0.66178101\n",
      "Epoch: 3\n",
      "Iteration:  100\n",
      "Semantic loss: 0.66480833\n",
      "Agnostic loss: 0.51958877\n",
      "Total loss: 1.18439710\n",
      "Validation loss: 2.10356045\n",
      "Semantic loss: 1.26279330\n",
      "Agnostic loss: 0.84076715\n",
      "Iteration:  200\n",
      "Semantic loss: 1.00956094\n",
      "Agnostic loss: 0.69544214\n",
      "Total loss: 1.70500302\n",
      "Validation loss: 1.13144171\n",
      "Semantic loss: 0.78070742\n",
      "Agnostic loss: 0.35073429\n",
      "Iteration:  300\n",
      "Semantic loss: 1.50642812\n",
      "Agnostic loss: 0.68374741\n",
      "Total loss: 2.19017553\n",
      "Validation loss: 1.36699462\n",
      "Semantic loss: 0.72119963\n",
      "Agnostic loss: 0.64579499\n",
      "Iteration:  400\n",
      "Semantic loss: 1.34223270\n",
      "Agnostic loss: 0.78655779\n",
      "Total loss: 2.12879038\n",
      "Validation loss: 1.95423245\n",
      "Semantic loss: 1.05568814\n",
      "Agnostic loss: 0.89854437\n",
      "Iteration:  500\n",
      "Semantic loss: 0.89665675\n",
      "Agnostic loss: 1.00104320\n",
      "Total loss: 1.89769995\n",
      "Validation loss: 1.09537351\n",
      "Semantic loss: 0.57745284\n",
      "Agnostic loss: 0.51792067\n",
      "Iteration:  600\n",
      "Semantic loss: 0.41716769\n",
      "Agnostic loss: 0.34385824\n",
      "Total loss: 0.76102591\n",
      "Validation loss: 1.61805463\n",
      "Semantic loss: 0.87022007\n",
      "Agnostic loss: 0.74783456\n",
      "Iteration:  700\n",
      "Semantic loss: 0.89339060\n",
      "Agnostic loss: 0.51808822\n",
      "Total loss: 1.41147876\n",
      "Validation loss: 1.20785677\n",
      "Semantic loss: 0.72777164\n",
      "Agnostic loss: 0.48008513\n",
      "Iteration:  800\n",
      "Semantic loss: 1.42675757\n",
      "Agnostic loss: 0.94125223\n",
      "Total loss: 2.36800981\n",
      "Validation loss: 0.86943644\n",
      "Semantic loss: 0.39208853\n",
      "Agnostic loss: 0.47734791\n",
      "Iteration:  900\n",
      "Semantic loss: 0.97380519\n",
      "Agnostic loss: 1.10191822\n",
      "Total loss: 2.07572341\n",
      "Validation loss: 1.28328800\n",
      "Semantic loss: 0.62558109\n",
      "Agnostic loss: 0.65770686\n",
      "Iteration: 1000\n",
      "Semantic loss: 1.17369246\n",
      "Agnostic loss: 0.81063211\n",
      "Total loss: 1.98432457\n",
      "Validation loss: 1.51236773\n",
      "Semantic loss: 0.85480833\n",
      "Agnostic loss: 0.65755945\n",
      "Iteration: 1100\n",
      "Semantic loss: 1.56827474\n",
      "Agnostic loss: 1.24039531\n",
      "Total loss: 2.80867004\n",
      "Validation loss: 2.14084148\n",
      "Semantic loss: 1.29966021\n",
      "Agnostic loss: 0.84118140\n",
      "Iteration: 1200\n",
      "Semantic loss: 1.44202375\n",
      "Agnostic loss: 0.69988585\n",
      "Total loss: 2.14190960\n",
      "Validation loss: 2.45477057\n",
      "Semantic loss: 1.08081460\n",
      "Agnostic loss: 1.37395585\n",
      "Iteration: 1300\n",
      "Semantic loss: 1.31839919\n",
      "Agnostic loss: 1.01951432\n",
      "Total loss: 2.33791351\n",
      "Validation loss: 1.51937056\n",
      "Semantic loss: 1.00286829\n",
      "Agnostic loss: 0.51650220\n",
      "Iteration: 1400\n",
      "Semantic loss: 0.68428272\n",
      "Agnostic loss: 0.55262285\n",
      "Total loss: 1.23690557\n",
      "Validation loss: 1.82766712\n",
      "Semantic loss: 0.93925571\n",
      "Agnostic loss: 0.88841140\n",
      "Iteration: 1500\n",
      "Semantic loss: 0.76846337\n",
      "Agnostic loss: 0.37207502\n",
      "Total loss: 1.14053845\n",
      "Validation loss: 1.83036697\n",
      "Semantic loss: 1.10903287\n",
      "Agnostic loss: 0.72133410\n",
      "Iteration: 1600\n",
      "Semantic loss: 1.96920002\n",
      "Agnostic loss: 1.64910519\n",
      "Total loss: 3.61830521\n",
      "Validation loss: 2.88595176\n",
      "Semantic loss: 1.70013261\n",
      "Agnostic loss: 1.18581915\n",
      "Iteration: 1700\n",
      "Semantic loss: 1.34480548\n",
      "Agnostic loss: 0.41355449\n",
      "Total loss: 1.75835991\n",
      "Validation loss: 1.99455631\n",
      "Semantic loss: 1.30663502\n",
      "Agnostic loss: 0.68792129\n",
      "Iteration: 1800\n",
      "Semantic loss: 0.69681299\n",
      "Agnostic loss: 0.49450094\n",
      "Total loss: 1.19131398\n",
      "Validation loss: 1.74195695\n",
      "Semantic loss: 1.23483253\n",
      "Agnostic loss: 0.50712448\n",
      "Iteration: 1900\n",
      "Semantic loss: 2.03218484\n",
      "Agnostic loss: 0.58087170\n",
      "Total loss: 2.61305666\n",
      "Validation loss: 0.87551492\n",
      "Semantic loss: 0.54317182\n",
      "Agnostic loss: 0.33234310\n",
      "Iteration: 2000\n",
      "Semantic loss: 0.80574900\n",
      "Agnostic loss: 0.95280701\n",
      "Total loss: 1.75855601\n",
      "Validation loss: 0.91983509\n",
      "Semantic loss: 0.58713609\n",
      "Agnostic loss: 0.33269900\n",
      "Iteration: 2100\n",
      "Semantic loss: 0.85644615\n",
      "Agnostic loss: 0.49905372\n",
      "Total loss: 1.35549986\n",
      "Validation loss: 2.42098236\n",
      "Semantic loss: 1.22816348\n",
      "Agnostic loss: 1.19281888\n",
      "Iteration: 2200\n",
      "Semantic loss: 0.73262292\n",
      "Agnostic loss: 0.43729544\n",
      "Total loss: 1.16991830\n",
      "Validation loss: 1.18200028\n",
      "Semantic loss: 0.77137899\n",
      "Agnostic loss: 0.41062126\n",
      "Iteration: 2300\n",
      "Semantic loss: 0.54130465\n",
      "Agnostic loss: 0.29998833\n",
      "Total loss: 0.84129298\n",
      "Validation loss: 1.15857530\n",
      "Semantic loss: 0.65133661\n",
      "Agnostic loss: 0.50723869\n",
      "Iteration: 2400\n",
      "Semantic loss: 0.61613452\n",
      "Agnostic loss: 0.49387950\n",
      "Total loss: 1.11001396\n",
      "Validation loss: 1.40268469\n",
      "Semantic loss: 0.88479471\n",
      "Agnostic loss: 0.51788998\n",
      "Iteration: 2500\n",
      "Semantic loss: 1.09313059\n",
      "Agnostic loss: 1.07982087\n",
      "Total loss: 2.17295146\n",
      "Validation loss: 1.33684397\n",
      "Semantic loss: 0.65159357\n",
      "Agnostic loss: 0.68525040\n",
      "Iteration: 2600\n",
      "Semantic loss: 0.47763216\n",
      "Agnostic loss: 0.26773828\n",
      "Total loss: 0.74537045\n",
      "Validation loss: 1.34042025\n",
      "Semantic loss: 0.72829944\n",
      "Agnostic loss: 0.61212081\n",
      "Iteration: 2700\n",
      "Semantic loss: 2.29446268\n",
      "Agnostic loss: 1.79123521\n",
      "Total loss: 4.08569813\n",
      "Validation loss: 1.40979838\n",
      "Semantic loss: 0.80601281\n",
      "Agnostic loss: 0.60378563\n",
      "Iteration: 2800\n",
      "Semantic loss: 0.63185167\n",
      "Agnostic loss: 0.71987760\n",
      "Total loss: 1.35172927\n",
      "Validation loss: 1.49250889\n",
      "Semantic loss: 0.88002449\n",
      "Agnostic loss: 0.61248440\n"
     ]
    }
   ],
   "source": [
    "ctc_train, ctc_train_sem, ctc_train_agn, ctc_eval, ctc_eval_sem, ctc_eval_agn = train(model, optimizer, scheduler, primus, epochs, save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [100*x for x in range(len(ctc_train))]\n",
    "\n",
    "plt.plot(iterations, ctc_train, label='Training')\n",
    "plt.plot(iterations, ctc_eval, label='Validation')\n",
    "plt.plot(iterations, ctc_train_sem, label='Semantic (training)')\n",
    "plt.plot(iterations, ctc_train_agn, label='Agnostic (training)')\n",
    "plt.plot(iterations, ctc_eval_sem, label='Semantic (validation)')\n",
    "plt.plot(iterations, ctc_eval_agn, label='Agnostic (validation)')\n",
    "\n",
    "plt.title('CTC Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        seq_err = 0\n",
    "        \n",
    "        seq_err_sem = 0\n",
    "        seq_err_agn = 0\n",
    "        \n",
    "        symbol_err_sem = 0\n",
    "        symbol_err_agn = 0\n",
    "        \n",
    "        symbol_pct_err_sem = 0\n",
    "        symbol_pct_err_agn = 0\n",
    "\n",
    "        total_length_sem = 0\n",
    "        total_length_agn = 0\n",
    "\n",
    "        for _ in range(dataloader.test_iterations):\n",
    "            test_batch, test_labels_sem, test_label_lengths_sem, test_labels_agn, test_label_lengths_agn = dataloader.next_batch(\"test\")\n",
    "\n",
    "            test_preds_sem, test_preds_agn = model(test_batch)\n",
    "            test_preds_sem = test_preds_sem.log_softmax(2).argmax(2)\n",
    "            test_preds_agn = test_preds_agn.log_softmax(2).argmax(2)\n",
    "            del test_batch\n",
    "\n",
    "            test_labels_sem = tensor_to_np_array(test_labels_sem)\n",
    "            test_label_lengths_sem = tensor_to_np_array(test_label_lengths_sem)\n",
    "            test_preds_sem = tensor_to_np_array(test_preds_sem).transpose(1,0)\n",
    "            \n",
    "            test_labels_agn = tensor_to_np_array(test_labels_agn)\n",
    "            test_label_lengths_agn = tensor_to_np_array(test_label_lengths_agn)\n",
    "            test_preds_agn = tensor_to_np_array(test_preds_agn).transpose(1,0)\n",
    "\n",
    "            curr_idx_sem = 0\n",
    "            curr_idx_agn = 0\n",
    "            for i in range(dataloader.batch_size):\n",
    "                length_sem = test_label_lengths_sem[i]\n",
    "                length_agn = test_label_lengths_agn[i]\n",
    "                \n",
    "                pred_sem = test_preds_sem[i]\n",
    "                label_sem = test_labels_sem[curr_idx_sem:curr_idx_sem+length_sem]\n",
    "                pred_agn = test_preds_agn[i]\n",
    "                label_agn = test_labels_agn[curr_idx_agn:curr_idx_agn+length_agn]\n",
    "                \n",
    "                pred_sem = np_to_string(pred_sem, int2word_sem, vocab_size_sem)\n",
    "                label_sem = [int2word_sem[x] for x in label_sem]\n",
    "                pred_agn = np_to_string(pred_agn, int2word_agn, vocab_size_agn)\n",
    "                label_agn = [int2word_agn[x] for x in label_agn]\n",
    "\n",
    "                err_sem = levenshtein(pred_sem, label_sem)\n",
    "                if err_sem > 0:\n",
    "                    seq_err_sem += 1\n",
    "\n",
    "                err_agn = levenshtein(pred_agn, label_agn)\n",
    "                if err_agn > 0:\n",
    "                    seq_err_agn += 1\n",
    "                    \n",
    "                if err_sem > 0 or err_agn > 0:\n",
    "                    seq_err += 1\n",
    "\n",
    "                symbol_err_sem += err_sem\n",
    "                symbol_pct_err_sem += sequence_levenshtein(pred_sem, label_sem)\n",
    "                symbol_err_agn += err_agn\n",
    "                symbol_pct_err_agn += sequence_levenshtein(pred_agn, label_agn)\n",
    "\n",
    "                curr_idx_sem += length_sem\n",
    "                total_length_sem += length_sem\n",
    "                curr_idx_agn += length_agn\n",
    "                total_length_agn += length_agn\n",
    "\n",
    "\n",
    "            del test_labels_sem\n",
    "            del test_preds_sem\n",
    "            del test_label_lengths_sem\n",
    "            del test_labels_agn\n",
    "            del test_preds_agn\n",
    "            del test_label_lengths_agn\n",
    "            \n",
    "            #print(seq_err, seq_err_sem, seq_err_agn)\n",
    "\n",
    "        seq_err /= dataloader.test_iterations * dataloader.batch_size\n",
    "        seq_err_sem /= dataloader.test_iterations * dataloader.batch_size\n",
    "        seq_err_agn /= dataloader.test_iterations * dataloader.batch_size\n",
    "        symbol_err_sem /= total_length_sem\n",
    "        symbol_pct_err_sem /= total_length_sem\n",
    "        symbol_err_agn /= total_length_agn\n",
    "        symbol_pct_err_agn /= total_length_agn\n",
    "        \n",
    "        del total_length_sem\n",
    "        del total_length_agn\n",
    "\n",
    "        return(seq_err, seq_err_sem, seq_err_agn, symbol_err_sem, symbol_pct_err_sem, symbol_err_agn, symbol_pct_err_agn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, primus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20008521cd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img = cv2.imread(\"Data/Example/000051652-1_2_1.png\",False)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20008590b50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img = resize(test_img, 128)\n",
    "test_img = normalize(test_img)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 128, 859)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = test_img.reshape((1,1,128,-1))\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([424, 1, 1782]), torch.Size([424, 1, 759]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_torch = torch.tensor(test_img, dtype=torch.float32).to(device)\n",
    "\n",
    "pred_sem, pred_agn = model(test_torch)\n",
    "pred_sem = pred_sem.log_softmax(2)\n",
    "pred_agn = pred_agn.log_softmax(2)\n",
    "\n",
    "pred_sem.size(), pred_agn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,  229, 1751,  262, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781,    0,    0, 1727, 1781, 1781,\n",
       "        1781, 1717, 1781,  678, 1781,    0,    0,  688, 1781, 1781, 1781,\n",
       "        1781, 1599,    0,    0, 1314, 1781, 1781, 1781, 1781, 1018, 1781,\n",
       "           0,    0,  823, 1781,  823, 1781, 1781, 1727, 1781,    0, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781, 1781,\n",
       "        1781, 1781, 1781, 1781, 1781, 1781], dtype=int64),\n",
       " array([ 65,   5,   3,  15, 758,  85,  91,  91, 758, 758, 758, 758,  86,\n",
       "        758,  89, 232, 232, 232, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758,  64,  64, 711, 711, 758,\n",
       "        758, 708, 758, 572, 758,  64,  64, 631, 758, 113, 758, 758, 570,\n",
       "         64,  64, 644, 758, 114, 114, 758, 573, 758,  64, 758, 584, 758,\n",
       "        584, 758, 758, 711, 758,  64, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758, 758,\n",
       "        758, 758, 758, 758, 758, 758, 758, 758], dtype=int64))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sem = tensor_to_np_array(pred_sem)\n",
    "pred_agn = tensor_to_np_array(pred_agn)\n",
    "\n",
    "pred_sem, pred_agn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clef-C1',\n",
       " 'keySignature-EbM',\n",
       " 'timeSignature-2/4',\n",
       " 'multirest-23',\n",
       " 'barline',\n",
       " 'rest-quarter',\n",
       " 'rest-eighth',\n",
       " 'note-Bb4_eighth',\n",
       " 'barline',\n",
       " 'note-Bb4_quarter.',\n",
       " 'note-G4_eighth',\n",
       " 'barline',\n",
       " 'note-Eb5_quarter.',\n",
       " 'note-D5_eighth',\n",
       " 'barline',\n",
       " 'note-C5_eighth',\n",
       " 'note-C5_eighth',\n",
       " 'rest-quarter',\n",
       " 'barline']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gt_file_sem = open('Data/Example/000051652-1_2_1.semantic', 'r')\n",
    "test_label_sem = sample_gt_file_sem.readline().rstrip().split('\\t')\n",
    "sample_gt_file_sem.close()\n",
    "\n",
    "test_label_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clef-C1',\n",
       " 'keySignature-EbM',\n",
       " 'timeSignature-2/4',\n",
       " 'multirest-21',\n",
       " 'barline',\n",
       " 'rest-quarter',\n",
       " 'rest-eighth',\n",
       " 'note-Bb4_eighth',\n",
       " 'barline',\n",
       " 'note-Bb4_quarter.',\n",
       " 'note-G4_eighth',\n",
       " 'barline',\n",
       " 'note-Eb5_quarter.',\n",
       " 'note-D5_eighth',\n",
       " 'barline',\n",
       " 'note-C5_eighth',\n",
       " 'note-C5_eighth',\n",
       " 'rest-quarter',\n",
       " 'barline']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sem_str = log_softmax_to_string(pred_sem, int2word_sem, vocab_size_sem)\n",
    "pred_sem_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein(pred_sem_str, test_label_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0043859649122807015"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_levenshtein(pred_sem_str, test_label_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clef.C-L1',\n",
       " 'accidental.flat-L4',\n",
       " 'accidental.flat-L2',\n",
       " 'accidental.flat-S3',\n",
       " 'digit.2-L4',\n",
       " 'digit.4-L2',\n",
       " 'digit.2-S5',\n",
       " 'digit.3-S5',\n",
       " 'multirest-L3',\n",
       " 'barline-L1',\n",
       " 'rest.quarter-L3',\n",
       " 'rest.eighth-L3',\n",
       " 'note.eighth-L4',\n",
       " 'barline-L1',\n",
       " 'note.quarter-L4',\n",
       " 'dot-S4',\n",
       " 'note.eighth-L3',\n",
       " 'barline-L1',\n",
       " 'note.quarter-S5',\n",
       " 'dot-S5',\n",
       " 'note.eighth-L5',\n",
       " 'barline-L1',\n",
       " 'note.eighth-S4',\n",
       " 'note.eighth-S4',\n",
       " 'rest.quarter-L3',\n",
       " 'barline-L1']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gt_file_agn = open('Data/Example/000051652-1_2_1.agnostic', 'r')\n",
    "test_label_agn = sample_gt_file_agn.readline().rstrip().split('\\t')\n",
    "sample_gt_file_agn.close()\n",
    "\n",
    "test_label_agn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clef.C-L1',\n",
       " 'accidental.flat-L4',\n",
       " 'accidental.flat-L2',\n",
       " 'accidental.flat-S3',\n",
       " 'digit.2-L4',\n",
       " 'digit.4-L2',\n",
       " 'digit.2-S5',\n",
       " 'digit.3-S5',\n",
       " 'multirest-L3',\n",
       " 'barline-L1',\n",
       " 'rest.quarter-L3',\n",
       " 'rest.eighth-L3',\n",
       " 'note.eighth-L4',\n",
       " 'barline-L1',\n",
       " 'note.quarter-L4',\n",
       " 'dot-S4',\n",
       " 'note.eighth-L3',\n",
       " 'barline-L1',\n",
       " 'note.quarter-S5',\n",
       " 'dot-S5',\n",
       " 'note.eighth-L5',\n",
       " 'barline-L1',\n",
       " 'note.eighth-S4',\n",
       " 'note.eighth-S4',\n",
       " 'rest.quarter-L3',\n",
       " 'barline-L1']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_agn_str = log_softmax_to_string(pred_agn, int2word_agn, vocab_size_agn)\n",
    "pred_agn_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein(pred_agn_str, test_label_agn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_levenshtein(pred_agn_str, test_label_agn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bkai",
   "language": "python",
   "name": "bkai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
